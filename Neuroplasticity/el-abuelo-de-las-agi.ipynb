{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12612865,"sourceType":"datasetVersion","datasetId":7967657}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"name":"LTSM vs HebbianLTSM casi final ya estamos mas cerc","provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Celda 1 /kaggle/input/aaaaaad/openwebtext_2GB.txt\n\nimport numpy as np\nimport pandas as pd\n\nimport os, random, math, time, sys, collections, itertools, contextlib\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom collections import defaultdict, deque\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom datetime import datetime\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\ntorch.backends.cudnn.benchmark = True\ntorch.set_float32_matmul_precision(\"high\")\nAMP_DTYPE = torch.bfloat16\nUSE_AMP = True\n\nsns.set_style(\"darkgrid\", {\"axes.facecolor\": \"#1c1c1c\", \"grid.color\": \"#444\"})\nplt.rcParams.update({\n    \"figure.facecolor\": \"#1c1c1c\",\n    \"savefig.facecolor\": \"#1c1c1c\",\n    \"text.color\": \"white\",\n    \"axes.labelcolor\": \"white\",\n    \"axes.edgecolor\": \"#aaa\"\n})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T19:55:50.041979Z","iopub.execute_input":"2025-08-03T19:55:50.042274Z","iopub.status.idle":"2025-08-03T19:55:56.543863Z","shell.execute_reply.started":"2025-08-03T19:55:50.042246Z","shell.execute_reply":"2025-08-03T19:55:56.543271Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"\n# 2\nos.makedirs('/kaggle/working/plastic_results/checkpoints', exist_ok=True)\nos.makedirs('/kaggle/working/plastic_results/csv_data', exist_ok=True)\nos.makedirs('/kaggle/working/baseline_results/checkpoints', exist_ok=True)\nos.makedirs('/kaggle/working/baseline_results/csv_data', exist_ok=True)\nos.makedirs('/kaggle/working/plots', exist_ok=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T19:55:56.544556Z","iopub.execute_input":"2025-08-03T19:55:56.544840Z","iopub.status.idle":"2025-08-03T19:55:56.550002Z","shell.execute_reply.started":"2025-08-03T19:55:56.544824Z","shell.execute_reply":"2025-08-03T19:55:56.549262Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Celda 3\n@torch.no_grad()\ndef generate_text(model, prompt, stoi, itos, device, length=100, temperature=0.8):\n    model.eval()\n\n    if hasattr(model, 'c') and hasattr(model.c, 'sequence_length'):\n         seq_len = model.c.sequence_length\n    elif hasattr(model, 'cfg') and hasattr(model.cfg, 'sequence_length'):\n         seq_len = model.cfg.sequence_length\n    else:\n        seq_len = 128\n\n    tokens = [stoi.get(c, 0) for c in prompt]\n    generated_tokens = tokens.copy()\n\n    h_state, c_state = None, None\n    if hasattr(model, 'lstm'):\n        num_layers = model.lstm.num_layers\n        hidden_size = model.lstm.hidden_size\n        h_state = torch.zeros(num_layers, 1, hidden_size).to(device)\n        c_state = torch.zeros(num_layers, 1, hidden_size).to(device)\n\n    if len(tokens) > 1:\n        prompt_tensor = torch.tensor([tokens[:-1]]).to(device)\n        if hasattr(model, 'mask'):\n            _, (h_state, c_state) = model.lstm(model.embed(prompt_tensor), (h_state, c_state))\n        else:\n            _, (h_state, c_state) = model.lstm(model.embed(prompt_tensor))\n\n    input_token = torch.tensor([[tokens[-1]]]).to(device)\n\n    for _ in range(length):\n        if hasattr(model, 'mask'):\n            emb = model.embed(input_token)\n            output, (h_state, c_state) = model.lstm(emb, (h_state, c_state))\n            last_h = output[:, -1, :]\n\n            W_eff = model.W.weight + model.c.hebb_eta * model.h_fast.to(model.W.weight.dtype) if not model.c.disable_hebbian else model.W.weight\n            acts = torch.sigmoid(F.linear(last_h, W_eff * model.mask.unsqueeze(1), model.W.bias))\n            logits = model.out(acts)\n        else:\n            emb = model.embed(input_token)\n            output, (h_state, c_state) = model.lstm(emb, (h_state, c_state))\n            hidden = torch.sigmoid(model.hidden_layer(output[:, -1, :]))\n            logits = model.out(hidden)\n\n        probs = F.softmax(logits.squeeze(0) / temperature, dim=-1)\n        next_token_val = torch.multinomial(probs, 1).item()\n\n        generated_tokens.append(next_token_val)\n        input_token = torch.tensor([[next_token_val]]).to(device)\n\n    generated_text = ''.join([itos.get(t, '') for t in generated_tokens])\n    model.train()\n    return generated_text, generated_tokens[len(tokens):]\n\nclass GenerationMetrics:\n    @staticmethod\n    def calculate_repetition_rate(tokens, n=4):\n        if len(tokens) < n:\n            return 0.0\n\n        ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n        if not ngrams:\n            return 0.0\n\n        unique_ngrams = len(set(ngrams))\n        total_ngrams = len(ngrams)\n\n        return 1.0 - (unique_ngrams / total_ngrams)\n\n    @staticmethod\n    def calculate_diversity(generated_texts_tokens):\n        all_tokens = []\n        for tokens in generated_texts_tokens:\n            all_tokens.extend(tokens)\n\n        if not all_tokens:\n            return 0.0\n\n        unique_tokens = len(set(all_tokens))\n        total_tokens = len(all_tokens)\n\n        return unique_tokens / total_tokens\n\ndef save_generation_samples(model, epoch, step, stoi, itos, device, cfg, model_name):\n    samples = []\n\n    prompts_to_use = [p for cat in cfg.test_prompts_categorized.values() for p in cat]\n    selected_prompts = random.sample(prompts_to_use, k=min(len(prompts_to_use), cfg.generation_samples_per_epoch))\n\n    for prompt in selected_prompts:\n        full_text, generated_tokens = generate_text(model, prompt, stoi, itos, device,\n                                                    cfg.generation_length, cfg.generation_temperature)\n\n        generated_part = full_text[len(prompt):]\n        repetition = GenerationMetrics.calculate_repetition_rate(generated_tokens)\n\n        samples.append({\n            'prompt': prompt,\n            'generated_text': generated_part,\n            'generated_tokens': generated_tokens,\n            'repetition_rate': repetition\n        })\n\n    return samples","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T19:55:56.551851Z","iopub.execute_input":"2025-08-03T19:55:56.552031Z","iopub.status.idle":"2025-08-03T19:55:56.568975Z","shell.execute_reply.started":"2025-08-03T19:55:56.552016Z","shell.execute_reply":"2025-08-03T19:55:56.568252Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Celda 4\n\nclass Config:\n    sequence_length = 128\n    batch_size = 64\n    max_data_mb = 300\n    learning_rate = 5e-4\n    num_epochs = 6\n    eval_every_steps = 200\n\n    embedding_dim = 256\n    lstm_hidden_size = 512\n    num_lstm_layers = 2\n    vocab_size = None\n\n    max_neurons = 1024\n    initial_active = 256\n    target_population_ratio = 0.3\n    plasticity_interval = 20\n    maturation_time = 200\n    survival_percentile = 70\n    birth_percentile = 90\n    tournament_interval = 200\n    elite_clone_ratio = 0.3\n    adaptive_thresholds = True\n    threshold_history_size = 1000\n    population_penalty = 0.001\n    capacity_penalty_start = 0.25\n    sparsity_lambda = 0.0005\n    max_births_per_update = 5\n    max_death_ratio = 0.1\n    birth_cooldown = 50\n    hebb_eta = 0.02\n    hebb_decay = 0.97\n    hebb_norm_clip = 10.0\n    hebb_top_neurons_ratio = 0.5\n    elite_protection_ratio = 0.15\n    elite_update_interval = 1000\n    maturity_threshold = 1000\n    max_plasticity_reduction = 0.7\n    checkpoint_window = 5\n    ensemble_interval = 2000\n    ensemble_blend_factor = 0.2\n    cache_computations = True\n    cache_size = 100\n\n    test_prompts_categorized = {\n        'code': [\n            \"def fibonacci(n):\",\n            \"class NeuralNetwork:\",\n            \"import numpy as np\",\n        ],\n        'narrative': [\n            \"Once upon a time\",\n            \"The scientist discovered\",\n            \"In the year 2050,\",\n        ],\n        'technical': [\n            \"Machine learning is\",\n            \"The algorithm works by\",\n            \"Neural networks can\",\n        ],\n        'conversational': [\n            \"Hello, how are\",\n            \"What do you think about\",\n            \"Could you please explain\",\n        ]\n    }\n\n    generation_length = 80\n    generation_temperature = 0.8\n    generation_samples_per_epoch = 2\n\n    phase_b_start = 6000\n    phase_b_data_mb = 60\n    replay_size = 5000\n\n    data_path = '/kaggle/input/openwebtext-2gb/openwebtext_2GB.txt'\n    plastic_checkpoints_dir = '/kaggle/working/plastic_results/checkpoints'\n    plastic_csv_dir = '/kaggle/working/plastic_results/csv_data'\n    baseline_save_dir = '/kaggle/working/baseline_results'\n    plots_dir = '/kaggle/working/plots'\n\n    disable_hebbian = False\n    disable_death = False\n    track_forgetting = True\n    track_adaptation_speed = True\n    track_generation_quality = True\n\ncfg = Config()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T19:55:56.571990Z","iopub.execute_input":"2025-08-03T19:55:56.572344Z","iopub.status.idle":"2025-08-03T19:55:56.587609Z","shell.execute_reply.started":"2025-08-03T19:55:56.572297Z","shell.execute_reply":"2025-08-03T19:55:56.587021Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Celda 5\nclass CharDataset(Dataset):\n    def __init__(self, text, seq_len, stoi=None, itos=None):\n        if stoi is None:\n            chars = sorted(set(text))\n            self.stoi = {c: i for i, c in enumerate(chars)}\n            self.itos = {i: c for c, i in self.stoi.items()}\n        else:\n            self.stoi, self.itos = stoi, itos\n        self.data = [self.stoi.get(c, 0) for c in text]\n        self.seq_len = seq_len\n\n    def __len__(self):\n        return max(1, (len(self.data) - 1) // self.seq_len)\n\n    def __getitem__(self, idx):\n        s = idx * self.seq_len\n        chunk = self.data[s:s + self.seq_len + 1]\n        if len(chunk) < self.seq_len + 1:\n            chunk = chunk + [0] * (self.seq_len + 1 - len(chunk))\n        x = torch.tensor(chunk[:-1], dtype=torch.long)\n        y = torch.tensor(chunk[1:], dtype=torch.long)\n        return x, y\n\n\ndef build_loaders(cfg, phase='A', existing_stoi=None, existing_itos=None):\n    print(f'Loading text for Phase {phase}...')\n    try:\n        with open(cfg.data_path, 'r', encoding='utf-8', errors='ignore') as f:\n            if phase == 'A':\n                text = f.read(int(cfg.max_data_mb * 1024 * 1024))\n            else:\n                f.seek(int(cfg.max_data_mb * 1024 * 1024))\n                text = f.read(int(cfg.phase_b_data_mb * 1024 * 1024))\n        print(f\"Loaded {len(text)/1e6:.1f}M characters for Phase {phase}\")\n    except FileNotFoundError:\n        print(\"Dataset not found, using synthetic data\")\n        if phase == 'A':\n            text = \"The quick brown fox jumps over the lazy dog. \" * 100000\n        else:\n            text = \"In a hole in the ground there lived a hobbit. \" * 100000\n\n    full = CharDataset(text, cfg.sequence_length, existing_stoi, existing_itos)\n\n    if phase == 'A':\n        cfg.vocab_size = len(full.itos)\n        print(f\"Vocabulary size: {cfg.vocab_size}\")\n\n    n = len(full)\n    train_size = int(0.9 * n)\n    train, val = torch.utils.data.random_split(\n        full, [train_size, n - train_size],\n        generator=torch.Generator().manual_seed(SEED + (1 if phase == 'B' else 0))\n    )\n\n    num_workers = min(4, (os.cpu_count() or 2))\n    use_workers = num_workers > 0\n    common_kwargs = dict(\n        batch_size=cfg.batch_size,\n        num_workers=num_workers,\n        pin_memory=True,\n        drop_last=True,\n        persistent_workers=use_workers\n    )\n    if use_workers:\n        common_kwargs['prefetch_factor'] = 2\n\n    def mk(ds, shuf):\n        return DataLoader(ds, shuffle=shuf, **common_kwargs)\n\n    return mk(train, True), mk(val, False), full.stoi, full.itos, full\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T19:55:56.588483Z","iopub.execute_input":"2025-08-03T19:55:56.588739Z","iopub.status.idle":"2025-08-03T19:55:56.610163Z","shell.execute_reply.started":"2025-08-03T19:55:56.588712Z","shell.execute_reply":"2025-08-03T19:55:56.609550Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# 5.5\nclass CatastrophicForgettingTracker:\n    def __init__(self, cfg):\n        self.cfg = cfg\n        self.phase_checkpoints = {}\n        self.phase_performance = defaultdict(dict)\n        self.retention_scores = []\n\n        # Prompts específicos por fase\n        self.phase_specific_prompts = {\n            'A': {\n                'code': [\"def calculate_mean(arr):\", \"import pandas as pd\", \"class DataProcessor:\"],\n                'narrative': [\"The old man sat\", \"In the beginning\", \"She remembered when\"],\n                'technical': [\"The standard deviation\", \"Linear regression assumes\", \"The p-value indicates\"]\n            },\n            'B': {\n                'code': [\"async function fetchData\", \"const reducer = (state\", \"interface UserProps\"],\n                'narrative': [\"The spaceship landed\", \"In the future\", \"The robot said\"],\n                'technical': [\"Quantum computing uses\", \"The blockchain verifies\", \"Neural architecture search\"]\n            }\n        }\n\n        self.forgetting_history = []\n        self.empirical_metrics_history = []\n        self.phase_a_activations = {}\n        self.initial_phase_a_ppl = None\n\n    def save_phase_checkpoint(self, model, phase, step, device):\n        checkpoint_data = {\n            'step': step,\n            'phase': phase,\n            'active_neurons': int(model.mask.sum().item()),\n            'phase_neurons': {\n                'A': list(model.phase_a_neurons),\n                'B': list(model.phase_b_neurons)\n            }\n        }\n        self.phase_checkpoints[phase] = checkpoint_data\n\n    @torch.no_grad()\n    def evaluate_phase_specific_performance(self, model, stoi, itos, device, current_phase, step):\n        \"\"\"\n        Evalúa PPL por fase/categoría.\n\n        - Soporta logits 2D (B,V) o 3D (B,T,V).\n        - Usa .detach() al guardar activaciones.\n        - Usa AMP (bf16 si disponible; fp16 en GPUs pre-Ada/Hopper) para acelerar.\n        \"\"\"\n        model.eval()\n        results = {}\n        activations_by_phase = {'A': [], 'B': []}\n\n        use_cuda = torch.cuda.is_available() and device.type == 'cuda'\n        amp_dtype = torch.bfloat16\n        if use_cuda:\n            major, _ = torch.cuda.get_device_capability(device)\n            if major < 8:  # <= Ampere: preferir fp16\n                amp_dtype = torch.float16\n\n        for phase, prompt_categories in self.phase_specific_prompts.items():\n            phase_results = {}\n            all_perplexities = []\n            phase_activations = []\n\n            for category, prompts in prompt_categories.items():\n                category_perplexities = []\n\n                for prompt in prompts:\n                    # Generación breve para calentar estado\n                    _full_text, _generated_tokens = generate_text(\n                        model, prompt, stoi, itos, device,\n                        length=50, temperature=0.8\n                    )\n\n                    tokens = [stoi.get(c, 0) for c in prompt]\n                    if len(tokens) > 1:\n                        input_tensor = torch.tensor([tokens[:-1]], device=device)\n                        target_tensor = torch.tensor([tokens[1:]], device=device)\n\n                        if use_cuda:\n                            with torch.cuda.amp.autocast(dtype=amp_dtype):\n                                if hasattr(model, 'mask'):\n                                    logits, analysis = model(input_tensor, return_analysis=True)\n                                    if analysis is not None and 'activations' in analysis:\n                                        phase_activations.append(\n                                            analysis['activations'].detach().float().cpu().numpy()\n                                        )\n                                else:\n                                    logits = model(input_tensor)\n                                    analysis = None\n                        else:\n                            if hasattr(model, 'mask'):\n                                logits, analysis = model(input_tensor, return_analysis=True)\n                                if analysis is not None and 'activations' in analysis:\n                                    phase_activations.append(\n                                        analysis['activations'].detach().float().cpu().numpy()\n                                    )\n                            else:\n                                logits = model(input_tensor)\n                                analysis = None\n\n                        # Manejo robusto de forma de logits\n                        if logits.dim() == 3:\n                            logits_last = logits[:, -1, :]\n                        else:\n                            logits_last = logits\n\n                        loss = F.cross_entropy(logits_last, target_tensor[:, -1])\n                        perplexity = math.exp(loss.item()) if loss.item() < 700 else float('inf')\n\n                        category_perplexities.append(perplexity)\n                        all_perplexities.append(perplexity)\n\n                phase_results[category] = np.mean(category_perplexities) if category_perplexities else float('inf')\n\n            results[phase] = {\n                'category_scores': phase_results,\n                'mean_perplexity': np.mean(all_perplexities) if all_perplexities else float('inf')\n            }\n\n            if phase_activations:\n                activations_by_phase[phase] = np.concatenate(phase_activations, axis=0)\n\n        # Registrar activaciones iniciales de fase A\n        if current_phase == 'A' and self.initial_phase_a_ppl is None:\n            self.initial_phase_a_ppl = results['A']['mean_perplexity']\n            self.phase_a_activations['initial'] = activations_by_phase['A']\n        elif current_phase == 'B':\n            self.phase_a_activations['current'] = activations_by_phase['A']\n\n        # Calcular retención cuando estamos en B\n        retention_score = None\n        if current_phase == 'B' and 'A' in self.phase_checkpoints:\n            if 'A' in results and results['A']['mean_perplexity'] < float('inf'):\n                baseline_ppl = self.phase_performance.get('A', {}).get('final_perplexity', 10.0)\n                current_ppl = results['A']['mean_perplexity']\n                retention_score = 1.0 - min(1.0, (current_ppl - baseline_ppl) / max(baseline_ppl, 1e-9))\n\n        self.phase_performance[current_phase][step] = results\n\n        record = {\n            'step': step,\n            'current_phase': current_phase,\n            'phase_a_ppl': results['A']['mean_perplexity'],\n            'phase_b_ppl': results['B']['mean_perplexity'] if 'B' in results else None,\n            'retention_score': retention_score\n        }\n\n        for phase in ['A', 'B']:\n            if phase in results:\n                for category, score in results[phase]['category_scores'].items():\n                    record[f'{phase}_{category}_ppl'] = score\n\n        self.forgetting_history.append(record)\n\n        model.train()\n        return results, retention_score, activations_by_phase\n\n    def calculate_empirical_metrics(self, model, current_phase, step):\n        metrics = {}\n\n        if self.initial_phase_a_ppl and current_phase == 'B':\n            current_phase_a_ppl = self.phase_performance.get('B', {}).get(step, {}).get('A', {}).get('mean_perplexity', float('inf'))\n            if current_phase_a_ppl < float('inf'):\n                metrics['retencion_fase_a'] = (self.initial_phase_a_ppl / current_phase_a_ppl) * 100\n            else:\n                metrics['retencion_fase_a'] = 0.0\n        else:\n            metrics['retencion_fase_a'] = 100.0\n\n        if 'initial' in self.phase_a_activations and 'current' in self.phase_a_activations:\n            initial_acts = self.phase_a_activations['initial'].flatten()\n            current_acts = self.phase_a_activations['current'].flatten()\n\n            if len(initial_acts) == len(current_acts):\n                correlation = np.corrcoef(initial_acts, current_acts)[0, 1]\n                metrics['interferencia_B'] = 1.0 - abs(correlation)\n            else:\n                metrics['interferencia_B'] = 0.0\n        else:\n            metrics['interferencia_B'] = 0.0\n\n        total_neurons = model.c.max_neurons\n        active_neurons = int(model.mask.sum().item())\n        metrics['costo_oportunidad'] = (total_neurons - active_neurons) / max(1, total_neurons)\n\n        self.empirical_metrics_history.append({\n            'step': step,\n            'phase': current_phase,\n            **metrics\n        })\n\n        return metrics\n\n    def save_forgetting_metrics_to_csv(self, output_dir):\n        if self.forgetting_history:\n            df = pd.DataFrame(self.forgetting_history)\n            df.to_csv(f\"{output_dir}/catastrophic_forgetting_metrics.csv\", index=False)\n\n        if self.empirical_metrics_history:\n            df_empirical = pd.DataFrame(self.empirical_metrics_history)\n            df_empirical.to_csv(f\"{output_dir}/empirical_metrics.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T19:55:56.610881Z","iopub.execute_input":"2025-08-03T19:55:56.611095Z","iopub.status.idle":"2025-08-03T19:55:56.632506Z","shell.execute_reply.started":"2025-08-03T19:55:56.611079Z","shell.execute_reply":"2025-08-03T19:55:56.631879Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# 6\nclass ReplayBuffer:\n    def __init__(self, max_batches):\n        self.buf = deque(maxlen=max_batches)\n        self.losses = deque(maxlen=max_batches)\n        self.phase_labels = deque(maxlen=max_batches)\n\n    def push(self, x, y, loss=None, phase='A'):\n        self.buf.append((x.cpu(), y.cpu()))\n        self.losses.append(loss if loss is not None else 1.0)\n        self.phase_labels.append(phase)\n\n    def sample(self, n=1):\n        if not self.buf:\n            return None\n\n        # Priorizar samples de fase A y con mayor pérdida\n        if len(self.buf) > n:\n            # Crear pesos de prioridad\n            weights = []\n            for i, (loss, phase) in enumerate(zip(self.losses, self.phase_labels)):\n                # Dar más peso a fase A y a pérdidas altas\n                weight = loss if loss is not None else 1.0\n                if phase == 'A':\n                    weight *= 2.0  # Duplicar peso para fase A\n                weights.append(weight)\n\n            # Normalizar pesos\n            total_weight = sum(weights)\n            if total_weight > 0:\n                weights = [w/total_weight for w in weights]\n            else:\n                weights = [1.0/len(self.buf)] * len(self.buf)\n\n            # Muestrear con pesos\n            indices = np.random.choice(len(self.buf), size=n, p=weights)\n            samples = [self.buf[i] for i in indices]\n\n            if n == 1:\n                return samples[0]\n            return samples\n        else:\n            # Si tenemos menos samples que los pedidos, devolver todos\n            if n == 1:\n                return random.choice(self.buf)\n            return list(self.buf)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T19:55:56.633595Z","iopub.execute_input":"2025-08-03T19:55:56.633838Z","iopub.status.idle":"2025-08-03T19:55:56.651819Z","shell.execute_reply.started":"2025-08-03T19:55:56.633818Z","shell.execute_reply":"2025-08-03T19:55:56.651250Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# 7\nclass CompetitiveThresholds:\n    \"\"\"Sistema de umbrales competitivos basados en percentiles\"\"\"\n    def __init__(self, survival_percentile=70, birth_percentile=90, history_size=1000):\n        self.survival_percentile = survival_percentile\n        self.birth_percentile = birth_percentile\n        self.contribution_history = deque(maxlen=history_size)\n        self.loss_history = deque(maxlen=history_size)\n        self.uniqueness_history = deque(maxlen=history_size)\n\n    def update(self, contributions, uniqueness_scores, loss):\n        \"\"\"Actualizar historial y calcular nuevos umbrales dinámicos\"\"\"\n        # Filtrar solo neuronas activas\n        active_contribs = contributions[contributions > 0].cpu().numpy()\n        active_unique = uniqueness_scores[uniqueness_scores > 0].cpu().numpy()\n\n        if len(active_contribs) > 0:\n            self.contribution_history.extend(active_contribs.tolist())\n            self.uniqueness_history.extend(active_unique.tolist())\n        self.loss_history.append(loss)\n\n        # Calcular umbrales basados en percentiles\n        if len(self.contribution_history) > 100:\n            # Umbral de muerte: percentil para supervivencia\n            contrib_threshold = np.percentile(self.contribution_history, 100 - self.survival_percentile)\n            unique_threshold = np.percentile(self.uniqueness_history, 100 - self.survival_percentile)\n\n            # Combinar ambos criterios\n            death_threshold = {\n                'contribution': contrib_threshold,\n                'uniqueness': unique_threshold\n            }\n\n            # Umbral de nacimiento basado en pérdida histórica\n            birth_threshold = np.percentile(self.loss_history, self.birth_percentile)\n        else:\n            # Valores por defecto mientras se acumula historial\n            death_threshold = {\n                'contribution': 0.01,\n                'uniqueness': 0.3\n            }\n            birth_threshold = 3.0\n\n        return death_threshold, birth_threshold\n\n\nclass CachedComputations:\n    \"\"\"Cache para cálculos costosos como correlaciones\"\"\"\n    def __init__(self, cache_size=100):\n        self.cache = {}\n        self.cache_size = cache_size\n        self.hits = 0\n        self.misses = 0\n\n    def get_or_compute(self, key, compute_fn):\n        \"\"\"Obtener del cache o computar\"\"\"\n        if key in self.cache:\n            self.hits += 1\n            return self.cache[key]\n\n        self.misses += 1\n        result = compute_fn()\n\n        # LRU simple: eliminar el más antiguo si excedemos tamaño\n        if len(self.cache) >= self.cache_size:\n            oldest_key = next(iter(self.cache))\n            del self.cache[oldest_key]\n\n        self.cache[key] = result\n        return result\n\n    def clear(self):\n        \"\"\"Limpiar cache\"\"\"\n        self.cache.clear()\n\n    def get_stats(self):\n        \"\"\"Obtener estadísticas de cache\"\"\"\n        total = self.hits + self.misses\n        hit_rate = self.hits / total if total > 0 else 0\n        return {\n            'hits': self.hits,\n            'misses': self.misses,\n            'hit_rate': hit_rate,\n            'size': len(self.cache)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T19:55:56.654094Z","iopub.execute_input":"2025-08-03T19:55:56.654336Z","iopub.status.idle":"2025-08-03T19:55:56.670955Z","shell.execute_reply.started":"2025-08-03T19:55:56.654319Z","shell.execute_reply":"2025-08-03T19:55:56.670276Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Celda 8\n\nclass PlasticLSTM(nn.Module):\n    def __init__(self, c):\n        super().__init__()\n        self.c = c\n\n        # Core modules\n        self.embed = nn.Embedding(c.vocab_size, c.embedding_dim)\n        self.lstm = nn.LSTM(\n            c.embedding_dim, c.lstm_hidden_size, c.num_lstm_layers,\n            batch_first=True, dropout=0.1\n        )\n        self.W = nn.Linear(c.lstm_hidden_size, c.max_neurons)\n        self.out = nn.Linear(c.max_neurons, c.vocab_size)\n\n        # Normalization on neuron pre-activations (stable with AMP)\n        self.neuron_norm = nn.LayerNorm(c.max_neurons)\n\n        # Persistent state buffers\n        self.register_buffer('mask', torch.zeros(c.max_neurons))\n        self.mask[:c.initial_active] = 1\n\n        self.register_buffer('age', torch.zeros(c.max_neurons))\n        self.register_buffer('contrib', torch.zeros(c.max_neurons))\n\n        # Fast Hebbian weights kept in lower precision to save memory/bandwidth\n        self.register_buffer(\n            'h_fast',\n            torch.zeros(c.max_neurons, c.lstm_hidden_size, dtype=AMP_DTYPE)\n        )\n\n        self.register_buffer('uniqueness_score', torch.ones(c.max_neurons))\n        self.register_buffer('error_reduction', torch.zeros(c.max_neurons))\n        self.register_buffer('phase_specialization', torch.zeros(c.max_neurons))\n\n        self.register_buffer('correlation_matrix', torch.eye(c.max_neurons))\n\n        self.register_buffer('competitive_score', torch.zeros(c.max_neurons))\n\n        # Threshold manager / cache\n        self.threshold_manager = CompetitiveThresholds(\n            c.survival_percentile,\n            c.birth_percentile,\n            c.threshold_history_size\n        )\n        if c.cache_computations:\n            self.computation_cache = CachedComputations(c.cache_size)\n        else:\n            self.computation_cache = None\n\n        # Book-keeping\n        self.last_birth_step = 0\n        self.last_tournament_step = 0\n        self.hard_patterns_bank = deque(maxlen=1000)\n        self.pattern_errors = deque(maxlen=1000)\n        self.life_events = []\n        self.step = 0\n        self._cache = (None, None, None)\n        self._capacity_penalty = 0.0\n\n        # Phases\n        self.current_phase = 'A'\n        self.phase_a_neurons = set(range(c.initial_active))\n        self.phase_b_neurons = set()\n        self.generation_history = defaultdict(list)\n\n        self._current_thresholds = {\n            'birth': 3.2,\n            'death': {'contribution': 0.01, 'uniqueness': 0.3}\n        }\n\n        # Inheritance tracking\n        self.register_buffer('inheritance_count', torch.zeros(c.max_neurons))\n        self.register_buffer('knowledge_sources', torch.zeros(c.max_neurons))\n        self.inheritance_history = []\n\n        # Population/phase control\n        self.population_flex_factor = 1.0\n        self.phase_transition_buffer = 0\n\n        # Elite / anchors\n        self.elite_neurons = set()\n        self.elite_protection_ratio = c.elite_protection_ratio\n        self.phase_elite_history = {'A': [], 'B': []}\n        self.phase_a_anchors = set()\n        self.dynamic_anchor_ratio = 0.2\n        self.min_anchor_ratio = 0.1\n        self.max_anchor_ratio = 0.5\n\n        # Plasticity controls\n        self.register_buffer('plasticity_levels', torch.ones(c.max_neurons))\n        self.register_buffer('phase_importance', torch.zeros(c.max_neurons))\n        self.register_buffer('cross_phase_utility', torch.zeros(c.max_neurons))\n        self.register_buffer('plasticity_factor', torch.ones(c.max_neurons))\n        self.maturity_threshold = c.maturity_threshold\n\n        # Regions (A/shared/B)\n        self.phase_regions = {\n            'A': (0, int(c.max_neurons * 0.6)),\n            'shared': (int(c.max_neurons * 0.6), int(c.max_neurons * 0.8)),\n            'B': (int(c.max_neurons * 0.8), c.max_neurons)\n        }\n\n        # Soft checkpoints\n        self.soft_checkpoints = deque(maxlen=c.checkpoint_window)\n        self.checkpoint_interval = 500\n\n        # Vector phase flags\n        self.register_buffer('phase_a_flag', torch.zeros(c.max_neurons, dtype=torch.bool))\n        self.register_buffer('phase_b_flag', torch.zeros(c.max_neurons, dtype=torch.bool))\n        self.phase_a_flag[:c.initial_active] = True\n\n        # Init\n        with torch.no_grad():\n            nn.init.xavier_uniform_(self.W.weight)\n            nn.init.zeros_(self.W.bias)\n            nn.init.xavier_uniform_(self.out.weight)\n\n    # --- MÉTODOS AÑADIDOS/CORREGIDOS ---\n    def set_phase(self, phase):\n        self.current_phase = phase\n        if phase == 'B':\n            self.phase_transition_buffer = 1.0 # Activa el buffer de transición\n        print(f\"Model phase set to: {self.current_phase}\")\n\n    def update_plasticity_levels(self, phase_a_performance, phase_b_performance):\n        if self.current_phase != 'B':\n            return\n\n        # Identificar neuronas de fase A que son importantes (alto contrib)\n        phase_a_indices = torch.tensor(list(self.phase_a_neurons), device=self.mask.device, dtype=torch.long)\n        if len(phase_a_indices) == 0:\n            return\n\n        contribs_a = self.contrib[phase_a_indices]\n        important_threshold = torch.quantile(contribs_a, 0.7) # Proteger el 30% superior\n        important_neurons_mask = contribs_a >= important_threshold\n        important_indices = phase_a_indices[important_neurons_mask]\n\n        # Reducir la plasticidad de las neuronas importantes de la fase A para protegerlas\n        reduction_factor = 0.05\n        self.plasticity_levels[important_indices] *= reduction_factor\n        self.plasticity_levels.clamp_(min=0.01) # Mínimo de plasticidad\n\n    # --- FIN DE MÉTODOS AÑADIDOS/CORREGIDOS ---\n\n    # Gradient hooks modulated by plasticity\n    def register_gradient_hooks(self):\n        def mod_grad_W(grad):\n            if grad is None:\n                return grad\n            mask = self.plasticity_levels[:grad.shape[0]].unsqueeze(1).to(grad.device)\n            return grad * mask\n\n        def mod_grad_bias(grad):\n            if grad is None:\n                return grad\n            return grad * self.plasticity_levels.to(grad.device)\n\n        def mod_grad_out(grad):\n            if grad is None:\n                return grad\n            mask = self.plasticity_levels[:grad.shape[1]].unsqueeze(0).to(grad.device)\n            return grad * mask\n\n        self.W.weight.register_hook(mod_grad_W)\n        self.W.bias.register_hook(mod_grad_bias)\n        self.out.weight.register_hook(mod_grad_out)\n\n    # Similarity utilities (compute in float32 for numerical stability)\n    def compute_neuron_similarity(self, idx1, idx2):\n        w1 = self.W.weight[idx1].float()\n        w2 = self.W.weight[idx2].float()\n        weight_sim = F.cosine_similarity(w1.unsqueeze(0), w2.unsqueeze(0)).item()\n\n        out1 = self.out.weight[:, idx1].float()\n        out2 = self.out.weight[:, idx2].float()\n        out_sim = F.cosine_similarity(out1.unsqueeze(0), out2.unsqueeze(0)).item()\n\n        corr_sim = self.correlation_matrix[idx1, idx2].item()\n        total_similarity = 0.4 * weight_sim + 0.3 * out_sim + 0.3 * corr_sim\n        return total_similarity\n\n    def find_inheritance_targets(self, dying_idx, k=10):\n        active_mask = (self.mask > 0)\n        active_indices = active_mask.nonzero(as_tuple=True)[0]\n        if active_indices.numel() == 0:\n            return []\n\n        idx = dying_idx if torch.is_tensor(dying_idx) else torch.tensor(dying_idx, device=self.W.weight.device)\n        dying_in_A = bool(self.phase_a_flag[idx])\n\n        w_all = self.W.weight[active_indices].float()\n        w_d = self.W.weight[idx].float().unsqueeze(0)\n        sim_w = F.cosine_similarity(w_all, w_d, dim=1)\n\n        o_all = self.out.weight[:, active_indices].T.float()\n        o_d = self.out.weight[:, idx].float().unsqueeze(0)\n        sim_o = F.cosine_similarity(o_all, o_d, dim=1)\n\n        corr_sim = self.correlation_matrix[idx, active_indices]\n\n        total_sim = 0.4 * sim_w + 0.3 * sim_o + 0.3 * corr_sim\n\n        plasticity_bonus = 1.0 - self.plasticity_levels[active_indices] * 0.5\n        total_sim = total_sim * plasticity_bonus\n\n        if dying_in_A:\n            same_phase_bonus = torch.where(self.phase_a_flag[active_indices], 1.5, 1.0)\n            total_sim = total_sim * same_phase_bonus\n        else:\n            same_phase_bonus = torch.where(self.phase_b_flag[active_indices], 1.5, 1.0)\n            total_sim = total_sim * same_phase_bonus\n\n        inheritance_penalty = 1.0 - (self.inheritance_count[active_indices] / 10).tanh()\n        adjusted_sim = total_sim * inheritance_penalty\n\n        self_mask = active_indices != idx\n        if self_mask.any():\n            adjusted_sim = adjusted_sim[self_mask]\n            total_sim_f = total_sim[self_mask]\n            candidates = active_indices[self_mask]\n        else:\n            total_sim_f = total_sim\n            candidates = active_indices\n\n        if candidates.numel() == 0:\n            return []\n\n        topk = min(k, candidates.numel())\n        vals, inds = adjusted_sim.topk(topk)\n        chosen = candidates[inds]\n\n        out_list = []\n        for c, v, o in zip(chosen, vals, total_sim_f[inds]):\n            out_list.append((c, v.item(), o.item()))\n        return out_list\n\n    def inherit_knowledge(self, dying_idx, inheritors):\n        if not inheritors:\n            return\n\n        dying_weights = self.W.weight.data[dying_idx].clone()\n        dying_out_weights = self.out.weight.data[:, dying_idx].clone()\n        dying_bias = self.W.bias.data[dying_idx].clone()\n        dying_contrib = self.contrib[dying_idx].clone()\n        dying_hebbian = self.h_fast[dying_idx].clone()\n\n        if dying_idx.item() in self.phase_a_neurons:\n            low_plasticity_indices = (self.plasticity_levels < 0.3).nonzero(as_tuple=True)[0]\n            if len(low_plasticity_indices) > 0:\n                anchor_idx = low_plasticity_indices[torch.randint(len(low_plasticity_indices), (1,))]\n                inheritors = [(anchor_idx, 1.0, 1.0)] + inheritors[:9]\n\n        total_similarity = sum(sim for _, _, sim in inheritors)\n\n        for target_idx, adjusted_sim, original_sim in inheritors:\n            inheritance_ratio = original_sim / total_similarity if total_similarity > 0 else 1.0 / len(inheritors)\n            plasticity = self.plasticity_levels[target_idx].item()\n            merge_factor = 0.8 * inheritance_ratio * plasticity\n\n            self.W.weight.data[target_idx] = (1 - merge_factor) * self.W.weight.data[target_idx] + merge_factor * dying_weights\n            self.out.weight.data[:, target_idx] = (1 - merge_factor) * self.out.weight.data[:, target_idx] + merge_factor * dying_out_weights\n            self.W.bias.data[target_idx] = (1 - merge_factor) * self.W.bias.data[target_idx] + merge_factor * dying_bias\n\n            contrib_inheritance = 0.3 * inheritance_ratio * dying_contrib\n            self.contrib[target_idx] += contrib_inheritance\n\n            self.h_fast[target_idx] = (1 - merge_factor * 0.5) * self.h_fast[target_idx] + (merge_factor * 0.5) * dying_hebbian.to(self.h_fast.dtype)\n\n            self.inheritance_count[target_idx] += 1\n            self.knowledge_sources[target_idx] += inheritance_ratio\n\n            self.inheritance_history.append({\n                'step': self.step,\n                'dying_neuron': dying_idx.item() if torch.is_tensor(dying_idx) else dying_idx,\n                'inheritor': target_idx.item() if torch.is_tensor(target_idx) else target_idx,\n                'inheritance_ratio': inheritance_ratio,\n                'merge_factor': merge_factor,\n                'phase': self.current_phase,\n                'to_low_plasticity': plasticity < 0.3\n            })\n\n    def get_adaptive_population_target(self, current_loss, phase):\n        base_target = self.c.max_neurons * self.c.target_population_ratio\n        if phase == 'B' and self.phase_transition_buffer > 0:\n            target = base_target * (1.0 + 0.5 * self.phase_transition_buffer)\n            self.phase_transition_buffer *= 0.95\n        else:\n            if hasattr(self, 'recent_loss_trend'):\n                if self.recent_loss_trend > 0:\n                    self.population_flex_factor = min(1.3, self.population_flex_factor * 1.02)\n                else:\n                    self.population_flex_factor = max(0.8, self.population_flex_factor * 0.98)\n            target = base_target * self.population_flex_factor\n        return int(target)\n\n    @property\n    def current_birth_threshold(self):\n        return self._current_thresholds['birth']\n\n    @property\n    def current_death_threshold(self):\n        death_thresh = self._current_thresholds['death']\n        if isinstance(death_thresh, dict):\n            return (death_thresh.get('contribution', 0.01) +\n                    death_thresh.get('uniqueness', 0.3)) / 2\n        return death_thresh\n\n    def forward(self, x, return_analysis=False):\n        self.step += 1\n\n        h, _ = self.lstm(self.embed(x))\n        last = h[:, -1, :]\n\n        # Effective weights (Hebbian add-on)\n        if self.c.disable_hebbian:\n            W_eff = self.W.weight\n        else:\n            h_fast_normed = self.h_fast.to(self.W.weight.dtype)\n            confidence = torch.sigmoid(self.error_reduction).unsqueeze(1)\n            h_fast_gated = h_fast_normed * confidence\n            W_eff = self.W.weight + self.c.hebb_eta * h_fast_gated\n\n        pre_acts = F.linear(last, W_eff * self.mask.unsqueeze(1), self.W.bias)  # (B, N)\n        pre_acts = self.neuron_norm(pre_acts)                                   # (B, N)\n        acts = torch.sigmoid(pre_acts)                                          # (B, N)\n\n        if self.training:\n            current_pop = self.mask.sum()\n            target_pop = self.get_adaptive_population_target(0, self.current_phase)\n            self._capacity_penalty = self.compute_capacity_penalty(current_pop, target_pop)\n            if self.c.sparsity_lambda > 0:\n                sparsity_penalty = self.c.sparsity_lambda * acts.abs().mean()\n                self._capacity_penalty += sparsity_penalty\n\n        logits = self.out(acts)\n        self._cache = (acts.detach(), last.detach(), None)\n\n        if return_analysis:\n            return logits, {\n                'activations': acts,\n                'hidden': last,\n                'uniqueness': self.uniqueness_score,\n                'phase_spec': self.phase_specialization,\n                'competitive_score': self.competitive_score,\n                'plasticity_levels': self.plasticity_levels\n            }\n\n        return logits\n\n    def compute_capacity_penalty(self, current_population, target_population):\n        if current_population <= target_population * self.c.capacity_penalty_start:\n            return 0.0\n        overpopulation_ratio = (current_population - target_population) / target_population\n        penalty = self.c.population_penalty * (overpopulation_ratio.exp() - 1)\n        return penalty.item() if torch.is_tensor(penalty) else penalty","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T19:55:56.671718Z","iopub.execute_input":"2025-08-03T19:55:56.671880Z","iopub.status.idle":"2025-08-03T19:55:56.706957Z","shell.execute_reply.started":"2025-08-03T19:55:56.671867Z","shell.execute_reply":"2025-08-03T19:55:56.706278Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Celda 9\n\n@torch.no_grad()\ndef compute_neuron_uniqueness(self, acts):\n    \"\"\"\n    Calcula la unicidad de las neuronas basándose en la correlación de sus activaciones.\n    La unicidad se define como 1 menos la correlación media absoluta con otras neuronas.\n    - Se ejecuta en FP32 para estabilidad numérica.\n    - Usa una máscara para considerar solo las neuronas activas.\n    - Actualiza el buffer `self.uniqueness_score`.\n    \"\"\"\n    active_mask = self.mask > 0\n    active_indices = active_mask.nonzero(as_tuple=True)[0]\n\n    if active_indices.numel() < 2:\n        self.uniqueness_score.fill_(1.0)\n        return\n\n    active_acts = acts[:, active_indices].float()\n\n    if torch.std(active_acts) < 1e-6:\n        self.uniqueness_score[active_indices] = 0.0\n        return\n\n    correlation_matrix = torch.corrcoef(active_acts.T)\n    correlation_matrix.nan_to_num_(0.0)\n\n    abs_corr = torch.abs(correlation_matrix)\n    avg_corr = (abs_corr.sum(dim=1) - 1) / max(1, (active_indices.numel() - 1))\n    uniqueness = 1.0 - avg_corr\n\n    decay = 0.95\n    current_uniqueness = self.uniqueness_score[active_indices]\n    self.uniqueness_score[active_indices] = decay * current_uniqueness + (1 - decay) * uniqueness.to(self.uniqueness_score.dtype)\n\nPlasticLSTM.compute_neuron_uniqueness = compute_neuron_uniqueness\n\n\n@torch.no_grad()\ndef competitive_structural_update(self, loss, loss_per_sample=None):\n    # Asegurar constantes AMP disponibles\n    try:\n        AMP_DTYPE\n    except NameError:\n        AMP_DTYPE = torch.float16\n\n    # --- Edad y plasticidad (mantener en FP32 por estabilidad) ---\n    self.age += self.mask\n    maturity_factor = torch.sigmoid((self.age - self.maturity_threshold) / 500)\n    # plasticity_levels es FP32; mantener control en FP32\n    self.plasticity_factor = 1.0 - (maturity_factor * 0.7) * self.plasticity_levels\n\n    # Actualizar élite/ancoras de forma ocasional\n    if self.step % 1000 == 0:\n        if hasattr(self, '_identify_elite_neurons'): self._identify_elite_neurons()\n        self._graduate_low_performing_anchors()\n\n    acts, hidden, _ = self._cache\n\n    # --- Métricas de unicidad y reducción de error (FP32) ---\n    if acts is not None:\n        # compute_neuron_uniqueness usa corrcoef -> FP32\n        self.compute_neuron_uniqueness(acts)\n\n        if loss_per_sample is not None:\n            # menor pérdida que la media se considera \"buena\"\n            loss_mean = loss\n            neuron_impact = acts * (loss_per_sample.unsqueeze(1) < loss_mean)\n            self.error_reduction = 0.9 * self.error_reduction + 0.1 * neuron_impact.mean(0)\n\n        plasticity_weight = 0.5 + 0.5 * self.plasticity_levels\n        survival_bonus = torch.clamp(self.age / 5000, 0, 0.3) * (1.0 - self.plasticity_levels * 0.5)\n\n        self.competitive_score = (\n            0.35 * self.contrib +\n            0.30 * self.uniqueness_score +\n            0.25 * self.error_reduction +\n            0.10 * torch.sigmoid(self.age / 1000)\n        ) * self.mask * plasticity_weight + survival_bonus * self.mask\n\n        self.contrib = 0.9 * self.contrib + 0.1 * self.competitive_score\n\n    if hidden is not None and loss_per_sample is not None and hasattr(self, 'track_error_patterns'):\n        self.track_error_patterns(hidden, loss_per_sample)\n\n    # --- Población y umbrales adaptativos ---\n    current_pop = int(self.mask.sum().item())\n    target_pop = self.get_adaptive_population_target(loss, self.current_phase)\n\n    death_thresh, birth_thresh = self.threshold_manager.update(\n        self.contrib, self.uniqueness_score, float(loss)\n    )\n\n    # Amortiguar transición de fase\n    if self.phase_transition_buffer > 0:\n        if isinstance(death_thresh, dict):\n            death_thresh = {\n                k: (v * 1.5 if isinstance(v, (int, float)) else v)\n                for k, v in death_thresh.items()\n            }\n        else:\n            death_thresh = death_thresh * 1.5\n        birth_thresh *= 0.8\n\n    self._current_thresholds['birth'] = float(birth_thresh)\n    self._current_thresholds['death'] = death_thresh\n\n    births_this_update = 0\n    deaths_this_update = 0\n\n    # --- Torneo si hay sobrepoblación grande ---\n    if (current_pop > target_pop * 1.2 and\n        self.step - self.last_tournament_step >= self.c.tournament_interval):\n\n        num_to_kill = max(\n            current_pop - int(target_pop * 1.05),\n            int(current_pop * self.c.max_death_ratio)\n        )\n\n        active_indices = (self.mask > 0).nonzero(as_tuple=True)[0]\n        scores = self.competitive_score[active_indices].clone()\n\n        if self.current_phase == 'B' and len(self.phase_a_neurons) > 0:\n            for idx in active_indices:\n                if idx.item() in self.phase_a_neurons:\n                    idx_in_active = (active_indices == idx).nonzero(as_tuple=True)[0]\n                    if len(idx_in_active) > 0:\n                        scores[idx_in_active[0]] *= (2.0 - self.plasticity_levels[idx])\n\n        _, sorted_indices = scores.sort(descending=True)\n        losers = active_indices[sorted_indices[-num_to_kill:]]\n\n        deaths_this_update = self._kill_neurons(losers, reason='tournament', inherit_knowledge=True)\n        self.last_tournament_step = self.step\n\n    # --- Selección blanda por umbrales ---\n    elif not self.c.disable_death and current_pop > target_pop * 1.1:\n        if isinstance(death_thresh, dict):\n            below_contrib_thresh = self.contrib < float(death_thresh.get('contribution', 0.01))\n            below_unique_thresh = self.uniqueness_score < float(death_thresh.get('uniqueness', 0.3))\n        else:\n            below_contrib_thresh = self.contrib < float(death_thresh)\n            below_unique_thresh = self.uniqueness_score < 0.3\n\n        is_mature = self.age > self.c.maturation_time\n        plasticity_death_factor = self.plasticity_levels.clamp(min=0.3)\n\n        elimination_score = (\n            below_contrib_thresh.float() * 0.4 +\n            below_unique_thresh.float() * 0.4 +\n            is_mature.float() * 0.2\n        ) * self.mask * plasticity_death_factor\n\n        death_candidates = (elimination_score > 0.5).nonzero(as_tuple=True)[0]\n\n        if len(death_candidates) > 0:\n            max_deaths = min(\n                len(death_candidates),\n                int(current_pop * 0.05),\n                current_pop - int(target_pop * 0.9)\n            )\n\n            if max_deaths > 0:\n                scores = elimination_score[death_candidates]\n                _, top_indices = scores.topk(min(max_deaths, len(scores)))\n                death_idx = death_candidates[top_indices]\n\n                deaths_this_update = self._kill_neurons(death_idx, reason='selection', inherit_knowledge=True)\n\n    # --- Historial de pérdidas para tendencias ---\n    if hasattr(self, 'loss_history'):\n        self.loss_history.append(float(loss))\n        if len(self.loss_history) > 20:\n            recent_improvement = (self.loss_history[-20] - self.loss_history[-1]) / (self.loss_history[-20] + 1e-12)\n            if recent_improvement < 0.001:\n                birth_thresh *= 0.95\n    else:\n        self.loss_history = deque(maxlen=100)\n        self.loss_history.append(float(loss))\n\n    # --- Nacimientos controlados ---\n    should_birth = (\n        float(loss) > float(birth_thresh) and\n        current_pop < int(target_pop * 1.3) and\n        self.step - self.last_birth_step >= self.c.birth_cooldown\n    )\n\n    if should_birth:\n        max_allowed_births = max(0, int(target_pop * 1.3) - current_pop)\n        if max_allowed_births > 0:\n            if float(loss) > float(birth_thresh):\n                error_factor = min((float(loss) - float(birth_thresh)) / float(birth_thresh), 1.0)\n            else:\n                error_factor = 0.5\n            num_births = max(1, int(self.c.max_births_per_update * error_factor))\n            num_births = min(num_births, max_allowed_births)\n\n            births_this_update = self._smart_birth(num_births, float(loss), float(birth_thresh), loss_per_sample)\n\n            if births_this_update > 0:\n                self.last_birth_step = self.step\n\n    if hasattr(self, 'loss_history') and len(self.loss_history) > 10:\n        self.recent_loss_trend = (self.loss_history[-1] - self.loss_history[-10]) / 10.0\n\n\ndef _graduate_low_performing_anchors(self):\n    if self.current_phase != 'B':\n        return\n\n    low_plasticity_indices = (self.plasticity_levels < 0.3).nonzero(as_tuple=True)[0]\n\n    for idx in low_plasticity_indices:\n        if self.mask[idx] > 0 and idx.item() in self.phase_a_neurons:\n            performance_percentile = (self.contrib[idx] > self.contrib[self.mask > 0]).float().mean()\n\n            if performance_percentile < 0.2 and self.age[idx] > 5000:\n                self.plasticity_levels[idx] = min(0.8, float(self.plasticity_levels[idx]) + 0.3)\n                self.phase_a_anchors.discard(idx.item())\n\n            elif performance_percentile > 0.8 and self.plasticity_levels[idx] > 0:\n                self.plasticity_levels[idx] = max(0.0, float(self.plasticity_levels[idx]) - 0.1)\n\n\ndef _kill_neurons(self, indices, reason='selection', inherit_knowledge=True):\n    if len(indices) == 0:\n        return 0\n\n    protected_indices = []\n    killable_indices = []\n\n    for idx in indices:\n        idx_val = idx.item() if torch.is_tensor(idx) else idx\n        protection_threshold = 0.8 if reason == 'tournament' else 0.3\n\n        if (idx_val in self.elite_neurons and reason != 'tournament') or \\\n           (self.plasticity_levels[idx] < protection_threshold and self.contrib[idx] > self.contrib.mean()):\n            protected_indices.append(idx_val)\n        else:\n            killable_indices.append(idx)\n\n    indices = torch.tensor(killable_indices, device=self.mask.device) if killable_indices else torch.tensor([], dtype=torch.long, device=self.mask.device)\n\n    if len(indices) == 0:\n        return 0\n\n    if inherit_knowledge:\n        for idx in indices:\n            if self.contrib[idx] > self.contrib.mean() * 0.5:\n                inheritors = self.find_inheritance_targets(idx, k=5)\n                if inheritors:\n                    self.inherit_knowledge(idx, inheritors)\n\n    # Reset de estado (FP32); pesos quedan en FP32, hebb en AMP_DTYPE\n    self.mask[indices] = 0\n    self.age[indices] = 0\n    self.contrib[indices] = 0\n    self.uniqueness_score[indices] = 1\n    self.error_reduction[indices] = 0\n    self.competitive_score[indices] = 0\n    self.W.weight.data[indices] = 0\n    self.W.bias.data[indices] = 0\n    self.out.weight.data[:, indices] = 0\n    self.h_fast[indices] = torch.zeros_like(self.h_fast[indices], dtype=self.h_fast.dtype)\n    self.inheritance_count[indices] = 0\n    self.knowledge_sources[indices] = 0\n    self.plasticity_levels[indices] = 1.0\n\n    # Flags de fase\n    self.phase_a_flag[indices] = False\n    self.phase_b_flag[indices] = False\n\n    for idx in indices:\n        self.life_events.append({\n            'type': 'death',\n            'step': self.step,\n            'id': idx.item(),\n            'phase': self.current_phase,\n            'reason': reason,\n            'final_score': float(self.competitive_score[idx]),\n            'knowledge_inherited': bool(inherit_knowledge),\n            'plasticity_level': float(self.plasticity_levels[idx])\n        })\n\n    for idx in indices:\n        idx_val = idx.item()\n        self.phase_a_neurons.discard(idx_val)\n        self.phase_b_neurons.discard(idx_val)\n        self.elite_neurons.discard(idx_val)\n        self.phase_a_anchors.discard(idx_val)\n\n    return len(indices)\n\n\ndef _smart_birth(self, num_births, loss, birth_thresh, loss_per_sample=None):\n    if self.current_phase == 'A':\n        valid_range = (self.phase_regions['A'][0], self.phase_regions['shared'][1])\n    else:\n        valid_range = (self.phase_regions['shared'][0], self.phase_regions['B'][1])\n\n    free_indices = (self.mask == 0).nonzero(as_tuple=True)[0]\n    free_indices = free_indices[(free_indices >= valid_range[0]) & (free_indices < valid_range[1])]\n\n    if len(free_indices) == 0 or num_births == 0:\n        return 0\n\n    num_births = min(num_births, len(free_indices))\n    birth_indices = free_indices[torch.randperm(len(free_indices), device=free_indices.device)[:num_births]]\n\n    births = 0\n    for idx in birth_indices:\n        init_type = 'random'\n\n        if len(self.hard_patterns_bank) > 10 and random.random() < 0.7 and len(list(self.pattern_errors)) > 100:\n            pattern_idx = np.argmax(list(self.pattern_errors)[-100:])\n            hard_pattern = list(self.hard_patterns_bank)[-100:][pattern_idx].to(self.W.weight.device)\n            noise = torch.randn_like(hard_pattern, dtype=self.W.weight.dtype) * 0.1\n            self.W.weight.data[idx] = hard_pattern.to(self.W.weight.dtype) + noise\n            init_type = 'hard_pattern'\n\n        elif self.mask.sum() > 10 and random.random() < self.c.elite_clone_ratio:\n            top_k = min(10, int(self.mask.sum()))\n            elite = self.competitive_score.topk(top_k).indices\n            donor = elite[random.randint(0, len(elite) - 1)]\n            self.W.weight.data[idx] = self.W.weight.data[donor] + torch.randn_like(self.W.weight.data[donor]) * 0.15\n            self.out.weight.data[:, idx] = self.out.weight.data[:, donor] + torch.randn_like(self.out.weight.data[:, donor]) * 0.1\n            init_type = 'elite_clone'\n        else:\n            nn.init.xavier_uniform_(self.W.weight.data[idx:idx+1])\n            nn.init.normal_(self.out.weight.data[:, idx], 0, 0.02)\n            init_type = 'random'\n\n        # Estado inicial\n        self.mask[idx] = 1\n        self.age[idx] = 0\n        self.contrib[idx] = 0.05\n        self.uniqueness_score[idx] = 0.5\n        self.error_reduction[idx] = 0\n        self.competitive_score[idx] = 0.05\n        self.h_fast[idx] = torch.zeros_like(self.h_fast[idx], dtype=self.h_fast.dtype)\n        self.W.bias.data[idx] = 0\n        self.inheritance_count[idx] = 0\n        self.knowledge_sources[idx] = 0\n        self.plasticity_factor[idx] = 1.0\n        self.plasticity_levels[idx] = 1.0\n\n        if self.current_phase == 'B':\n            self.phase_specialization[idx] = 1\n            self.phase_b_neurons.add(idx.item())\n            self.phase_b_flag[idx] = True\n            self.phase_a_flag[idx] = False\n        else:\n            self.phase_specialization[idx] = 0\n            self.phase_a_neurons.add(idx.item())\n            self.phase_a_flag[idx] = True\n            self.phase_b_flag[idx] = False\n\n        self.life_events.append({\n            'type': 'birth',\n            'step': self.step,\n            'id': idx.item(),\n            'loss': float(loss),\n            'phase': self.current_phase,\n            'init_type': init_type,\n            'birth_threshold': float(birth_thresh),\n            'initial_plasticity': 1.0\n        })\n\n        births += 1\n\n    return births\n\n\n# Enlazar métodos a la clase\nPlasticLSTM.structural_update = competitive_structural_update\nPlasticLSTM._graduate_low_performing_anchors = _graduate_low_performing_anchors\nPlasticLSTM._kill_neurons = _kill_neurons\nPlasticLSTM._smart_birth = _smart_birth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T19:55:56.707760Z","iopub.execute_input":"2025-08-03T19:55:56.708002Z","iopub.status.idle":"2025-08-03T19:55:56.745917Z","shell.execute_reply.started":"2025-08-03T19:55:56.707982Z","shell.execute_reply":"2025-08-03T19:55:56.745416Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# 10\n@torch.no_grad()\ndef analyze_competition_dynamics(self):\n    active_mask = self.mask > 0\n    active_indices = active_mask.nonzero(as_tuple=True)[0]\n    if active_indices.numel() == 0:\n        return {}\n\n    active_scores = self.competitive_score[active_indices]\n    active_contribs = self.contrib[active_indices]\n    active_unique = self.uniqueness_score[active_indices]\n    active_plasticity = self.plasticity_levels[active_indices]\n\n    top_10_pct = int(len(active_indices) * 0.1)\n    bottom_10_pct = top_10_pct\n    if top_10_pct > 0:\n        top_scores = active_scores.topk(top_10_pct).values\n        bottom_scores = active_scores.topk(bottom_10_pct, largest=False).values\n        elite_gap = (top_scores.mean() - bottom_scores.mean()).item()\n    else:\n        elite_gap = 0.0\n\n    # Conteos por fase\n    phase_a_active = sum(1 for idx in active_indices.tolist() if idx in self.phase_a_neurons)\n    phase_b_active = sum(1 for idx in active_indices.tolist() if idx in self.phase_b_neurons)\n\n    # Plasticidad por fase\n    if phase_a_active > 0:\n        phase_a_indices = torch.tensor([i for i in active_indices.tolist() if i in self.phase_a_neurons],\n                                       device=active_indices.device, dtype=torch.long)\n        phase_a_plasticity = self.plasticity_levels[phase_a_indices].mean().item()\n    else:\n        phase_a_plasticity = 1.0\n\n    if phase_b_active > 0:\n        phase_b_indices = torch.tensor([i for i in active_indices.tolist() if i in self.phase_b_neurons],\n                                       device=active_indices.device, dtype=torch.long)\n        phase_b_plasticity = self.plasticity_levels[phase_b_indices].mean().item()\n    else:\n        phase_b_plasticity = 1.0\n\n    low_plasticity_count = int((active_plasticity < 0.3).sum().item())\n    medium_plasticity_count = int(((active_plasticity >= 0.3) & (active_plasticity < 0.7)).sum().item())\n    high_plasticity_count = int((active_plasticity >= 0.7).sum().item())\n\n    return {\n        'active_count': int(active_indices.numel()),\n        'avg_competitive_score': float(active_scores.mean().item()),\n        'std_competitive_score': float(active_scores.std(unbiased=False).item()),\n        'avg_contribution': float(active_contribs.mean().item()),\n        'avg_uniqueness': float(active_unique.mean().item()),\n        'elite_gap': float(elite_gap),\n        'phase_a_ratio': float(phase_a_active / active_indices.numel()),\n        'phase_b_ratio': float(phase_b_active / active_indices.numel()),\n        'avg_plasticity': float(active_plasticity.mean().item()),\n        'phase_a_avg_plasticity': float(phase_a_plasticity),\n        'phase_b_avg_plasticity': float(phase_b_plasticity),\n        'plasticity_distribution': {\n            'low': low_plasticity_count,\n            'medium': medium_plasticity_count,\n            'high': high_plasticity_count\n        },\n        'cache_stats': self.computation_cache.get_stats() if hasattr(self, 'computation_cache') and self.computation_cache else None\n    }\n\n\n@torch.no_grad()\ndef get_population_health(self):\n    current_pop = int(self.mask.sum().item())\n    target_pop = int(self.get_adaptive_population_target(0.0, self.current_phase))\n\n    if current_pop > 0:\n        active_mask = self.mask > 0\n        diversity = float(self.uniqueness_score[active_mask].mean().item())\n        redundancy = 1.0 - diversity\n\n        active_plasticity = self.plasticity_levels[active_mask]\n        effective_capacity = float((self.plasticity_levels * self.mask).sum().item())\n\n        phase_a_active = sum(1 for idx in active_mask.nonzero(as_tuple=True)[0].tolist()\n                             if idx in self.phase_a_neurons)\n        phase_b_active = sum(1 for idx in active_mask.nonzero(as_tuple=True)[0].tolist()\n                             if idx in self.phase_b_neurons)\n\n        knowledge_preservation = float((active_plasticity < 0.3).sum().item()) / current_pop\n        adaptation_capacity = float((active_plasticity > 0.7).sum().item()) / current_pop\n    else:\n        diversity = 0.0\n        redundancy = 1.0\n        effective_capacity = 0.0\n        phase_a_active = 0\n        phase_b_active = 0\n        knowledge_preservation = 0.0\n        adaptation_capacity = 0.0\n\n    return {\n        'current_population': current_pop,\n        'target_population': target_pop,\n        'population_ratio': float(current_pop / max(1, self.c.max_neurons)),\n        'overpopulation': max(0, current_pop - target_pop),\n        'diversity': diversity,\n        'redundancy': redundancy,\n        'capacity_penalty': float(self._capacity_penalty),\n        'effective_capacity': float(effective_capacity / max(1, self.c.max_neurons)),\n        'phase_distribution': {\n            'phase_a': int(phase_a_active),\n            'phase_b': int(phase_b_active),\n            'unassigned': int(current_pop - phase_a_active - phase_b_active)\n        },\n        'knowledge_preservation_ratio': knowledge_preservation,\n        'adaptation_capacity_ratio': adaptation_capacity,\n        'plasticity_balance': float(1.0 - abs(knowledge_preservation - adaptation_capacity)),\n    }\n\n\n@torch.no_grad()\ndef get_continual_learning_metrics(self):\n    active_mask = self.mask > 0\n    active_indices = active_mask.nonzero(as_tuple=True)[0]\n    if active_indices.numel() == 0:\n        return {}\n\n    phase_a_list = [idx for idx in active_indices.tolist() if idx in self.phase_a_neurons]\n    phase_b_list = [idx for idx in active_indices.tolist() if idx in self.phase_b_neurons]\n\n    phase_a_count_total = len(self.phase_a_neurons) if len(self.phase_a_neurons) > 0 else 1\n    phase_a_retention = len(phase_a_list) / phase_a_count_total\n\n    phase_b_growth = len(phase_b_list) / (len(self.phase_b_neurons) + 1e-6)\n\n    cross_phase_neurons = 0\n    if hasattr(self, 'cross_phase_utility'):\n        for idx in active_indices:\n            if self.cross_phase_utility[idx] > 0.5:\n                cross_phase_neurons += 1\n\n    inheritance_active = int((self.inheritance_count[active_indices] > 0).sum().item())\n    knowledge_transfer_ratio = inheritance_active / active_indices.numel()\n\n    if len(phase_a_list) > 0:\n        pa_idx = torch.tensor(phase_a_list, device=active_indices.device, dtype=torch.long)\n        pa_plast = self.plasticity_levels[pa_idx].mean().item()\n    else:\n        pa_plast = 0.0\n\n    if len(phase_b_list) > 0:\n        pb_idx = torch.tensor(phase_b_list, device=active_indices.device, dtype=torch.long)\n        pb_plast = self.plasticity_levels[pb_idx].mean().item()\n    else:\n        pb_plast = 0.0\n\n    return {\n        'phase_a_retention': float(phase_a_retention),\n        'phase_b_growth': float(phase_b_growth),\n        'cross_phase_neurons': int(cross_phase_neurons),\n        'knowledge_transfer_ratio': float(knowledge_transfer_ratio),\n        'avg_inheritance_depth': float(self.inheritance_count[active_indices].float().mean().item()),\n        'plasticity_gradient': {\n            'phase_a': float(pa_plast),\n            'phase_b': float(pb_plast),\n            'difference': float(abs(pa_plast - pb_plast))\n        }\n    }\n\n\nPlasticLSTM.analyze_competition_dynamics = analyze_competition_dynamics\nPlasticLSTM.get_population_health = get_population_health\nPlasticLSTM.get_continual_learning_metrics = get_continual_learning_metrics\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T19:55:56.746870Z","iopub.execute_input":"2025-08-03T19:55:56.747325Z","iopub.status.idle":"2025-08-03T19:55:56.775693Z","shell.execute_reply.started":"2025-08-03T19:55:56.747307Z","shell.execute_reply":"2025-08-03T19:55:56.775126Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# 11\n@torch.no_grad()\ndef efficient_hebbian_update(self, loss_per_sample=None):\n    \"\"\"\n    Actualización hebbiana eficiente y AMP-safe.\n    - Alinea dtypes para evitar conflictos bajo autocast (fp16/fp32).\n    - Selecciona top neuronas por competitive_score.\n    - Normaliza y decae h_fast con compuertas de plasticidad.\n    \"\"\"\n    if self.c.disable_hebbian:\n        return\n\n    acts, last, _ = self._cache\n    if acts is None:\n        return\n\n    # Filtrado por buenas muestras\n    if loss_per_sample is not None:\n        good_samples = loss_per_sample < loss_per_sample.mean()\n        if not good_samples.any():\n            return\n        acts = acts[good_samples]\n        last = last[good_samples]\n\n    num_active = int(self.mask.sum())\n    if num_active > 10:\n        num_to_update = int(num_active * self.c.hebb_top_neurons_ratio)\n        top_neurons = self.competitive_score.topk(num_to_update).indices\n        update_mask = torch.zeros_like(self.mask)\n        update_mask[top_neurons] = 1\n    else:\n        update_mask = self.mask\n\n    # Centrado\n    acts_centered = acts - acts.mean(1, keepdim=True)   # (B, N)\n    last_centered = last - last.mean(1, keepdim=True)   # (B, H)\n\n    # === Arreglo AMP: alinear dtypes ===\n    compute_dtype = self.h_fast.dtype  # normalmente float16\n    acts_centered = acts_centered.to(compute_dtype)\n    last_centered = last_centered.to(compute_dtype)\n\n    plasticity_modulation = (self.plasticity_levels * update_mask).to(compute_dtype)          # (N,)\n    uniqueness_weight = (self.uniqueness_score * plasticity_modulation).unsqueeze(1).to(compute_dtype)  # (N,1)\n\n    # dW: (N, H) con einsum\n    dw = torch.einsum('bn,bh->nh', acts_centered, last_centered)\n    dw = dw / max(1, acts_centered.size(0))\n    dw = dw * uniqueness_weight\n\n    plasticity_mask = self.plasticity_factor.unsqueeze(1).to(compute_dtype)  # (N,1)\n    dw = dw * plasticity_mask\n\n    # Decaimiento dependiente de unicidad y plasticidad\n    decay_base = self.c.hebb_decay + (1 - self.c.hebb_decay) * (1 - self.uniqueness_score)\n    decay_plasticity_adjusted = decay_base + (1 - decay_base) * (1 - self.plasticity_levels)\n    decay_rate = decay_plasticity_adjusted.unsqueeze(1).to(compute_dtype)  # (N,1)\n\n    mask_expanded = plasticity_modulation.unsqueeze(1)  # (N,1) ya en compute_dtype\n    self.h_fast = self.h_fast * decay_rate + dw * mask_expanded\n\n    # Clip norm con modulación de plasticidad (AMP-safe)\n    if update_mask.sum() > 0:\n        norms = self.h_fast.norm(dim=1, keepdim=True)  # (N,1) en compute_dtype\n        norm_factor = (norms / self.c.hebb_norm_clip).clamp(min=1.0)\n        # Ajuste depende de plasticidad (convertir a compute_dtype)\n        plast_levels = self.plasticity_levels.unsqueeze(1).to(compute_dtype)\n        norm_adjustment = 1.0 + (norm_factor - 1.0) * plast_levels\n        self.h_fast = self.h_fast / norm_adjustment\n\nPlasticLSTM.hebbian_update = efficient_hebbian_update\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T19:55:56.776423Z","iopub.execute_input":"2025-08-03T19:55:56.776724Z","iopub.status.idle":"2025-08-03T19:55:56.794626Z","shell.execute_reply.started":"2025-08-03T19:55:56.776707Z","shell.execute_reply":"2025-08-03T19:55:56.793910Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# 12\ndef calculate_efficiency_metrics(self):\n    active_neurons = int(self.mask.sum().item())\n    total_neurons = self.c.max_neurons\n\n    active_params = (\n        active_neurons * self.c.lstm_hidden_size +          # W.weight\n        active_neurons +                                    # W.bias\n        active_neurons * self.c.vocab_size                  # out.weight\n    )\n\n    total_potential_params = (\n        total_neurons * self.c.lstm_hidden_size +\n        total_neurons +\n        total_neurons * self.c.vocab_size\n    )\n\n    return {\n        'active_neurons': active_neurons,\n        'total_neurons': total_neurons,\n        'neuron_utilization': active_neurons / max(1, total_neurons),\n        'active_params': active_params,\n        'total_potential_params': total_potential_params,\n        'param_efficiency': active_params / max(1, total_potential_params),\n        'memory_saved_ratio': 1 - (active_params / max(1, total_potential_params))\n    }\n\n\ndef analyze_phase_specialization(self):\n    phase_a_born = len(self.phase_a_neurons)\n    phase_b_born = len(self.phase_b_neurons)\n\n    active_mask = self.mask > 0\n    active_indices = set(active_mask.nonzero(as_tuple=True)[0].cpu().numpy())\n\n    phase_a_active = len(self.phase_a_neurons.intersection(active_indices))\n    phase_b_active = len(self.phase_b_neurons.intersection(active_indices))\n\n    results = {\n        'phase_a_born': phase_a_born,\n        'phase_a_active': phase_a_active,\n        'phase_b_born': phase_b_born,\n        'phase_b_active': phase_b_active\n    }\n\n    if phase_a_active > 0:\n        phase_a_indices = torch.tensor(list(self.phase_a_neurons.intersection(active_indices)),\n                                       device=self.mask.device, dtype=torch.long)\n        results['phase_a_avg_contrib'] = float(self.contrib[phase_a_indices].mean().item())\n    else:\n        results['phase_a_avg_contrib'] = 0.0\n\n    if phase_b_active > 0:\n        phase_b_indices = torch.tensor(list(self.phase_b_neurons.intersection(active_indices)),\n                                       device=self.mask.device, dtype=torch.long)\n        results['phase_b_avg_contrib'] = float(self.contrib[phase_b_indices].mean().item())\n    else:\n        results['phase_b_avg_contrib'] = 0.0\n\n    return results\n\n\ndef get_neuron_info(self, idx):\n    birth_events = [e for e in self.life_events if e['type'] == 'birth' and e['id'] == idx]\n    death_events = [e for e in self.life_events if e['type'] == 'death' and e['id'] == idx]\n\n    return {\n        'id': idx,\n        'age': int(self.age[idx].item()),\n        'contribution': float(self.contrib[idx].item()),\n        'uniqueness': float(self.uniqueness_score[idx].item()),\n        'competitive_score': float(self.competitive_score[idx].item()),\n        'is_active': bool(self.mask[idx].item()),\n        'birth_death': birth_events + death_events,\n        'phase': 'A' if idx in self.phase_a_neurons else ('B' if idx in self.phase_b_neurons else 'Unknown')\n    }\n\n\ndef get_inheritance_statistics(self):\n    active_mask = self.mask > 0\n    active_indices = active_mask.nonzero(as_tuple=True)[0]\n    if active_indices.numel() == 0:\n        return {}\n\n    active_inheritance = self.inheritance_count[active_indices]\n    active_sources = self.knowledge_sources[active_indices]\n\n    stats = {\n        'total_inheritance_events': len(self.inheritance_history),\n        'neurons_with_inheritance': int((active_inheritance > 0).sum().item()),\n        'avg_inheritance_per_neuron': float(active_inheritance.float().mean().item()),\n        'max_inheritance_count': int(active_inheritance.max().item()),\n        'avg_knowledge_sources': float(active_sources.float().mean().item()),\n        'inheritance_ratio': float((active_inheritance > 0).sum().item()) / active_indices.numel()\n    }\n\n    phase_a_inheritance = sum(1 for event in self.inheritance_history if event.get('phase') == 'A')\n    phase_b_inheritance = sum(1 for event in self.inheritance_history if event.get('phase') == 'B')\n\n    stats['phase_a_inheritance_events'] = int(phase_a_inheritance)\n    stats['phase_b_inheritance_events'] = int(phase_b_inheritance)\n    return stats\n\n\ndef calculate_functional_diversity(self, test_sequences=None, max_neurons_eval=512, batches=6, seq_len=32):\n    \"\"\"\n    Estima diversidad funcional midiendo la entropía del histograma de activaciones.\n    Optimizado para no crear tensores gigantes:\n      - Muestrea hasta `max_neurons_eval` neuronas activas.\n      - Usa pocas secuencias cortas (batches * seq_len).\n    \"\"\"\n    device = next(self.parameters()).device\n\n    # Selección de neuronas activas a evaluar\n    active_idx = (self.mask > 0).nonzero(as_tuple=True)[0]\n    if active_idx.numel() == 0:\n        return {\n            'mean_functional_diversity': 0.0,\n            'std_functional_diversity': 0.0,\n            'min_functional_diversity': 0.0,\n            'max_functional_diversity': 0.0\n        }\n\n    if active_idx.numel() > max_neurons_eval:\n        perm = torch.randperm(active_idx.numel(), device=active_idx.device)[:max_neurons_eval]\n        eval_idx = active_idx[perm]\n    else:\n        eval_idx = active_idx\n\n    # Construcción de secuencias de prueba si no se pasan\n    if test_sequences is None:\n        test_sequences = []\n        for _ in range(batches):\n            seq = torch.randint(0, self.c.vocab_size, (1, seq_len), device=device, dtype=torch.long)\n            test_sequences.append(seq)\n\n    # Colección de activaciones sólo para las neuronas seleccionadas\n    activations = []\n    with torch.no_grad():\n        for seq in test_sequences:\n            logits, analysis = self.forward(seq, return_analysis=True)\n            acts = analysis['activations']  # (B, N)\n            activations.append(acts[:, eval_idx].detach().float().cpu())\n\n    if not activations:\n        return {\n            'mean_functional_diversity': 0.0,\n            'std_functional_diversity': 0.0,\n            'min_functional_diversity': 0.0,\n            'max_functional_diversity': 0.0\n        }\n\n    acts_mat = torch.cat(activations, dim=0).clamp(0, 1)  # (T, K)\n\n    # Entropía por neurona con 10 bins\n    K = acts_mat.shape[1]\n    diversity_scores = []\n    for k in range(K):\n        hist = torch.histc(acts_mat[:, k], bins=10, min=0.0, max=1.0)\n        s = hist.sum()\n        if s <= 0:\n            diversity_scores.append(0.0)\n            continue\n        p = (hist / s).clamp_min(1e-10)\n        entropy = float((-p * torch.log(p)).sum().item())\n        diversity_scores.append(entropy)\n\n    diversity_scores = torch.tensor(diversity_scores)\n    return {\n        'mean_functional_diversity': float(diversity_scores.mean().item()),\n        'std_functional_diversity': float(diversity_scores.std(unbiased=False).item()),\n        'min_functional_diversity': float(diversity_scores.min().item()),\n        'max_functional_diversity': float(diversity_scores.max().item())\n    }\n\n\nPlasticLSTM.calculate_efficiency_metrics = calculate_efficiency_metrics\nPlasticLSTM.analyze_phase_specialization = analyze_phase_specialization\nPlasticLSTM.get_neuron_info = get_neuron_info\nPlasticLSTM.get_inheritance_statistics = get_inheritance_statistics\nPlasticLSTM.calculate_functional_diversity = calculate_functional_diversity\n\n\nimport json\nimport pandas as pd\n\nclass NeuronAnalyzer:\n    def __init__(self, model, cfg, val_loader, itos, model_type='plastic'):\n        self.model = model\n        self.cfg = cfg\n        self.val_loader = val_loader\n        self.itos = itos\n        self.history = defaultdict(list)\n        self.model_type = model_type\n\n        if self.model_type == 'plastic':\n            self.csv_dir = self.cfg.plastic_csv_dir\n            self.checkpoints_dir = self.cfg.plastic_checkpoints_dir\n        else:\n            self.csv_dir = os.path.join(self.cfg.baseline_save_dir, 'csv_data')\n            self.checkpoints_dir = os.path.join(self.cfg.baseline_save_dir, 'checkpoints')\n            os.makedirs(self.csv_dir, exist_ok=True)\n            os.makedirs(self.checkpoints_dir, exist_ok=True)\n\n        self.history['inheritance_events'] = []\n        self.history['neurons_with_inheritance'] = []\n        self.history['inheritance_ratio'] = []\n\n    @torch.no_grad()\n    def evaluate(self, loader, device):\n        self.model.eval()\n        total_loss, n = 0.0, 0\n\n        use_cuda = torch.cuda.is_available() and device.type == 'cuda'\n        # Elegir dtype de AMP (bf16 preferido si está soportado)\n        amp_dtype = torch.bfloat16\n        if use_cuda:\n            major, _ = torch.cuda.get_device_capability(device)\n            if major < 8:  # Ampere o menor: usar fp16\n                amp_dtype = torch.float16\n\n        for x, y in loader:\n            x = x.to(device, non_blocking=True)\n            y = y.to(device, non_blocking=True)\n            if use_cuda:\n                with torch.cuda.amp.autocast(dtype=amp_dtype):\n                    output = self.model(x)\n                    logits = output[0] if isinstance(output, tuple) else output\n                    loss = F.cross_entropy(logits, y[:, -1])\n            else:\n                output = self.model(x)\n                logits = output[0] if isinstance(output, tuple) else output\n                loss = F.cross_entropy(logits, y[:, -1])\n\n            bs = x.size(0)\n            total_loss += float(loss.item()) * bs\n            n += bs\n\n        avg_loss = total_loss / max(1, n)\n        ppl = math.exp(avg_loss) if avg_loss < 700 else float('inf')\n        return avg_loss, ppl\n\n    def log_metrics(self, step, loss, ppl, active_neurons, phase='A'):\n        self.history['step'].append(step)\n        self.history['loss'].append(loss)\n        self.history['ppl'].append(ppl)\n        self.history['active_neurons'].append(active_neurons)\n        self.history['active'].append(active_neurons)\n        self.history['phase'].append(phase)\n        if self.model_type == 'plastic' and self.cfg.adaptive_thresholds:\n            self.history['birth_threshold'].append(float(self.model.current_birth_threshold))\n            self.history['death_threshold'].append(float(self.model.current_death_threshold))\n\n    def log_inheritance_metrics(self, step):\n        if hasattr(self.model, 'get_inheritance_statistics'):\n            inheritance_stats = self.model.get_inheritance_statistics()\n            self.history['inheritance_events'].append(inheritance_stats.get('total_inheritance_events', 0))\n            self.history['neurons_with_inheritance'].append(inheritance_stats.get('neurons_with_inheritance', 0))\n            self.history['inheritance_ratio'].append(inheritance_stats.get('inheritance_ratio', 0.0))\n\n    def save_plot(self, fig, name):\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        filename = f\"{self.cfg.plots_dir}/{name}_{timestamp}.html\"\n        fig.write_html(filename)\n        print(f\" Saved plot: {filename}\")\n        return filename\n\n    def save_matplotlib_plot(self, name):\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        filename = f\"{self.cfg.plots_dir}/{name}_{timestamp}.png\"\n        plt.savefig(filename, dpi=150, bbox_inches='tight')\n        print(f\" Saved plot: {filename}\")\n        return filename\n\n    def save_all_data_to_csv(self):\n        print(f\"\\nGuardando datos en CSV en: {self.csv_dir}\")\n        df_history = pd.DataFrame(self.history)\n        df_history.to_csv(f\"{self.csv_dir}/training_history.csv\", index=False)\n        print(\" - training_history.csv guardado.\")\n\n        if hasattr(self.model, 'generation_history') and self.model.generation_history:\n            gen_data = []\n            for step, samples in self.model.generation_history.items():\n                for sample in samples:\n                    gen_data.append({\n                        'step': step,\n                        'prompt': sample.get('prompt', ''),\n                        'generated_text': sample.get('generated_text', ''),\n                        'repetition_rate': sample.get('repetition_rate', 0.0)\n                    })\n            if gen_data:\n                df_gen = pd.DataFrame(gen_data)\n                df_gen.to_csv(f\"{self.csv_dir}/generation_samples.csv\", index=False)\n                print(\" - generation_samples.csv guardado.\")\n\n        if self.model_type == 'plastic':\n            if hasattr(self.model, 'life_events') and self.model.life_events:\n                df_life = pd.DataFrame(self.model.life_events)\n                df_life.to_csv(f\"{self.csv_dir}/neuron_life_events.csv\", index=False)\n                print(\" - neuron_life_events.csv guardado.\")\n            if hasattr(self.model, 'inheritance_history') and self.model.inheritance_history:\n                df_inheritance = pd.DataFrame(self.model.inheritance_history)\n                df_inheritance.to_csv(f\"{self.csv_dir}/inheritance_history.csv\", index=False)\n                print(\" - inheritance_history.csv guardado.\")\n        print(\"Todos los datos CSV han sido guardados.\")\n\n    def generate_final_report(self):\n        print(\"\\n\" + \"=\"*80)\n        print(f\"{self.model_type.upper()} LSTM - REPORTE NUMÉRICO FINAL\")\n        print(\"=\"*80)\n\n        final_ppl = float(self.history['ppl'][-1]) if self.history['ppl'] else float('nan')\n        print(f\"\\n[1] RENDIMIENTO:\")\n        print(f\"  - Perplejidad Final: {final_ppl:.2f}\")\n\n        phase_a_ppls = [p for p, ph in zip(self.history['ppl'], self.history['phase']) if ph == 'A']\n        phase_b_ppls = [p for p, ph in zip(self.history['ppl'], self.history['phase']) if ph == 'B']\n        if phase_a_ppls and phase_b_ppls:\n            print(f\"  - PPL Final (Fase A): {phase_a_ppls[-1]:.2f}\")\n            print(f\"  - PPL Inicial (Fase B): {phase_b_ppls[0]:.2f}\")\n            print(f\"  - PPL Final (Fase B): {phase_b_ppls[-1]:.2f}\")\n            print(f\"  - Mejora por Adaptación (Caída de PPL en Fase B): {phase_b_ppls[0] - phase_b_ppls[-1]:.2f} puntos\")\n\n        if self.model_type == 'plastic':\n            eff_metrics = self.model.calculate_efficiency_metrics()\n            spec_metrics = self.model.analyze_phase_specialization()\n\n            print(\"\\n[2] EFICIENCIA:\")\n            print(f\"  - Neuronas Activas: {int(eff_metrics['active_neurons'])} / {self.cfg.max_neurons} ({eff_metrics['neuron_utilization'] * 100:.2f}%)\")\n            print(f\"  - Parámetros Activos: {int(eff_metrics['active_params']):,}\")\n            print(f\"  - Ahorro de Memoria Estimado: {eff_metrics['memory_saved_ratio'] * 100:.2f}%\")\n\n            print(\"\\n[3] DINÁMICA DE POBLACIÓN:\")\n            print(f\"  - Supervivencia Fase A: {spec_metrics.get('phase_a_active', 0)} de {spec_metrics.get('phase_a_born', 0)} nacidas.\")\n            print(f\"  - Supervivencia Fase B: {spec_metrics.get('phase_b_active', 0)} de {spec_metrics.get('phase_b_born', 0)} nacidas.\")\n\n        print(\"\\n\" + \"=\"*80)\n\n    def save_checkpoint(self):\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        checkpoint_path = f\"{self.checkpoints_dir}/{self.model_type}_lstm_{timestamp}.pt\"\n        config_dict = {k: v for k, v in vars(self.cfg).items() if not k.startswith('__') and not callable(v)}\n        torch.save({'model_state_dict': self.model.state_dict(), 'config': config_dict}, checkpoint_path)\n        print(f\"\\nCheckpoint del modelo guardado en: {checkpoint_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T19:55:56.795346Z","iopub.execute_input":"2025-08-03T19:55:56.795574Z","iopub.status.idle":"2025-08-03T19:55:56.831778Z","shell.execute_reply.started":"2025-08-03T19:55:56.795548Z","shell.execute_reply":"2025-08-03T19:55:56.831168Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import json\nimport pandas as pd\n\nclass NeuronAnalyzer:\n    def __init__(self, model, cfg, val_loader, itos, model_type='plastic'):\n        self.model = model\n        self.cfg = cfg\n        self.val_loader = val_loader\n        self.itos = itos\n        self.history = defaultdict(list)\n        self.model_type = model_type\n\n        if self.model_type == 'plastic':\n            self.csv_dir = self.cfg.plastic_csv_dir\n            self.checkpoints_dir = self.cfg.plastic_checkpoints_dir\n        else:\n            self.csv_dir = os.path.join(self.cfg.baseline_save_dir, 'csv_data')\n            self.checkpoints_dir = os.path.join(self.cfg.baseline_save_dir, 'checkpoints')\n            os.makedirs(self.csv_dir, exist_ok=True)\n            os.makedirs(self.checkpoints_dir, exist_ok=True)\n\n        self.history['inheritance_events'] = []\n        self.history['neurons_with_inheritance'] = []\n        self.history['inheritance_ratio'] = []\n\n    @torch.no_grad()\n    def evaluate(self, loader, device):\n        self.model.eval()\n        total_loss, n = 0, 0\n        for x, y in loader:\n            x, y = x.to(device), y.to(device)\n            output = self.model(x)\n            logits = output[0] if isinstance(output, tuple) else output\n            loss = F.cross_entropy(logits, y[:, -1])\n            total_loss += loss.item() * x.size(0)\n            n += x.size(0)\n        avg_loss = total_loss / max(1, n)\n        ppl = math.exp(avg_loss) if avg_loss < 700 else float('inf')\n        return avg_loss, ppl\n\n    def log_metrics(self, step, loss, ppl, active_neurons, phase='A'):\n        self.history['step'].append(step)\n        self.history['loss'].append(loss)\n        self.history['ppl'].append(ppl)\n        self.history['active_neurons'].append(active_neurons)\n        self.history['active'].append(active_neurons)\n        self.history['phase'].append(phase)\n        if self.model_type == 'plastic' and self.cfg.adaptive_thresholds:\n            self.history['birth_threshold'].append(self.model.current_birth_threshold)\n            self.history['death_threshold'].append(self.model.current_death_threshold)\n\n    def log_inheritance_metrics(self, step):\n        if hasattr(self.model, 'get_inheritance_statistics'):\n            inheritance_stats = self.model.get_inheritance_statistics()\n            self.history['inheritance_events'].append(inheritance_stats.get('total_inheritance_events', 0))\n            self.history['neurons_with_inheritance'].append(inheritance_stats.get('neurons_with_inheritance', 0))\n            self.history['inheritance_ratio'].append(inheritance_stats.get('inheritance_ratio', 0))\n\n    def save_plot(self, fig, name):\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        filename = f\"{self.cfg.plots_dir}/{name}_{timestamp}.html\"\n        fig.write_html(filename)\n        print(f\" Saved plot: {filename}\")\n        return filename\n\n    def save_matplotlib_plot(self, name):\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        filename = f\"{self.cfg.plots_dir}/{name}_{timestamp}.png\"\n        plt.savefig(filename, dpi=150, bbox_inches='tight')\n        print(f\" Saved plot: {filename}\")\n        return filename\n\n    def save_all_data_to_csv(self):\n        print(f\"\\nGuardando datos en CSV en: {self.csv_dir}\")\n        df_history = pd.DataFrame(self.history)\n        df_history.to_csv(f\"{self.csv_dir}/training_history.csv\", index=False)\n        print(\" - training_history.csv guardado.\")\n        if hasattr(self.model, 'generation_history') and self.model.generation_history:\n            gen_data = []\n            for step, samples in self.model.generation_history.items():\n                for sample in samples:\n                    gen_data.append({\n                        'step': step,\n                        'prompt': sample.get('prompt', ''),\n                        'generated_text': sample.get('generated_text', ''),\n                        'repetition_rate': sample.get('repetition_rate', 0.0)\n                    })\n            if gen_data:\n                df_gen = pd.DataFrame(gen_data)\n                df_gen.to_csv(f\"{self.csv_dir}/generation_samples.csv\", index=False)\n                print(\" - generation_samples.csv guardado.\")\n        if self.model_type == 'plastic':\n            if hasattr(self.model, 'life_events') and self.model.life_events:\n                df_life = pd.DataFrame(self.model.life_events)\n                df_life.to_csv(f\"{self.csv_dir}/neuron_life_events.csv\", index=False)\n                print(\" - neuron_life_events.csv guardado.\")\n            if hasattr(self.model, 'inheritance_history') and self.model.inheritance_history:\n                df_inheritance = pd.DataFrame(self.model.inheritance_history)\n                df_inheritance.to_csv(f\"{self.csv_dir}/inheritance_history.csv\", index=False)\n                print(\" - inheritance_history.csv guardado.\")\n        print(\"Todos los datos CSV han sido guardados.\")\n\n    def generate_final_report(self):\n        print(\"\\n\" + \"=\"*80)\n        print(f\"{self.model_type.upper()} LSTM - REPORTE NUMÉRICO FINAL\")\n        print(\"=\"*80)\n        final_ppl = float(self.history['ppl'][-1]) if self.history['ppl'] else float('nan')\n        print(f\"\\n[1] RENDIMIENTO:\")\n        print(f\"  - Perplejidad Final: {final_ppl:.2f}\")\n        phase_a_ppls = [p for p, ph in zip(self.history['ppl'], self.history['phase']) if ph == 'A']\n        phase_b_ppls = [p for p, ph in zip(self.history['ppl'], self.history['phase']) if ph == 'B']\n        if phase_a_ppls and phase_b_ppls:\n            print(f\"  - PPL Final (Fase A): {phase_a_ppls[-1]:.2f}\")\n            print(f\"  - PPL Inicial (Fase B): {phase_b_ppls[0]:.2f}\")\n            print(f\"  - PPL Final (Fase B): {phase_b_ppls[-1]:.2f}\")\n            print(f\"  - Mejora por Adaptación (Caída de PPL en Fase B): {phase_b_ppls[0] - phase_b_ppls[-1]:.2f} puntos\")\n        if self.model_type == 'plastic':\n            eff_metrics = self.model.calculate_efficiency_metrics()\n            spec_metrics = self.model.analyze_phase_specialization()\n            print(\"\\n[2] EFICIENCIA:\")\n            print(f\"  - Neuronas Activas: {int(eff_metrics['active_neurons'])} / {self.cfg.max_neurons} ({eff_metrics['neuron_utilization'] * 100:.2f}%)\")\n            print(f\"  - Parámetros Activos: {int(eff_metrics['active_params']):,}\")\n            print(f\"  - Ahorro de Memoria Estimado: {eff_metrics['memory_saved_ratio'] * 100:.2f}%\")\n            print(\"\\n[3] DINÁMICA DE POBLACIÓN:\")\n            print(f\"  - Supervivencia Fase A: {spec_metrics.get('phase_a_active', 0)} de {spec_metrics.get('phase_a_born', 0)} nacidas.\")\n            print(f\"  - Supervivencia Fase B: {spec_metrics.get('phase_b_active', 0)} de {spec_metrics.get('phase_b_born', 0)} nacidas.\")\n        print(\"\\n\" + \"=\"*80)\n\n    def save_checkpoint(self):\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        checkpoint_path = f\"{self.checkpoints_dir}/{self.model_type}_lstm_{timestamp}.pt\"\n        config_dict = {k: v for k, v in vars(self.cfg).items() if not k.startswith('__') and not callable(v)}\n        torch.save({'model_state_dict': self.model.state_dict(), 'config': config_dict}, checkpoint_path)\n        print(f\"\\nCheckpoint del modelo guardado en: {checkpoint_path}\")\n\n\ndef plot_training_progress_enhanced(self):\n    fig = make_subplots(\n        rows=6, cols=1,\n        subplot_titles=('Validation Loss', 'Validation Perplexity', 'Active Neurons',\n                       'Birth/Death Events', 'Adaptive Thresholds', 'Generation Quality (Repetition Rate)'),\n        vertical_spacing=0.07,\n        shared_xaxes=True\n    )\n    if 'phase' in self.history and len(self.history['phase']) > 1:\n        phase_changes = [self.history['step'][i] for i in range(1, len(self.history['phase'])) if self.history['phase'][i] != self.history['phase'][i-1]]\n        last_step = self.history['step'][-1]\n        shapes = []\n        shapes.append(dict(type=\"rect\", xref=\"x\", yref=\"paper\", x0=0, y0=0, x1=phase_changes[0] if phase_changes else last_step, y1=1, fillcolor=\"rgba(70,70,120,0.2)\", layer=\"below\", line_width=0))\n        if phase_changes:\n            shapes.append(dict(type=\"rect\", xref=\"x\", yref=\"paper\", x0=phase_changes[0], y0=0, x1=last_step, y1=1, fillcolor=\"rgba(120,120,70,0.2)\", layer=\"below\", line_width=0))\n        fig.update_layout(shapes=shapes)\n    fig.add_trace(go.Scatter(x=self.history['step'], y=self.history['loss'], name='Loss', line=dict(color='cyan')), row=1, col=1)\n    fig.add_trace(go.Scatter(x=self.history['step'], y=self.history['ppl'], name='PPL', line=dict(color='magenta')), row=2, col=1)\n    fig.add_trace(go.Scatter(x=self.history['step'], y=self.history['active'], name='Active Neurons', line=dict(color='lime')), row=3, col=1)\n    if self.model.life_events:\n        births = [e for e in self.model.life_events if e['type'] == 'birth']\n        deaths = [e for e in self.model.life_events if e['type'] == 'death']\n        if births:\n            fig.add_trace(go.Scatter(x=[e['step'] for e in births], y=[1]*len(births), mode='markers', name='Births', marker=dict(symbol='star', size=8)), row=4, col=1)\n        if deaths:\n            fig.add_trace(go.Scatter(x=[e['step'] for e in deaths], y=[0]*len(deaths), mode='markers', name='Deaths', marker=dict(symbol='x', size=8)), row=4, col=1)\n    if self.cfg.adaptive_thresholds and 'birth_threshold' in self.history:\n        fig.add_trace(go.Scatter(x=self.history['step'], y=self.history['birth_threshold'], name='Birth Thresh', line=dict(color='orange')), row=5, col=1)\n        fig.add_trace(go.Scatter(x=self.history['step'], y=[t*100 for t in self.history['death_threshold']], name='Death Thresh (x100)', line=dict(color='tomato', dash='dash')), row=5, col=1)\n    if hasattr(self.model, 'generation_history') and self.model.generation_history:\n        steps, repetitions = [], []\n        for step, samples in sorted(self.model.generation_history.items()):\n            if samples:\n                steps.append(step)\n                avg_rep = np.mean([s['repetition_rate'] for s in samples])\n                repetitions.append(avg_rep)\n        if steps:\n            fig.add_trace(go.Scatter(x=steps, y=repetitions, name='Repetition Rate', line=dict(color='yellow'), mode='lines+markers'), row=6, col=1)\n    fig.update_layout(template='plotly_dark', height=1500, title_text=\"Enhanced Training Progress (Plastic LSTM)\")\n    fig.show()\n    self.save_plot(fig, \"training_progress_enhanced_plastic\")\n\n\ndef analyze_top_neurons(self, k=10):\n    active_mask = self.model.mask > 0\n    active_idx = active_mask.nonzero(as_tuple=True)[0]\n    if len(active_idx) == 0:\n        print(\"No active neurons!\")\n        return\n    contribs = self.model.contrib[active_idx]\n    top_idx = active_idx[contribs.argsort(descending=True)[:k]]\n    print(f\"\\nTop {k} neurons by contribution:\")\n    print(\"-\" * 80)\n    neuron_data = []\n    for rank, idx in enumerate(top_idx):\n        info = self.model.get_neuron_info(idx.item())\n        neuron_data.append(info)\n        print(f\"Rank {rank+1}: Neuron {idx.item()}\")\n        print(f\"  Age: {info['age']} steps | Contribution: {info['contribution']:.6f}\")\n        print(f\"  Born in phase: {[e['phase'] for e in info['birth_death'] if e['type'] == 'birth'][0] if info['birth_death'] else 'Initial'}\")\n        print()\n    return neuron_data\n\n\ndef generate_report(self):\n    print(\"\\n\" + \"=\"*80)\n    print(\"NEUROPLASTIC LSTM - FINAL ANALYSIS REPORT\")\n    print(\"=\"*80)\n    phase_a_births = len([e for e in self.model.life_events if e['type'] == 'birth' and e.get('phase', 'A') == 'A'])\n    phase_b_births = len([e for e in self.model.life_events if e['type'] == 'birth' and e.get('phase') == 'B'])\n    phase_a_deaths = len([e for e in self.model.life_events if e['type'] == 'death' and e.get('phase', 'A') == 'A'])\n    phase_b_deaths = len([e for e in self.model.life_events if e['type'] == 'death' and e.get('phase') == 'B'])\n    print(\"\\n[1] POPULATION DYNAMICS:\")\n    print(f\"  - Initial Neurons: {self.cfg.initial_active}\")\n    print(f\"  - Final Active Neurons: {int(self.model.mask.sum().item())}\")\n    print(f\"  - Phase A: {phase_a_births} Births, {phase_a_deaths} Deaths\")\n    print(f\"  - Phase B: {phase_b_births} Births, {phase_b_deaths} Deaths\")\n    eff_metrics = self.model.calculate_efficiency_metrics()\n    print(\"\\n[2] PARAMETER EFFICIENCY:\")\n    print(f\"  - Active Parameters: {int(eff_metrics['active_params']):,} / {int(eff_metrics['total_potential_params']):,}\")\n    print(f\"  - Utilization: {eff_metrics['param_efficiency']*100:.2f}% of potential capacity\")\n    print(f\"  - Estimated Memory Saved: {eff_metrics['memory_saved_ratio']*100:.2f}%\")\n    spec_metrics = self.model.analyze_phase_specialization()\n    print(\"\\n[3] NEURONAL SPECIALIZATION:\")\n    print(f\"  - Phase A Born & Active: {spec_metrics['phase_a_active']} / {spec_metrics['phase_a_born']} ({spec_metrics['phase_a_active']/max(1,spec_metrics['phase_a_born']):.1%} survival)\")\n    print(f\"  - Phase B Born & Active: {spec_metrics['phase_b_active']} / {spec_metrics['phase_b_born']} ({spec_metrics['phase_b_active']/max(1,spec_metrics['phase_b_born']):.1%} survival)\")\n    if 'phase_a_avg_contrib' in spec_metrics:\n        print(f\"  - Avg. Contribution (Phase A neurons): {spec_metrics.get('phase_a_avg_contrib', 0):.4f}\")\n        print(f\"  - Avg. Contribution (Phase B neurons): {spec_metrics.get('phase_b_avg_contrib', 0):.4f}\")\n    print(\"\\n\" + \"=\"*80)\n\n\ndef save_all_results(self):\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    extended_analysis = {\n        'efficiency': self.model.calculate_efficiency_metrics(),\n        'specialization': self.model.analyze_phase_specialization(),\n    }\n    checkpoint_path = f\"{self.checkpoints_dir}/plastic_lstm_{timestamp}.pt\"\n    torch.save({\n        'model_state_dict': self.model.state_dict(),\n        'life_events': self.model.life_events,\n        'generation_history': dict(self.model.generation_history),\n        'config': {k: v for k, v in self.cfg.__dict__.items() if not k.startswith('__')},\n        'history': dict(self.history),\n        'extended_analysis': extended_analysis,\n        'timestamp': timestamp\n    }, checkpoint_path)\n    print(f\"\\nSaved checkpoint: {checkpoint_path}\")\n    metrics_path = f\"{self.cfg.plots_dir}/metrics_plastic_{timestamp}.json\"\n    with open(metrics_path, 'w') as f:\n        def simple_dumper(obj):\n            if isinstance(obj, (np.integer, np.floating, np.bool_)):\n                return obj.item()\n            if isinstance(obj, set):\n                return list(obj)\n            raise TypeError(f\"Object of type {type(obj).__name__} is not JSON serializable\")\n        json.dump({\n            'history': dict(self.history),\n            'life_events': self.model.life_events,\n            'generation_history': dict(self.model.generation_history),\n            'extended_analysis': extended_analysis,\n            'timestamp': timestamp\n        }, f, indent=2, default=simple_dumper)\n    print(f\"Saved metrics: {metrics_path}\")\n\nNeuronAnalyzer.plot_training_progress_enhanced = plot_training_progress_enhanced\nNeuronAnalyzer.analyze_top_neurons = analyze_top_neurons\nNeuronAnalyzer.generate_report = generate_report\nNeuronAnalyzer.save_all_results = save_all_results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T19:55:56.832457Z","iopub.execute_input":"2025-08-03T19:55:56.832670Z","iopub.status.idle":"2025-08-03T19:55:56.870694Z","shell.execute_reply.started":"2025-08-03T19:55:56.832654Z","shell.execute_reply":"2025-08-03T19:55:56.870050Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# 14\ndef plot_training_progress_enhanced(self):\n    \"\"\"Plot mejorado con métricas de generación y mejor visualización de fases (robusto a claves faltantes).\"\"\"\n    fig = make_subplots(\n        rows=6, cols=1,\n        subplot_titles=(\n            'Validation Loss',\n            'Validation Perplexity',\n            'Active Neurons',\n            'Birth/Death Events',\n            'Adaptive Thresholds',\n            'Generation Quality (Repetition Rate)'\n        ),\n        vertical_spacing=0.07,\n        shared_xaxes=True\n    )\n\n    steps_hist = self.history.get('step', [])\n\n    # --- Fondos por fase ---\n    phases = self.history.get('phase', [])\n    if phases and len(phases) == len(steps_hist) and len(phases) > 1:\n        phase_changes = [steps_hist[i] for i in range(1, len(phases)) if phases[i] != phases[i-1]]\n        last_step = steps_hist[-1] if steps_hist else 0\n\n        shapes = []\n        shapes.append(dict(type=\"rect\", xref=\"x\", yref=\"paper\",\n                           x0=0, y0=0, x1=phase_changes[0] if phase_changes else last_step, y1=1,\n                           fillcolor=\"rgba(70,70,120,0.2)\", layer=\"below\", line_width=0))\n        if phase_changes:\n            shapes.append(dict(type=\"rect\", xref=\"x\", yref=\"paper\",\n                               x0=phase_changes[0], y0=0, x1=last_step, y1=1,\n                               fillcolor=\"rgba(120,120,70,0.2)\", layer=\"below\", line_width=0))\n        fig.update_layout(shapes=shapes)\n\n    # --- Loss ---\n    loss = self.history.get('loss', [])\n    if steps_hist and loss and len(loss) == len(steps_hist):\n        fig.add_trace(go.Scatter(x=steps_hist, y=loss, name='Loss', line=dict(color='cyan')), row=1, col=1)\n\n    # --- Perplexity ---\n    ppl = self.history.get('ppl', [])\n    if steps_hist and ppl and len(ppl) == len(steps_hist):\n        fig.add_trace(go.Scatter(x=steps_hist, y=ppl, name='PPL', line=dict(color='magenta')), row=2, col=1)\n\n    # --- Neuronas activas ---\n    active = self.history.get('active', [])\n    if steps_hist and active and len(active) == len(steps_hist):\n        fig.add_trace(go.Scatter(x=steps_hist, y=active, name='Active Neurons', line=dict(color='lime')), row=3, col=1)\n\n    # --- Nacimientos / muertes ---\n    if getattr(self, 'model', None) and getattr(self.model, 'life_events', None):\n        births = [e for e in self.model.life_events if e.get('type') == 'birth']\n        deaths = [e for e in self.model.life_events if e.get('type') == 'death']\n        if births:\n            fig.add_trace(\n                go.Scatter(\n                    x=[e.get('step', 0) for e in births],\n                    y=[1] * len(births),\n                    mode='markers',\n                    name='Births',\n                    marker=dict(color='lightgreen', symbol='star', size=8)\n                ), row=4, col=1\n            )\n        if deaths:\n            fig.add_trace(\n                go.Scatter(\n                    x=[e.get('step', 0) for e in deaths],\n                    y=[0] * len(deaths),\n                    mode='markers',\n                    name='Deaths',\n                    marker=dict(color='red', symbol='x', size=8)\n                ), row=4, col=1\n            )\n\n    # --- Umbrales adaptativos ---\n    if self.cfg.adaptive_thresholds and 'birth_threshold' in self.history and 'death_threshold' in self.history:\n        birth_th = self.history.get('birth_threshold', [])\n        death_th = self.history.get('death_threshold', [])\n        if steps_hist and birth_th and len(birth_th) == len(steps_hist):\n            fig.add_trace(go.Scatter(x=steps_hist, y=birth_th, name='Birth Thresh', line=dict(color='orange')), row=5, col=1)\n        if steps_hist and death_th and len(death_th) == len(steps_hist):\n            fig.add_trace(go.Scatter(x=steps_hist, y=[t * 100 for t in death_th], name='Death Thresh (x100)', line=dict(color='tomato', dash='dash')), row=5, col=1)\n\n    # --- Calidad de generación (repetition rate) ---\n    if hasattr(self.model, 'generation_history') and self.model.generation_history:\n        steps_rep, repetitions = [], []\n        for stp, samples in sorted(self.model.generation_history.items()):\n            if samples:\n                steps_rep.append(stp)\n                repetitions.append(float(np.mean([s.get('repetition_rate', 0.0) for s in samples])))\n        if steps_rep:\n            fig.add_trace(\n                go.Scatter(x=steps_rep, y=repetitions, name='Repetition Rate', line=dict(color='yellow'), mode='lines+markers'),\n                row=6, col=1\n            )\n\n    fig.update_layout(template='plotly_dark', height=1500, title_text=\"Enhanced Training Progress (Plastic LSTM)\")\n    fig.show()\n    self.save_plot(fig, \"training_progress_enhanced_plastic\")\n\n# Asociar a la clase\nNeuronAnalyzer.plot_training_progress_enhanced = plot_training_progress_enhanced\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T19:55:56.871828Z","iopub.execute_input":"2025-08-03T19:55:56.872083Z","iopub.status.idle":"2025-08-03T19:55:56.896952Z","shell.execute_reply.started":"2025-08-03T19:55:56.872062Z","shell.execute_reply":"2025-08-03T19:55:56.896263Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# 15\ndef analyze_top_neurons(self, k=10):\n    active_mask = self.model.mask > 0\n    active_idx = active_mask.nonzero(as_tuple=True)[0]\n    if len(active_idx) == 0:\n        print(\"No active neurons!\")\n        return\n    contribs = self.model.contrib[active_idx]\n    top_idx = active_idx[contribs.argsort(descending=True)[:k]]\n    print(f\"\\nTop {k} neurons by contribution:\")\n    print(\"-\" * 80)\n    neuron_data = []\n    for rank, idx in enumerate(top_idx):\n        info = self.model.get_neuron_info(idx.item())\n        neuron_data.append(info)\n        print(f\"Rank {rank+1}: Neuron {idx.item()}\")\n        print(f\"  Age: {info['age']} steps | Contribution: {info['contribution']:.6f}\")\n        print(f\"  Born in phase: {[e['phase'] for e in info['birth_death'] if e['type'] == 'birth'][0] if info['birth_death'] else 'Initial'}\")\n        print()\n    return neuron_data\n\ndef generate_report(self):\n    print(\"\\n\" + \"=\"*80)\n    print(\"NEUROPLASTIC LSTM - FINAL ANALYSIS REPORT\")\n    print(\"=\"*80)\n    phase_a_births = len([e for e in self.model.life_events if e['type'] == 'birth' and e.get('phase', 'A') == 'A'])\n    phase_b_births = len([e for e in self.model.life_events if e['type'] == 'birth' and e.get('phase') == 'B'])\n    phase_a_deaths = len([e for e in self.model.life_events if e['type'] == 'death' and e.get('phase', 'A') == 'A'])\n    phase_b_deaths = len([e for e in self.model.life_events if e['type'] == 'death' and e.get('phase') == 'B'])\n    print(\"\\n[1] POPULATION DYNAMICS:\")\n    print(f\"  - Initial Neurons: {self.cfg.initial_active}\")\n    print(f\"  - Final Active Neurons: {int(self.model.mask.sum().item())}\")\n    print(f\"  - Phase A: {phase_a_births} Births, {phase_a_deaths} Deaths\")\n    print(f\"  - Phase B: {phase_b_births} Births, {phase_b_deaths} Deaths\")\n    eff_metrics = self.model.calculate_efficiency_metrics()\n    print(\"\\n[2] PARAMETER EFFICIENCY:\")\n    print(f\"  - Active Parameters: {int(eff_metrics['active_params']):,} / {int(eff_metrics['total_potential_params']):,}\")\n    print(f\"  - Utilization: {eff_metrics['param_efficiency']*100:.2f}% of potential capacity\")\n    print(f\"  - Estimated Memory Saved: {eff_metrics['memory_saved_ratio']*100:.2f}%\")\n    spec_metrics = self.model.analyze_phase_specialization()\n    print(\"\\n[3] NEURONAL SPECIALIZATION:\")\n    print(f\"  - Phase A Born & Active: {spec_metrics['phase_a_active']} / {spec_metrics['phase_a_born']} ({spec_metrics['phase_a_active']/max(1, spec_metrics['phase_a_born']):.1%} survival)\")\n    print(f\"  - Phase B Born & Active: {spec_metrics['phase_b_active']} / {spec_metrics['phase_b_born']} ({spec_metrics['phase_b_active']/max(1, spec_metrics['phase_b_born']):.1%} survival)\")\n    if 'phase_a_avg_contrib' in spec_metrics:\n        print(f\"  - Avg. Contribution (Phase A neurons): {spec_metrics.get('phase_a_avg_contrib', 0):.4f}\")\n        print(f\"  - Avg. Contribution (Phase B neurons): {spec_metrics.get('phase_b_avg_contrib', 0):.4f}\")\n    print(\"\\n\" + \"=\"*80)\n\ndef save_all_results(self):\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    extended_analysis = {\n        'efficiency': self.model.calculate_efficiency_metrics(),\n        'specialization': self.model.analyze_phase_specialization(),\n    }\n    checkpoint_path = f\"{self.checkpoints_dir}/plastic_lstm_{timestamp}.pt\"\n    torch.save({\n        'model_state_dict': self.model.state_dict(),\n        'life_events': self.model.life_events,\n        'generation_history': dict(self.model.generation_history),\n        'config': {k: v for k, v in self.cfg.__dict__.items() if not k.startswith('__')},\n        'history': dict(self.history),\n        'extended_analysis': extended_analysis,\n        'timestamp': timestamp\n    }, checkpoint_path)\n    print(f\"\\nSaved checkpoint: {checkpoint_path}\")\n    metrics_path = f\"{self.cfg.plots_dir}/metrics_plastic_{timestamp}.json\"\n    with open(metrics_path, 'w') as f:\n        def simple_dumper(obj):\n            if isinstance(obj, (np.integer, np.floating, np.bool_)):\n                return obj.item()\n            if isinstance(obj, set):\n                return list(obj)\n            raise TypeError(f\"Object of type {type(obj).__name__} is not JSON serializable\")\n        json.dump({\n            'history': dict(self.history),\n            'life_events': self.model.life_events,\n            'generation_history': dict(self.model.generation_history),\n            'extended_analysis': extended_analysis,\n            'timestamp': timestamp\n        }, f, indent=2, default=simple_dumper)\n    print(f\"Saved metrics: {metrics_path}\")\n\nNeuronAnalyzer.analyze_top_neurons = analyze_top_neurons\nNeuronAnalyzer.generate_report = generate_report\nNeuronAnalyzer.save_all_results = save_all_results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T19:55:56.897715Z","iopub.execute_input":"2025-08-03T19:55:56.898281Z","iopub.status.idle":"2025-08-03T19:55:56.917696Z","shell.execute_reply.started":"2025-08-03T19:55:56.898236Z","shell.execute_reply":"2025-08-03T19:55:56.916945Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Celda 16\ndef train_plastic_lstm(cfg):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Usando dispositivo: {device}\")\n\n    use_amp = getattr(cfg, \"use_amp\", True) and device.type == \"cuda\"\n    grad_clip = float(getattr(cfg, \"grad_clip\", 1.0))\n    amp_dtype = torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n\n    train_loader, val_loader, stoi, itos, _ = build_loaders(cfg, phase='A')\n    model = PlasticLSTM(cfg).to(device)\n    model.stoi = stoi\n    model.itos = itos\n    print(f\"Plastic LSTM inicializado con {cfg.initial_active} neuronas activas.\")\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate)\n    replay_buffer = ReplayBuffer(cfg.replay_size)\n    analyzer = NeuronAnalyzer(model, cfg, val_loader, itos, model_type='plastic')\n    forgetting_tracker = CatastrophicForgettingTracker(cfg)\n\n    initial_ppl_a = None\n    step = 0\n    model.train()\n    phase_b_triggered = False\n\n    population_history = []\n    diversity_history = []\n    continual_learning_history = []\n\n    last_time = time.time()\n    tokens_counted = 0\n    tokens_per_step = cfg.batch_size * cfg.sequence_length\n\n    for epoch in range(cfg.num_epochs):\n        print(f\"\\n{'='*60}\\nÉpoca {epoch+1}/{cfg.num_epochs}\\n{'='*60}\")\n        current_loader = train_loader\n        pbar = tqdm(current_loader, desc=f'Época {epoch+1}')\n\n        for batch_idx, (x, y) in enumerate(pbar):\n            step += 1\n\n            # Transición a Fase B\n            if step >= cfg.phase_b_start and not phase_b_triggered:\n                print(f\"\\n FASE B: Inyectando nuevos datos en el paso {step}\")\n\n                forgetting_tracker.save_phase_checkpoint(model, 'A', step, device)\n\n                with torch.cuda.amp.autocast(dtype=amp_dtype, enabled=use_amp):\n                    phase_a_results, _, _ = forgetting_tracker.evaluate_phase_specific_performance(\n                        model, stoi, itos, device, 'A', step\n                    )\n                forgetting_tracker.phase_performance['A']['final_perplexity'] = phase_a_results['A']['mean_perplexity']\n\n                model.set_phase('B')\n\n                # --- CAMBIO 1: Reducir tasa de aprendizaje ---\n                print(\"!! REDUCIENDO TASA DE APRENDIZAJE PARA FASE B !!\")\n                for param_group in optimizer.param_groups:\n                    param_group['lr'] *= 0.1\n                # ----------------------------------------------\n\n                train_loader_b, val_loader_b, _, _, _ = build_loaders(\n                    cfg, phase='B', existing_stoi=stoi, existing_itos=itos\n                )\n                current_loader, val_loader = train_loader_b, val_loader_b\n                analyzer.val_loader = val_loader_b\n                phase_b_triggered = True\n                pbar.close()\n                pbar = tqdm(current_loader, desc=f'Época {epoch+1} - Fase B', initial=batch_idx)\n\n            x = x.to(device, non_blocking=True)\n            y = y.to(device, non_blocking=True)\n\n            optimizer.zero_grad(set_to_none=True)\n\n            with torch.cuda.amp.autocast(dtype=amp_dtype, enabled=use_amp):\n                logits = model(x)\n                loss_per_sample = F.cross_entropy(logits, y[:, -1], reduction='none')\n                loss = loss_per_sample.mean()\n                total_loss = loss + model._capacity_penalty\n\n            if use_amp:\n                scaler.scale(total_loss).backward()\n                scaler.unscale_(optimizer)\n                if grad_clip is not None and grad_clip > 0:\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                total_loss.backward()\n                if grad_clip is not None and grad_clip > 0:\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n                optimizer.step()\n\n            model.hebbian_update(loss_per_sample.detach())\n            if step % cfg.plasticity_interval == 0:\n                model.structural_update(loss.item(), loss_per_sample.detach())\n\n            # Replay buffer\n            replay_buffer.push(x, y)\n\n            if step > 100 and step % 10 == 0:\n                replay_samples = replay_buffer.sample(n=15)\n                if replay_samples:\n                    for replay_sample in replay_samples:\n                        rx, ry = replay_sample\n                        rx = rx.to(device, non_blocking=True)\n                        ry = ry.to(device, non_blocking=True)\n                        optimizer.zero_grad(set_to_none=True)\n                        with torch.cuda.amp.autocast(dtype=amp_dtype, enabled=use_amp):\n                            r_logits = model(rx)\n                            r_loss = F.cross_entropy(r_logits, ry[:, -1])\n\n                            # --- CAMBIO 2: Ponderar pérdida del replay ---\n                            replay_loss_weight = 200.0\n                            r_total_loss = (r_loss * replay_loss_weight) + model._capacity_penalty\n                            # ---------------------------------------------\n\n                        if use_amp:\n                            scaler.scale(r_total_loss).backward()\n                            scaler.unscale_(optimizer)\n                            if grad_clip is not None and grad_clip > 0:\n                                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n                            scaler.step(optimizer)\n                            scaler.update()\n                        else:\n                            r_total_loss.backward()\n                            if grad_clip is not None and grad_clip > 0:\n                                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n                            optimizer.step()\n\n            tokens_counted += tokens_per_step\n            now = time.time()\n            elapsed = now - last_time\n            tps = tokens_counted / max(1e-6, elapsed)\n\n            if step % cfg.eval_every_steps == 0:\n                with torch.cuda.amp.autocast(dtype=amp_dtype, enabled=use_amp):\n                    val_loss, val_ppl = analyzer.evaluate(val_loader, device)\n                model.train()\n\n                if initial_ppl_a is None and not phase_b_triggered:\n                    initial_ppl_a = val_ppl\n\n                active_neurons = int(model.mask.sum().item())\n                analyzer.log_metrics(step, val_loss, val_ppl, active_neurons,\n                                     phase='B' if phase_b_triggered else 'A')\n\n                analyzer.log_inheritance_metrics(step)\n\n                if step % 500 == 0 and step > 0:\n                    model.soft_checkpoints.append({\n                        'weights': model.W.weight.clone(),\n                        'mask': model.mask.clone(),\n                        'performance': 1.0 / (val_ppl + 1e-6),\n                        'step': step\n                    })\n\n                if step % 2000 == 0 and len(model.soft_checkpoints) >= 3:\n                    if hasattr(model, '_apply_ensemble_knowledge'): _apply_ensemble_knowledge(model)\n\n                pop_health = model.get_population_health()\n                population_history.append({\n                    'step': step,\n                    'population': pop_health['current_population'],\n                    'target': pop_health['target_population'],\n                    'diversity': pop_health['diversity'],\n                    'penalty': pop_health['capacity_penalty'],\n                    'effective_capacity': pop_health['effective_capacity'],\n                    'knowledge_preservation': pop_health['knowledge_preservation_ratio'],\n                    'adaptation_capacity': pop_health['adaptation_capacity_ratio']\n                })\n\n                if step % 1000 == 0:\n                    with torch.cuda.amp.autocast(dtype=amp_dtype, enabled=use_amp):\n                        cf_results, retention, activations = forgetting_tracker.evaluate_phase_specific_performance(\n                            model, stoi, itos, device, 'B' if phase_b_triggered else 'A', step\n                        )\n\n                    empirical_metrics = forgetting_tracker.calculate_empirical_metrics(\n                        model, 'B' if phase_b_triggered else 'A', step\n                    )\n\n                    continual_metrics = model.get_continual_learning_metrics()\n                    continual_learning_history.append({\n                        'step': step,\n                        'phase': 'B' if phase_b_triggered else 'A',\n                        **continual_metrics\n                    })\n\n                    if phase_b_triggered:\n                        model.update_plasticity_levels(\n                            phase_a_performance=cf_results.get('A', {}).get('mean_perplexity'),\n                            phase_b_performance=cf_results.get('B', {}).get('mean_perplexity')\n                        )\n\n                        print(f\"\\n[Métricas Empíricas - Paso {step}]\")\n                        print(f\"  Retención Fase A: {empirical_metrics['retencion_fase_a']:.1f}%\")\n                        print(f\"  Interferencia B→A: {empirical_metrics['interferencia_B']:.3f}\")\n                        print(f\"  Costo de oportunidad: {empirical_metrics['costo_oportunidad']:.3f}\")\n                        print(f\"  Transferencia de conocimiento: {continual_metrics['knowledge_transfer_ratio']:.2%}\")\n\n                if step % 2000 == 0:\n                    diversity_metrics = model.calculate_functional_diversity()\n                    diversity_history.append({\n                        'step': step,\n                        'mean_diversity': diversity_metrics['mean_functional_diversity'],\n                        'std_diversity': diversity_metrics['std_functional_diversity']\n                    })\n\n                pbar.set_postfix({\n                    'loss': f'{loss.item():.3f}',\n                    'val_ppl': f'{val_ppl:.1f}',\n                    'neurons': f'{active_neurons}/{model.get_adaptive_population_target(loss.item(), model.current_phase)}',\n                    'penalty': f'{model._capacity_penalty:.4f}',\n                    'tok/s': f'{tps:,.0f}'\n                })\n\n                if step % (cfg.eval_every_steps * 5) == 0:\n                    samples = save_generation_samples(\n                        model, epoch, step, stoi, itos, device, cfg, \"PlasticLSTM\"\n                    )\n                    model.generation_history[step] = samples\n\n                    if step % (cfg.eval_every_steps * 10) == 0:\n                        comp_stats = model.analyze_competition_dynamics()\n                        inherit_stats = model.get_inheritance_statistics()\n\n                        print(f\"\\n[Estadísticas del Sistema - Paso {step}]\")\n                        print(f\"  Plasticidad promedio: {comp_stats['avg_plasticity']:.3f}\")\n                        print(f\"  Distribución: Baja={comp_stats['plasticity_distribution']['low']}, \"\n                              f\"Media={comp_stats['plasticity_distribution']['medium']}, \"\n                              f\"Alta={comp_stats['plasticity_distribution']['high']}\")\n                        print(f\"  Balance de plasticidad: {pop_health['plasticity_balance']:.3f}\")\n\n                last_time = time.time()\n                tokens_counted = 0\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"ENTRENAMIENTO COMPLETO - ANÁLISIS FINAL\")\n    print(\"=\"*60)\n\n    analyzer.generate_final_report()\n\n    final_comp = model.analyze_competition_dynamics()\n    final_health = model.get_population_health()\n    final_inheritance = model.get_inheritance_statistics()\n    final_continual = model.get_continual_learning_metrics()\n\n    print(\"\\n[4] SISTEMA COMPETITIVO:\")\n    print(f\"  - Población Final: {final_health['current_population']} / {final_health['target_population']} objetivo\")\n    print(f\"  - Capacidad efectiva: {final_health['effective_capacity']*100:.1f}%\")\n    print(f\"  - Distribución de plasticidad:\")\n    print(f\"    • Baja (preservación): {final_comp['plasticity_distribution']['low']} neuronas\")\n    print(f\"    • Media (mixta): {final_comp['plasticity_distribution']['medium']} neuronas\")\n    print(f\"    • Alta (adaptación): {final_comp['plasticity_distribution']['high']} neuronas\")\n\n    print(\"\\n[5] APRENDIZAJE CONTINUO:\")\n    print(f\"  - Retención Fase A: {final_continual['phase_a_retention']*100:.1f}%\")\n    print(f\"  - Crecimiento Fase B: {final_continual['phase_b_growth']*100:.1f}%\")\n    print(f\"  - Neuronas cross-phase: {final_continual['cross_phase_neurons']}\")\n    print(f\"  - Gradiente de plasticidad A→B: {final_continual['plasticity_gradient']['difference']:.3f}\")\n\n    analyzer.save_all_data_to_csv()\n\n    df_population = pd.DataFrame(population_history)\n    df_population.to_csv(f\"{cfg.plastic_csv_dir}/population_dynamics.csv\", index=False)\n\n    forgetting_tracker.save_forgetting_metrics_to_csv(cfg.plastic_csv_dir)\n\n    if continual_learning_history:\n        df_continual = pd.DataFrame(continual_learning_history)\n        df_continual.to_csv(f\"{cfg.plastic_csv_dir}/continual_learning_metrics.csv\", index=False)\n\n    if diversity_history:\n        df_diversity = pd.DataFrame(diversity_history)\n        df_diversity.to_csv(f\"{cfg.plastic_csv_dir}/functional_diversity.csv\", index=False)\n\n    analyzer.save_checkpoint()\n\n    return model, analyzer, initial_ppl_a\n\n\ndef _apply_ensemble_knowledge(model):\n    if len(model.soft_checkpoints) < 2:\n        return\n\n    perfs = torch.tensor([cp['performance'] for cp in model.soft_checkpoints], device=model.W.weight.device)\n    weights = torch.softmax(perfs * 10, dim=0)\n\n    poor_performers = model.competitive_score < model.competitive_score.median()\n    high_plasticity = model.plasticity_levels > 0.7\n    eligible = poor_performers & high_plasticity\n\n    idxs = eligible.nonzero().squeeze()\n    if idxs.numel() == 0:\n        return\n\n    for idx in idxs:\n        if model.mask[idx] > 0:\n            ensemble_weight = 0.0\n            total_w = 0.0\n            for w, cp in zip(weights, model.soft_checkpoints):\n                if cp['mask'][idx] > 0:\n                    ensemble_weight = ensemble_weight + (w * cp['weights'][idx].to(model.W.weight.device))\n                    total_w = total_w + w\n            if isinstance(ensemble_weight, torch.Tensor) and ensemble_weight.abs().sum() > 0:\n                blend_factor = 0.2 * model.plasticity_levels[idx]\n                model.W.weight.data[idx] = (1 - blend_factor) * model.W.weight.data[idx] + blend_factor * ensemble_weight\n\n\nif __name__ == \"__main__\":\n    plastic_model, plastic_analyzer, plastic_initial_ppl = train_plastic_lstm(cfg)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T19:55:56.918521Z","iopub.execute_input":"2025-08-03T19:55:56.918783Z","iopub.status.idle":"2025-08-04T01:58:21.957500Z","shell.execute_reply.started":"2025-08-03T19:55:56.918761Z","shell.execute_reply":"2025-08-04T01:58:21.956676Z"}},"outputs":[{"name":"stdout","text":"Usando dispositivo: cuda\nLoading text for Phase A...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/4188485343.py:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n","output_type":"stream"},{"name":"stdout","text":"Loaded 314.6M characters for Phase A\nVocabulary size: 4933\nPlastic LSTM inicializado con 256 neuronas activas.\n\n============================================================\nÉpoca 1/6\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Época 1:   0%|          | 0/34559 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08367055e105479484a6206d760d1e8d"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/4188485343.py:77: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(dtype=amp_dtype, enabled=use_amp):\n/tmp/ipykernel_36/4188485343.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(dtype=amp_dtype, enabled=use_amp):\n/tmp/ipykernel_36/4188485343.py:139: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(dtype=amp_dtype, enabled=use_amp):\n/tmp/ipykernel_36/4188485343.py:176: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(dtype=amp_dtype, enabled=use_amp):\n/tmp/ipykernel_36/3802716830.py:81: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(dtype=amp_dtype):\n","output_type":"stream"},{"name":"stdout","text":"\n[Estadísticas del Sistema - Paso 2000]\n  Plasticidad promedio: 1.000\n  Distribución: Baja=0, Media=0, Alta=256\n  Balance de plasticidad: 0.000\n\n[Estadísticas del Sistema - Paso 4000]\n  Plasticidad promedio: 1.000\n  Distribución: Baja=0, Media=0, Alta=258\n  Balance de plasticidad: 0.000\n\n FASE B: Inyectando nuevos datos en el paso 6000\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/4188485343.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(dtype=amp_dtype, enabled=use_amp):\n","output_type":"stream"},{"name":"stdout","text":"Model phase set to: B\n!! REDUCIENDO TASA DE APRENDIZAJE PARA FASE B !!\nLoading text for Phase B...\nLoaded 62.9M characters for Phase B\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Época 1 - Fase B:  87%|########6 | 5999/6911 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90a77cf0ffa849bc866c223d7d70d576"}},"metadata":{}},{"name":"stdout","text":"\n[Métricas Empíricas - Paso 6000]\n  Retención Fase A: 22.8%\n  Interferencia B→A: 0.322\n  Costo de oportunidad: 0.741\n  Transferencia de conocimiento: 0.00%\n\n[Estadísticas del Sistema - Paso 6000]\n  Plasticidad promedio: 0.717\n  Distribución: Baja=79, Media=0, Alta=186\n  Balance de plasticidad: 0.000\n\n[Métricas Empíricas - Paso 7000]\n  Retención Fase A: 26.7%\n  Interferencia B→A: 0.329\n  Costo de oportunidad: 0.729\n  Transferencia de conocimiento: 0.00%\n\n[Métricas Empíricas - Paso 8000]\n  Retención Fase A: 19.8%\n  Interferencia B→A: 0.346\n  Costo de oportunidad: 0.714\n  Transferencia de conocimiento: 0.00%\n\n[Estadísticas del Sistema - Paso 8000]\n  Plasticidad promedio: 0.232\n  Distribución: Baja=237, Media=0, Alta=56\n  Balance de plasticidad: 0.922\n\n[Métricas Empíricas - Paso 9000]\n  Retención Fase A: 12.0%\n  Interferencia B→A: 0.348\n  Costo de oportunidad: 0.707\n  Transferencia de conocimiento: 0.00%\n\n[Métricas Empíricas - Paso 10000]\n  Retención Fase A: 14.9%\n  Interferencia B→A: 0.357\n  Costo de oportunidad: 0.689\n  Transferencia de conocimiento: 0.00%\n\n[Estadísticas del Sistema - Paso 10000]\n  Plasticidad promedio: 0.197\n  Distribución: Baja=264, Media=0, Alta=54\n  Balance de plasticidad: 0.340\n\n[Métricas Empíricas - Paso 11000]\n  Retención Fase A: 15.5%\n  Interferencia B→A: 0.373\n  Costo de oportunidad: 0.670\n  Transferencia de conocimiento: 0.00%\n\n[Métricas Empíricas - Paso 12000]\n  Retención Fase A: 17.6%\n  Interferencia B→A: 0.379\n  Costo de oportunidad: 0.678\n  Transferencia de conocimiento: 40.30%\n\n[Estadísticas del Sistema - Paso 12000]\n  Plasticidad promedio: 0.220\n  Distribución: Baja=264, Media=0, Alta=66\n  Balance de plasticidad: 0.400\n\n[Métricas Empíricas - Paso 13000]\n  Retención Fase A: 16.3%\n  Interferencia B→A: 0.374\n  Costo de oportunidad: 0.675\n  Transferencia de conocimiento: 53.45%\n\n[Métricas Empíricas - Paso 14000]\n  Retención Fase A: 15.7%\n  Interferencia B→A: 0.384\n  Costo de oportunidad: 0.671\n  Transferencia de conocimiento: 63.20%\n\n[Estadísticas del Sistema - Paso 14000]\n  Plasticidad promedio: 0.233\n  Distribución: Baja=264, Media=0, Alta=73\n  Balance de plasticidad: 0.433\n\n[Métricas Empíricas - Paso 15000]\n  Retención Fase A: 12.7%\n  Interferencia B→A: 0.372\n  Costo de oportunidad: 0.681\n  Transferencia de conocimiento: 81.35%\n\n[Métricas Empíricas - Paso 16000]\n  Retención Fase A: 19.5%\n  Interferencia B→A: 0.385\n  Costo de oportunidad: 0.675\n  Transferencia de conocimiento: 84.38%\n\n[Estadísticas del Sistema - Paso 16000]\n  Plasticidad promedio: 0.223\n  Distribución: Baja=264, Media=0, Alta=69\n  Balance de plasticidad: 0.414\n\n[Métricas Empíricas - Paso 17000]\n  Retención Fase A: 19.5%\n  Interferencia B→A: 0.393\n  Costo de oportunidad: 0.672\n  Transferencia de conocimiento: 83.93%\n\n[Métricas Empíricas - Paso 18000]\n  Retención Fase A: 29.1%\n  Interferencia B→A: 0.374\n  Costo de oportunidad: 0.682\n  Transferencia de conocimiento: 88.34%\n\n[Estadísticas del Sistema - Paso 18000]\n  Plasticidad promedio: 0.205\n  Distribución: Baja=264, Media=0, Alta=62\n  Balance de plasticidad: 0.380\n\n[Métricas Empíricas - Paso 19000]\n  Retención Fase A: 22.8%\n  Interferencia B→A: 0.386\n  Costo de oportunidad: 0.683\n  Transferencia de conocimiento: 89.54%\n\n[Métricas Empíricas - Paso 20000]\n  Retención Fase A: 17.1%\n  Interferencia B→A: 0.378\n  Costo de oportunidad: 0.677\n  Transferencia de conocimiento: 87.31%\n\n[Estadísticas del Sistema - Paso 20000]\n  Plasticidad promedio: 0.216\n  Distribución: Baja=264, Media=0, Alta=67\n  Balance de plasticidad: 0.405\n\n[Métricas Empíricas - Paso 21000]\n  Retención Fase A: 29.8%\n  Interferencia B→A: 0.380\n  Costo de oportunidad: 0.684\n  Transferencia de conocimiento: 87.65%\n\n[Métricas Empíricas - Paso 22000]\n  Retención Fase A: 19.9%\n  Interferencia B→A: 0.398\n  Costo de oportunidad: 0.677\n  Transferencia de conocimiento: 86.10%\n\n[Estadísticas del Sistema - Paso 22000]\n  Plasticidad promedio: 0.215\n  Distribución: Baja=264, Media=0, Alta=67\n  Balance de plasticidad: 0.405\n\n[Métricas Empíricas - Paso 23000]\n  Retención Fase A: 17.1%\n  Interferencia B→A: 0.391\n  Costo de oportunidad: 0.681\n  Transferencia de conocimiento: 90.52%\n\n[Métricas Empíricas - Paso 24000]\n  Retención Fase A: 19.5%\n  Interferencia B→A: 0.390\n  Costo de oportunidad: 0.684\n  Transferencia de conocimiento: 91.36%\n\n[Estadísticas del Sistema - Paso 24000]\n  Plasticidad promedio: 0.197\n  Distribución: Baja=264, Media=0, Alta=60\n  Balance de plasticidad: 0.370\n\n[Métricas Empíricas - Paso 25000]\n  Retención Fase A: 15.4%\n  Interferencia B→A: 0.389\n  Costo de oportunidad: 0.685\n  Transferencia de conocimiento: 93.19%\n\n[Métricas Empíricas - Paso 26000]\n  Retención Fase A: 17.8%\n  Interferencia B→A: 0.400\n  Costo de oportunidad: 0.671\n  Transferencia de conocimiento: 89.91%\n\n[Estadísticas del Sistema - Paso 26000]\n  Plasticidad promedio: 0.227\n  Distribución: Baja=264, Media=0, Alta=73\n  Balance de plasticidad: 0.433\n\n[Métricas Empíricas - Paso 27000]\n  Retención Fase A: 14.2%\n  Interferencia B→A: 0.365\n  Costo de oportunidad: 0.683\n  Transferencia de conocimiento: 93.85%\n\n[Métricas Empíricas - Paso 28000]\n  Retención Fase A: 10.0%\n  Interferencia B→A: 0.387\n  Costo de oportunidad: 0.671\n  Transferencia de conocimiento: 91.39%\n\n[Estadísticas del Sistema - Paso 28000]\n  Plasticidad promedio: 0.226\n  Distribución: Baja=264, Media=0, Alta=73\n  Balance de plasticidad: 0.433\n\n[Métricas Empíricas - Paso 29000]\n  Retención Fase A: 14.5%\n  Interferencia B→A: 0.377\n  Costo de oportunidad: 0.684\n  Transferencia de conocimiento: 95.68%\n\n[Métricas Empíricas - Paso 30000]\n  Retención Fase A: 18.8%\n  Interferencia B→A: 0.383\n  Costo de oportunidad: 0.684\n  Transferencia de conocimiento: 95.99%\n\n[Estadísticas del Sistema - Paso 30000]\n  Plasticidad promedio: 0.195\n  Distribución: Baja=264, Media=0, Alta=60\n  Balance de plasticidad: 0.370\n\n[Métricas Empíricas - Paso 31000]\n  Retención Fase A: 11.3%\n  Interferencia B→A: 0.379\n  Costo de oportunidad: 0.683\n  Transferencia de conocimiento: 95.69%\n\n[Métricas Empíricas - Paso 32000]\n  Retención Fase A: 11.6%\n  Interferencia B→A: 0.372\n  Costo de oportunidad: 0.681\n  Transferencia de conocimiento: 95.41%\n\n[Estadísticas del Sistema - Paso 32000]\n  Plasticidad promedio: 0.203\n  Distribución: Baja=264, Media=0, Alta=63\n  Balance de plasticidad: 0.385\n\n[Métricas Empíricas - Paso 33000]\n  Retención Fase A: 12.4%\n  Interferencia B→A: 0.381\n  Costo de oportunidad: 0.680\n  Transferencia de conocimiento: 96.34%\n\n[Métricas Empíricas - Paso 34000]\n  Retención Fase A: 20.1%\n  Interferencia B→A: 0.383\n  Costo de oportunidad: 0.680\n  Transferencia de conocimiento: 96.65%\n\n[Estadísticas del Sistema - Paso 34000]\n  Plasticidad promedio: 0.204\n  Distribución: Baja=264, Media=0, Alta=64\n  Balance de plasticidad: 0.390\n\n============================================================\nÉpoca 2/6\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Época 2:   0%|          | 0/34559 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cebe66617463486e96f08c0ddeb761f8"}},"metadata":{}},{"name":"stdout","text":"\n[Métricas Empíricas - Paso 35000]\n  Retención Fase A: 16.6%\n  Interferencia B→A: 0.382\n  Costo de oportunidad: 0.676\n  Transferencia de conocimiento: 94.58%\n\n[Métricas Empíricas - Paso 36000]\n  Retención Fase A: 20.5%\n  Interferencia B→A: 0.401\n  Costo de oportunidad: 0.677\n  Transferencia de conocimiento: 94.56%\n\n[Estadísticas del Sistema - Paso 36000]\n  Plasticidad promedio: 0.211\n  Distribución: Baja=264, Media=0, Alta=67\n  Balance de plasticidad: 0.405\n\n[Métricas Empíricas - Paso 37000]\n  Retención Fase A: 15.8%\n  Interferencia B→A: 0.385\n  Costo de oportunidad: 0.683\n  Transferencia de conocimiento: 96.31%\n\n[Métricas Empíricas - Paso 39000]\n  Retención Fase A: 10.4%\n  Interferencia B→A: 0.401\n  Costo de oportunidad: 0.672\n  Transferencia de conocimiento: 93.75%\n\n[Métricas Empíricas - Paso 40000]\n  Retención Fase A: 12.5%\n  Interferencia B→A: 0.394\n  Costo de oportunidad: 0.675\n  Transferencia de conocimiento: 94.29%\n\n[Estadísticas del Sistema - Paso 40000]\n  Plasticidad promedio: 0.216\n  Distribución: Baja=264, Media=0, Alta=69\n  Balance de plasticidad: 0.414\n\n[Métricas Empíricas - Paso 41000]\n  Retención Fase A: 10.8%\n  Interferencia B→A: 0.388\n  Costo de oportunidad: 0.679\n  Transferencia de conocimiento: 95.14%\n\n[Métricas Empíricas - Paso 42000]\n  Retención Fase A: 11.7%\n  Interferencia B→A: 0.399\n  Costo de oportunidad: 0.681\n  Transferencia de conocimiento: 96.94%\n\n[Estadísticas del Sistema - Paso 42000]\n  Plasticidad promedio: 0.201\n  Distribución: Baja=264, Media=0, Alta=63\n  Balance de plasticidad: 0.385\n\n[Métricas Empíricas - Paso 43000]\n  Retención Fase A: 12.5%\n  Interferencia B→A: 0.383\n  Costo de oportunidad: 0.684\n  Transferencia de conocimiento: 99.07%\n\n[Métricas Empíricas - Paso 44000]\n  Retención Fase A: 9.9%\n  Interferencia B→A: 0.392\n  Costo de oportunidad: 0.681\n  Transferencia de conocimiento: 97.55%\n\n[Estadísticas del Sistema - Paso 44000]\n  Plasticidad promedio: 0.201\n  Distribución: Baja=264, Media=0, Alta=63\n  Balance de plasticidad: 0.385\n\n[Métricas Empíricas - Paso 45000]\n  Retención Fase A: 12.6%\n  Interferencia B→A: 0.382\n  Costo de oportunidad: 0.682\n  Transferencia de conocimiento: 97.24%\n\n[Métricas Empíricas - Paso 46000]\n  Retención Fase A: 10.8%\n  Interferencia B→A: 0.392\n  Costo de oportunidad: 0.671\n  Transferencia de conocimiento: 94.36%\n\n[Estadísticas del Sistema - Paso 46000]\n  Plasticidad promedio: 0.225\n  Distribución: Baja=264, Media=0, Alta=73\n  Balance de plasticidad: 0.433\n\n[Métricas Empíricas - Paso 47000]\n  Retención Fase A: 11.2%\n  Interferencia B→A: 0.398\n  Costo de oportunidad: 0.671\n  Transferencia de conocimiento: 94.96%\n\n[Métricas Empíricas - Paso 48000]\n  Retención Fase A: 10.1%\n  Interferencia B→A: 0.408\n  Costo de oportunidad: 0.683\n  Transferencia de conocimiento: 97.23%\n\n[Estadísticas del Sistema - Paso 48000]\n  Plasticidad promedio: 0.197\n  Distribución: Baja=264, Media=0, Alta=61\n  Balance de plasticidad: 0.375\n\n[Métricas Empíricas - Paso 49000]\n  Retención Fase A: 12.3%\n  Interferencia B→A: 0.388\n  Costo de oportunidad: 0.681\n  Transferencia de conocimiento: 97.86%\n\n[Métricas Empíricas - Paso 50000]\n  Retención Fase A: 9.4%\n  Interferencia B→A: 0.391\n  Costo de oportunidad: 0.681\n  Transferencia de conocimiento: 97.55%\n\n[Estadísticas del Sistema - Paso 50000]\n  Plasticidad promedio: 0.201\n  Distribución: Baja=264, Media=0, Alta=63\n  Balance de plasticidad: 0.385\n\n[Métricas Empíricas - Paso 51000]\n  Retención Fase A: 10.9%\n  Interferencia B→A: 0.399\n  Costo de oportunidad: 0.679\n  Transferencia de conocimiento: 96.66%\n\n[Métricas Empíricas - Paso 53000]\n  Retención Fase A: 8.4%\n  Interferencia B→A: 0.397\n  Costo de oportunidad: 0.670\n  Transferencia de conocimiento: 95.27%\n\n[Métricas Empíricas - Paso 54000]\n  Retención Fase A: 14.9%\n  Interferencia B→A: 0.401\n  Costo de oportunidad: 0.683\n  Transferencia de conocimiento: 96.92%\n\n[Estadísticas del Sistema - Paso 54000]\n  Plasticidad promedio: 0.196\n  Distribución: Baja=264, Media=0, Alta=61\n  Balance de plasticidad: 0.375\n\n[Métricas Empíricas - Paso 55000]\n  Retención Fase A: 12.2%\n  Interferencia B→A: 0.405\n  Costo de oportunidad: 0.685\n  Transferencia de conocimiento: 99.07%\n\n[Métricas Empíricas - Paso 56000]\n  Retención Fase A: 12.3%\n  Interferencia B→A: 0.392\n  Costo de oportunidad: 0.683\n  Transferencia de conocimiento: 98.46%\n\n[Estadísticas del Sistema - Paso 56000]\n  Plasticidad promedio: 0.196\n  Distribución: Baja=264, Media=0, Alta=61\n  Balance de plasticidad: 0.375\n\n[Métricas Empíricas - Paso 57000]\n  Retención Fase A: 11.4%\n  Interferencia B→A: 0.409\n  Costo de oportunidad: 0.681\n  Transferencia de conocimiento: 97.25%\n\n[Métricas Empíricas - Paso 58000]\n  Retención Fase A: 13.7%\n  Interferencia B→A: 0.414\n  Costo de oportunidad: 0.676\n  Transferencia de conocimiento: 95.48%\n\n[Estadísticas del Sistema - Paso 58000]\n  Plasticidad promedio: 0.213\n  Distribución: Baja=264, Media=0, Alta=68\n  Balance de plasticidad: 0.410\n\n[Métricas Empíricas - Paso 59000]\n  Retención Fase A: 14.8%\n  Interferencia B→A: 0.417\n  Costo de oportunidad: 0.670\n  Transferencia de conocimiento: 94.38%\n\n[Métricas Empíricas - Paso 60000]\n  Retención Fase A: 13.5%\n  Interferencia B→A: 0.413\n  Costo de oportunidad: 0.674\n  Transferencia de conocimiento: 96.11%\n\n[Estadísticas del Sistema - Paso 60000]\n  Plasticidad promedio: 0.218\n  Distribución: Baja=264, Media=0, Alta=70\n  Balance de plasticidad: 0.419\n\n[Métricas Empíricas - Paso 61000]\n  Retención Fase A: 10.8%\n  Interferencia B→A: 0.433\n  Costo de oportunidad: 0.674\n  Transferencia de conocimiento: 94.61%\n\n[Métricas Empíricas - Paso 62000]\n  Retención Fase A: 9.1%\n  Interferencia B→A: 0.418\n  Costo de oportunidad: 0.685\n  Transferencia de conocimiento: 99.07%\n\n[Estadísticas del Sistema - Paso 62000]\n  Plasticidad promedio: 0.191\n  Distribución: Baja=264, Media=0, Alta=59\n  Balance de plasticidad: 0.365\n\n[Métricas Empíricas - Paso 63000]\n  Retención Fase A: 9.4%\n  Interferencia B→A: 0.409\n  Costo de oportunidad: 0.678\n  Transferencia de conocimiento: 96.97%\n\n[Métricas Empíricas - Paso 64000]\n  Retención Fase A: 12.9%\n  Interferencia B→A: 0.425\n  Costo de oportunidad: 0.674\n  Transferencia de conocimiento: 95.81%\n\n[Estadísticas del Sistema - Paso 64000]\n  Plasticidad promedio: 0.218\n  Distribución: Baja=264, Media=0, Alta=70\n  Balance de plasticidad: 0.419\n\n[Métricas Empíricas - Paso 65000]\n  Retención Fase A: 16.5%\n  Interferencia B→A: 0.420\n  Costo de oportunidad: 0.683\n  Transferencia de conocimiento: 98.77%\n\n[Métricas Empíricas - Paso 66000]\n  Retención Fase A: 15.9%\n  Interferencia B→A: 0.424\n  Costo de oportunidad: 0.671\n  Transferencia de conocimiento: 94.36%\n\n[Estadísticas del Sistema - Paso 66000]\n  Plasticidad promedio: 0.225\n  Distribución: Baja=264, Media=0, Alta=73\n  Balance de plasticidad: 0.433\n\n[Métricas Empíricas - Paso 67000]\n  Retención Fase A: 10.1%\n  Interferencia B→A: 0.412\n  Costo de oportunidad: 0.671\n  Transferencia de conocimiento: 93.47%\n\n[Métricas Empíricas - Paso 68000]\n  Retención Fase A: 14.2%\n  Interferencia B→A: 0.417\n  Costo de oportunidad: 0.678\n  Transferencia de conocimiento: 97.58%\n\n[Estadísticas del Sistema - Paso 68000]\n  Plasticidad promedio: 0.208\n  Distribución: Baja=264, Media=0, Alta=66\n  Balance de plasticidad: 0.400\n\n[Métricas Empíricas - Paso 69000]\n  Retención Fase A: 9.8%\n  Interferencia B→A: 0.423\n  Costo de oportunidad: 0.676\n  Transferencia de conocimiento: 96.69%\n\n============================================================\nÉpoca 3/6\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Época 3:   0%|          | 0/34559 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8fd614bb2e6403887c72bec74fb5484"}},"metadata":{}},{"name":"stdout","text":"\n[Métricas Empíricas - Paso 70000]\n  Retención Fase A: 7.9%\n  Interferencia B→A: 0.415\n  Costo de oportunidad: 0.680\n  Transferencia de conocimiento: 97.56%\n\n[Estadísticas del Sistema - Paso 70000]\n  Plasticidad promedio: 0.204\n  Distribución: Baja=264, Media=0, Alta=64\n  Balance de plasticidad: 0.390\n\n[Métricas Empíricas - Paso 71000]\n  Retención Fase A: 7.7%\n  Interferencia B→A: 0.435\n  Costo de oportunidad: 0.674\n  Transferencia de conocimiento: 95.51%\n\n[Métricas Empíricas - Paso 72000]\n  Retención Fase A: 6.4%\n  Interferencia B→A: 0.421\n  Costo de oportunidad: 0.682\n  Transferencia de conocimiento: 96.93%\n\n[Estadísticas del Sistema - Paso 72000]\n  Plasticidad promedio: 0.199\n  Distribución: Baja=264, Media=0, Alta=62\n  Balance de plasticidad: 0.380\n\n[Métricas Empíricas - Paso 73000]\n  Retención Fase A: 11.3%\n  Interferencia B→A: 0.407\n  Costo de oportunidad: 0.684\n  Transferencia de conocimiento: 98.77%\n\n[Métricas Empíricas - Paso 74000]\n  Retención Fase A: 8.9%\n  Interferencia B→A: 0.429\n  Costo de oportunidad: 0.682\n  Transferencia de conocimiento: 97.24%\n\n[Estadísticas del Sistema - Paso 74000]\n  Plasticidad promedio: 0.199\n  Distribución: Baja=264, Media=0, Alta=62\n  Balance de plasticidad: 0.380\n\n[Métricas Empíricas - Paso 75000]\n  Retención Fase A: 8.6%\n  Interferencia B→A: 0.420\n  Costo de oportunidad: 0.683\n  Transferencia de conocimiento: 98.15%\n\n[Métricas Empíricas - Paso 76000]\n  Retención Fase A: 11.1%\n  Interferencia B→A: 0.424\n  Costo de oportunidad: 0.679\n  Transferencia de conocimiento: 96.66%\n\n[Estadísticas del Sistema - Paso 76000]\n  Plasticidad promedio: 0.206\n  Distribución: Baja=264, Media=0, Alta=65\n  Balance de plasticidad: 0.395\n\n[Métricas Empíricas - Paso 77000]\n  Retención Fase A: 9.5%\n  Interferencia B→A: 0.422\n  Costo de oportunidad: 0.684\n  Transferencia de conocimiento: 98.15%\n\n[Métricas Empíricas - Paso 78000]\n  Retención Fase A: 5.6%\n  Interferencia B→A: 0.422\n  Costo de oportunidad: 0.683\n  Transferencia de conocimiento: 97.54%\n\n[Estadísticas del Sistema - Paso 78000]\n  Plasticidad promedio: 0.196\n  Distribución: Baja=264, Media=0, Alta=61\n  Balance de plasticidad: 0.375\n\n[Métricas Empíricas - Paso 79000]\n  Retención Fase A: 8.2%\n  Interferencia B→A: 0.435\n  Costo de oportunidad: 0.683\n  Transferencia de conocimiento: 98.77%\n\n[Métricas Empíricas - Paso 80000]\n  Retención Fase A: 6.1%\n  Interferencia B→A: 0.420\n  Costo de oportunidad: 0.681\n  Transferencia de conocimiento: 97.55%\n\n[Estadísticas del Sistema - Paso 80000]\n  Plasticidad promedio: 0.201\n  Distribución: Baja=264, Media=0, Alta=63\n  Balance de plasticidad: 0.385\n\n[Métricas Empíricas - Paso 81000]\n  Retención Fase A: 8.0%\n  Interferencia B→A: 0.409\n  Costo de oportunidad: 0.681\n  Transferencia de conocimiento: 98.17%\n\n[Métricas Empíricas - Paso 82000]\n  Retención Fase A: 12.5%\n  Interferencia B→A: 0.438\n  Costo de oportunidad: 0.678\n  Transferencia de conocimiento: 97.27%\n\n[Estadísticas del Sistema - Paso 82000]\n  Plasticidad promedio: 0.208\n  Distribución: Baja=264, Media=0, Alta=66\n  Balance de plasticidad: 0.400\n\n[Métricas Empíricas - Paso 83000]\n  Retención Fase A: 8.0%\n  Interferencia B→A: 0.422\n  Costo de oportunidad: 0.673\n  Transferencia de conocimiento: 95.82%\n\n[Métricas Empíricas - Paso 84000]\n  Retención Fase A: 7.5%\n  Interferencia B→A: 0.431\n  Costo de oportunidad: 0.684\n  Transferencia de conocimiento: 99.38%\n\n[Estadísticas del Sistema - Paso 84000]\n  Plasticidad promedio: 0.194\n  Distribución: Baja=264, Media=0, Alta=60\n  Balance de plasticidad: 0.370\n\n[Métricas Empíricas - Paso 85000]\n  Retención Fase A: 7.9%\n  Interferencia B→A: 0.455\n  Costo de oportunidad: 0.681\n  Transferencia de conocimiento: 98.17%\n\n[Métricas Empíricas - Paso 86000]\n  Retención Fase A: 7.1%\n  Interferencia B→A: 0.437\n  Costo de oportunidad: 0.675\n  Transferencia de conocimiento: 96.40%\n\n[Estadísticas del Sistema - Paso 86000]\n  Plasticidad promedio: 0.216\n  Distribución: Baja=264, Media=0, Alta=69\n  Balance de plasticidad: 0.414\n\n[Métricas Empíricas - Paso 87000]\n  Retención Fase A: 6.8%\n  Interferencia B→A: 0.445\n  Costo de oportunidad: 0.676\n  Transferencia de conocimiento: 96.69%\n\n[Métricas Empíricas - Paso 88000]\n  Retención Fase A: 9.8%\n  Interferencia B→A: 0.429\n  Costo de oportunidad: 0.676\n  Transferencia de conocimiento: 95.78%\n\n[Estadísticas del Sistema - Paso 88000]\n  Plasticidad promedio: 0.213\n  Distribución: Baja=264, Media=0, Alta=68\n  Balance de plasticidad: 0.410\n\n[Métricas Empíricas - Paso 89000]\n  Retención Fase A: 7.0%\n  Interferencia B→A: 0.434\n  Costo de oportunidad: 0.678\n  Transferencia de conocimiento: 96.97%\n\n[Métricas Empíricas - Paso 90000]\n  Retención Fase A: 5.7%\n  Interferencia B→A: 0.428\n  Costo de oportunidad: 0.670\n  Transferencia de conocimiento: 94.97%\n\n[Estadísticas del Sistema - Paso 90000]\n  Plasticidad promedio: 0.227\n  Distribución: Baja=264, Media=0, Alta=74\n  Balance de plasticidad: 0.438\n\n[Métricas Empíricas - Paso 91000]\n  Retención Fase A: 8.4%\n  Interferencia B→A: 0.442\n  Costo de oportunidad: 0.679\n  Transferencia de conocimiento: 97.57%\n\n[Métricas Empíricas - Paso 92000]\n  Retención Fase A: 10.2%\n  Interferencia B→A: 0.436\n  Costo de oportunidad: 0.680\n  Transferencia de conocimiento: 96.95%\n\n[Estadísticas del Sistema - Paso 92000]\n  Plasticidad promedio: 0.204\n  Distribución: Baja=264, Media=0, Alta=64\n  Balance de plasticidad: 0.390\n\n[Métricas Empíricas - Paso 93000]\n  Retención Fase A: 4.3%\n  Interferencia B→A: 0.441\n  Costo de oportunidad: 0.678\n  Transferencia de conocimiento: 95.76%\n\n[Métricas Empíricas - Paso 94000]\n  Retención Fase A: 3.8%\n  Interferencia B→A: 0.441\n  Costo de oportunidad: 0.671\n  Transferencia de conocimiento: 95.25%\n\n[Estadísticas del Sistema - Paso 94000]\n  Plasticidad promedio: 0.225\n  Distribución: Baja=264, Media=0, Alta=73\n  Balance de plasticidad: 0.433\n\n[Métricas Empíricas - Paso 95000]\n  Retención Fase A: 3.9%\n  Interferencia B→A: 0.428\n  Costo de oportunidad: 0.684\n  Transferencia de conocimiento: 98.77%\n\n[Métricas Empíricas - Paso 96000]\n  Retención Fase A: 4.1%\n  Interferencia B→A: 0.442\n  Costo de oportunidad: 0.673\n  Transferencia de conocimiento: 96.12%\n\n[Estadísticas del Sistema - Paso 96000]\n  Plasticidad promedio: 0.220\n  Distribución: Baja=264, Media=0, Alta=71\n  Balance de plasticidad: 0.424\n\n[Métricas Empíricas - Paso 97000]\n  Retención Fase A: 4.5%\n  Interferencia B→A: 0.434\n  Costo de oportunidad: 0.685\n  Transferencia de conocimiento: 99.69%\n\n[Métricas Empíricas - Paso 98000]\n  Retención Fase A: 8.8%\n  Interferencia B→A: 0.434\n  Costo de oportunidad: 0.676\n  Transferencia de conocimiento: 96.39%\n\n[Estadísticas del Sistema - Paso 98000]\n  Plasticidad promedio: 0.213\n  Distribución: Baja=264, Media=0, Alta=68\n  Balance de plasticidad: 0.410\n\n[Métricas Empíricas - Paso 99000]\n  Retención Fase A: 5.5%\n  Interferencia B→A: 0.441\n  Costo de oportunidad: 0.676\n  Transferencia de conocimiento: 96.39%\n\n[Métricas Empíricas - Paso 100000]\n  Retención Fase A: 6.8%\n  Interferencia B→A: 0.451\n  Costo de oportunidad: 0.677\n  Transferencia de conocimiento: 96.07%\n\n[Estadísticas del Sistema - Paso 100000]\n  Plasticidad promedio: 0.211\n  Distribución: Baja=264, Media=0, Alta=67\n  Balance de plasticidad: 0.405\n\n[Métricas Empíricas - Paso 101000]\n  Retención Fase A: 5.6%\n  Interferencia B→A: 0.448\n  Costo de oportunidad: 0.679\n  Transferencia de conocimiento: 97.26%\n\n[Métricas Empíricas - Paso 102000]\n  Retención Fase A: 6.5%\n  Interferencia B→A: 0.442\n  Costo de oportunidad: 0.675\n  Transferencia de conocimiento: 96.10%\n\n[Estadísticas del Sistema - Paso 102000]\n  Plasticidad promedio: 0.215\n  Distribución: Baja=264, Media=0, Alta=69\n  Balance de plasticidad: 0.414\n\n[Métricas Empíricas - Paso 103000]\n  Retención Fase A: 3.8%\n  Interferencia B→A: 0.442\n  Costo de oportunidad: 0.674\n  Transferencia de conocimiento: 96.11%\n\n============================================================\nÉpoca 4/6\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Época 4:   0%|          | 0/34559 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fb2ac99bdac4d79a0629714a79d6063"}},"metadata":{}},{"name":"stdout","text":"\n[Métricas Empíricas - Paso 104000]\n  Retención Fase A: 4.5%\n  Interferencia B→A: 0.458\n  Costo de oportunidad: 0.676\n  Transferencia de conocimiento: 96.39%\n\n[Estadísticas del Sistema - Paso 104000]\n  Plasticidad promedio: 0.213\n  Distribución: Baja=264, Media=0, Alta=68\n  Balance de plasticidad: 0.410\n\n[Métricas Empíricas - Paso 105000]\n  Retención Fase A: 8.4%\n  Interferencia B→A: 0.451\n  Costo de oportunidad: 0.680\n  Transferencia de conocimiento: 97.26%\n\n[Métricas Empíricas - Paso 107000]\n  Retención Fase A: 7.1%\n  Interferencia B→A: 0.454\n  Costo de oportunidad: 0.675\n  Transferencia de conocimiento: 95.50%\n\n[Métricas Empíricas - Paso 108000]\n  Retención Fase A: 5.1%\n  Interferencia B→A: 0.452\n  Costo de oportunidad: 0.679\n  Transferencia de conocimiento: 96.66%\n\n[Estadísticas del Sistema - Paso 108000]\n  Plasticidad promedio: 0.206\n  Distribución: Baja=264, Media=0, Alta=65\n  Balance de plasticidad: 0.395\n\n[Métricas Empíricas - Paso 109000]\n  Retención Fase A: 5.2%\n  Interferencia B→A: 0.456\n  Costo de oportunidad: 0.679\n  Transferencia de conocimiento: 97.57%\n\n[Métricas Empíricas - Paso 110000]\n  Retención Fase A: 6.2%\n  Interferencia B→A: 0.444\n  Costo de oportunidad: 0.681\n  Transferencia de conocimiento: 97.86%\n\n[Estadísticas del Sistema - Paso 110000]\n  Plasticidad promedio: 0.201\n  Distribución: Baja=264, Media=0, Alta=63\n  Balance de plasticidad: 0.385\n\n[Métricas Empíricas - Paso 111000]\n  Retención Fase A: 3.6%\n  Interferencia B→A: 0.452\n  Costo de oportunidad: 0.683\n  Transferencia de conocimiento: 98.46%\n\n[Métricas Empíricas - Paso 112000]\n  Retención Fase A: 3.4%\n  Interferencia B→A: 0.454\n  Costo de oportunidad: 0.672\n  Transferencia de conocimiento: 94.94%\n\n[Estadísticas del Sistema - Paso 112000]\n  Plasticidad promedio: 0.223\n  Distribución: Baja=264, Media=0, Alta=72\n  Balance de plasticidad: 0.429\n\n[Métricas Empíricas - Paso 113000]\n  Retención Fase A: 5.4%\n  Interferencia B→A: 0.456\n  Costo de oportunidad: 0.674\n  Transferencia de conocimiento: 95.81%\n\n[Métricas Empíricas - Paso 114000]\n  Retención Fase A: 3.7%\n  Interferencia B→A: 0.468\n  Costo de oportunidad: 0.677\n  Transferencia de conocimiento: 96.37%\n\n[Estadísticas del Sistema - Paso 114000]\n  Plasticidad promedio: 0.211\n  Distribución: Baja=264, Media=0, Alta=67\n  Balance de plasticidad: 0.405\n\n[Métricas Empíricas - Paso 115000]\n  Retención Fase A: 6.5%\n  Interferencia B→A: 0.449\n  Costo de oportunidad: 0.684\n  Transferencia de conocimiento: 98.46%\n\n[Métricas Empíricas - Paso 116000]\n  Retención Fase A: 4.3%\n  Interferencia B→A: 0.449\n  Costo de oportunidad: 0.684\n  Transferencia de conocimiento: 98.77%\n\n[Estadísticas del Sistema - Paso 116000]\n  Plasticidad promedio: 0.194\n  Distribución: Baja=264, Media=0, Alta=60\n  Balance de plasticidad: 0.370\n\n[Métricas Empíricas - Paso 117000]\n  Retención Fase A: 3.5%\n  Interferencia B→A: 0.454\n  Costo de oportunidad: 0.684\n  Transferencia de conocimiento: 98.46%\n\n[Métricas Empíricas - Paso 118000]\n  Retención Fase A: 3.4%\n  Interferencia B→A: 0.456\n  Costo de oportunidad: 0.685\n  Transferencia de conocimiento: 99.07%\n\n[Estadísticas del Sistema - Paso 118000]\n  Plasticidad promedio: 0.191\n  Distribución: Baja=264, Media=0, Alta=59\n  Balance de plasticidad: 0.365\n\n[Métricas Empíricas - Paso 119000]\n  Retención Fase A: 4.5%\n  Interferencia B→A: 0.460\n  Costo de oportunidad: 0.682\n  Transferencia de conocimiento: 98.47%\n\n[Métricas Empíricas - Paso 120000]\n  Retención Fase A: 5.3%\n  Interferencia B→A: 0.450\n  Costo de oportunidad: 0.670\n  Transferencia de conocimiento: 94.38%\n\n[Estadísticas del Sistema - Paso 120000]\n  Plasticidad promedio: 0.227\n  Distribución: Baja=264, Media=0, Alta=74\n  Balance de plasticidad: 0.438\n\n[Métricas Empíricas - Paso 121000]\n  Retención Fase A: 4.3%\n  Interferencia B→A: 0.453\n  Costo de oportunidad: 0.678\n  Transferencia de conocimiento: 96.97%\n\n[Métricas Empíricas - Paso 122000]\n  Retención Fase A: 4.6%\n  Interferencia B→A: 0.463\n  Costo de oportunidad: 0.676\n  Transferencia de conocimiento: 96.69%\n\n[Estadísticas del Sistema - Paso 122000]\n  Plasticidad promedio: 0.213\n  Distribución: Baja=264, Media=0, Alta=68\n  Balance de plasticidad: 0.410\n\n[Métricas Empíricas - Paso 123000]\n  Retención Fase A: 6.0%\n  Interferencia B→A: 0.474\n  Costo de oportunidad: 0.673\n  Transferencia de conocimiento: 95.52%\n\n[Métricas Empíricas - Paso 124000]\n  Retención Fase A: 9.6%\n  Interferencia B→A: 0.446\n  Costo de oportunidad: 0.680\n  Transferencia de conocimiento: 97.87%\n\n[Estadísticas del Sistema - Paso 124000]\n  Plasticidad promedio: 0.204\n  Distribución: Baja=264, Media=0, Alta=64\n  Balance de plasticidad: 0.390\n\n[Métricas Empíricas - Paso 125000]\n  Retención Fase A: 6.4%\n  Interferencia B→A: 0.465\n  Costo de oportunidad: 0.678\n  Transferencia de conocimiento: 96.67%\n\n[Métricas Empíricas - Paso 126000]\n  Retención Fase A: 4.0%\n  Interferencia B→A: 0.462\n  Costo de oportunidad: 0.673\n  Transferencia de conocimiento: 95.52%\n\n[Estadísticas del Sistema - Paso 126000]\n  Plasticidad promedio: 0.220\n  Distribución: Baja=264, Media=0, Alta=71\n  Balance de plasticidad: 0.424\n\n[Métricas Empíricas - Paso 127000]\n  Retención Fase A: 3.8%\n  Interferencia B→A: 0.448\n  Costo de oportunidad: 0.671\n  Transferencia de conocimiento: 94.96%\n\n[Métricas Empíricas - Paso 128000]\n  Retención Fase A: 4.8%\n  Interferencia B→A: 0.452\n  Costo de oportunidad: 0.682\n  Transferencia de conocimiento: 98.16%\n\n[Estadísticas del Sistema - Paso 128000]\n  Plasticidad promedio: 0.199\n  Distribución: Baja=264, Media=0, Alta=62\n  Balance de plasticidad: 0.380\n\n[Métricas Empíricas - Paso 129000]\n  Retención Fase A: 4.0%\n  Interferencia B→A: 0.482\n  Costo de oportunidad: 0.673\n  Transferencia de conocimiento: 95.82%\n\n[Métricas Empíricas - Paso 130000]\n  Retención Fase A: 3.4%\n  Interferencia B→A: 0.474\n  Costo de oportunidad: 0.671\n  Transferencia de conocimiento: 95.25%\n\n[Estadísticas del Sistema - Paso 130000]\n  Plasticidad promedio: 0.225\n  Distribución: Baja=264, Media=0, Alta=73\n  Balance de plasticidad: 0.433\n\n[Métricas Empíricas - Paso 131000]\n  Retención Fase A: 3.0%\n  Interferencia B→A: 0.475\n  Costo de oportunidad: 0.683\n  Transferencia de conocimiento: 97.85%\n\n[Métricas Empíricas - Paso 132000]\n  Retención Fase A: 2.0%\n  Interferencia B→A: 0.466\n  Costo de oportunidad: 0.682\n  Transferencia de conocimiento: 98.47%\n\n[Estadísticas del Sistema - Paso 132000]\n  Plasticidad promedio: 0.199\n  Distribución: Baja=264, Media=0, Alta=62\n  Balance de plasticidad: 0.380\n\n[Métricas Empíricas - Paso 133000]\n  Retención Fase A: 3.1%\n  Interferencia B→A: 0.471\n  Costo de oportunidad: 0.675\n  Transferencia de conocimiento: 96.70%\n\n[Métricas Empíricas - Paso 134000]\n  Retención Fase A: 7.2%\n  Interferencia B→A: 0.469\n  Costo de oportunidad: 0.671\n  Transferencia de conocimiento: 94.96%\n\n[Estadísticas del Sistema - Paso 134000]\n  Plasticidad promedio: 0.225\n  Distribución: Baja=264, Media=0, Alta=73\n  Balance de plasticidad: 0.433\n\n[Métricas Empíricas - Paso 135000]\n  Retención Fase A: 3.9%\n  Interferencia B→A: 0.474\n  Costo de oportunidad: 0.676\n  Transferencia de conocimiento: 96.08%\n\n[Métricas Empíricas - Paso 136000]\n  Retención Fase A: 3.7%\n  Interferencia B→A: 0.466\n  Costo de oportunidad: 0.684\n  Transferencia de conocimiento: 99.69%\n\n[Estadísticas del Sistema - Paso 136000]\n  Plasticidad promedio: 0.194\n  Distribución: Baja=264, Media=0, Alta=60\n  Balance de plasticidad: 0.370\n\n[Métricas Empíricas - Paso 137000]\n  Retención Fase A: 3.2%\n  Interferencia B→A: 0.468\n  Costo de oportunidad: 0.680\n  Transferencia de conocimiento: 98.17%\n\n[Métricas Empíricas - Paso 138000]\n  Retención Fase A: 1.1%\n  Interferencia B→A: 0.469\n  Costo de oportunidad: 0.683\n  Transferencia de conocimiento: 98.15%\n\n[Estadísticas del Sistema - Paso 138000]\n  Plasticidad promedio: 0.196\n  Distribución: Baja=264, Media=0, Alta=61\n  Balance de plasticidad: 0.375\n\n============================================================\nÉpoca 5/6\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Época 5:   0%|          | 0/34559 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90dc94ce6cdd497b94df47674fab91d8"}},"metadata":{}},{"name":"stdout","text":"\n[Métricas Empíricas - Paso 139000]\n  Retención Fase A: 3.5%\n  Interferencia B→A: 0.486\n  Costo de oportunidad: 0.676\n  Transferencia de conocimiento: 95.78%\n\n[Métricas Empíricas - Paso 140000]\n  Retención Fase A: 3.6%\n  Interferencia B→A: 0.484\n  Costo de oportunidad: 0.686\n  Transferencia de conocimiento: 96.89%\n\n[Estadísticas del Sistema - Paso 140000]\n  Plasticidad promedio: 0.189\n  Distribución: Baja=264, Media=0, Alta=58\n  Balance de plasticidad: 0.360\n\n[Métricas Empíricas - Paso 141000]\n  Retención Fase A: 7.7%\n  Interferencia B→A: 0.482\n  Costo de oportunidad: 0.676\n  Transferencia de conocimiento: 94.28%\n\n[Métricas Empíricas - Paso 142000]\n  Retención Fase A: 5.4%\n  Interferencia B→A: 0.488\n  Costo de oportunidad: 0.681\n  Transferencia de conocimiento: 96.94%\n\n[Estadísticas del Sistema - Paso 142000]\n  Plasticidad promedio: 0.201\n  Distribución: Baja=264, Media=0, Alta=63\n  Balance de plasticidad: 0.385\n\n[Métricas Empíricas - Paso 144000]\n  Retención Fase A: 5.5%\n  Interferencia B→A: 0.507\n  Costo de oportunidad: 0.673\n  Transferencia de conocimiento: 95.22%\n\n[Estadísticas del Sistema - Paso 144000]\n  Plasticidad promedio: 0.220\n  Distribución: Baja=264, Media=0, Alta=71\n  Balance de plasticidad: 0.424\n\n[Métricas Empíricas - Paso 145000]\n  Retención Fase A: 4.6%\n  Interferencia B→A: 0.487\n  Costo de oportunidad: 0.683\n  Transferencia de conocimiento: 97.85%\n\n[Métricas Empíricas - Paso 146000]\n  Retención Fase A: 4.9%\n  Interferencia B→A: 0.472\n  Costo de oportunidad: 0.673\n  Transferencia de conocimiento: 94.93%\n\n[Estadísticas del Sistema - Paso 146000]\n  Plasticidad promedio: 0.220\n  Distribución: Baja=264, Media=0, Alta=71\n  Balance de plasticidad: 0.424\n\n[Métricas Empíricas - Paso 147000]\n  Retención Fase A: 6.6%\n  Interferencia B→A: 0.475\n  Costo de oportunidad: 0.676\n  Transferencia de conocimiento: 96.69%\n\n[Métricas Empíricas - Paso 148000]\n  Retención Fase A: 9.2%\n  Interferencia B→A: 0.482\n  Costo de oportunidad: 0.686\n  Transferencia de conocimiento: 98.45%\n\n[Estadísticas del Sistema - Paso 148000]\n  Plasticidad promedio: 0.189\n  Distribución: Baja=264, Media=0, Alta=58\n  Balance de plasticidad: 0.360\n\n[Métricas Empíricas - Paso 149000]\n  Retención Fase A: 3.4%\n  Interferencia B→A: 0.483\n  Costo de oportunidad: 0.684\n  Transferencia de conocimiento: 97.84%\n\n[Métricas Empíricas - Paso 150000]\n  Retención Fase A: 7.5%\n  Interferencia B→A: 0.496\n  Costo de oportunidad: 0.671\n  Transferencia de conocimiento: 94.66%\n\n[Estadísticas del Sistema - Paso 150000]\n  Plasticidad promedio: 0.225\n  Distribución: Baja=264, Media=0, Alta=73\n  Balance de plasticidad: 0.433\n\n[Métricas Empíricas - Paso 151000]\n  Retención Fase A: 7.8%\n  Interferencia B→A: 0.471\n  Costo de oportunidad: 0.683\n  Transferencia de conocimiento: 98.46%\n\n[Métricas Empíricas - Paso 152000]\n  Retención Fase A: 2.2%\n  Interferencia B→A: 0.478\n  Costo de oportunidad: 0.686\n  Transferencia de conocimiento: 99.38%\n\n[Estadísticas del Sistema - Paso 152000]\n  Plasticidad promedio: 0.189\n  Distribución: Baja=264, Media=0, Alta=58\n  Balance de plasticidad: 0.360\n\n[Métricas Empíricas - Paso 153000]\n  Retención Fase A: 5.6%\n  Interferencia B→A: 0.485\n  Costo de oportunidad: 0.684\n  Transferencia de conocimiento: 99.07%\n\n[Métricas Empíricas - Paso 154000]\n  Retención Fase A: 2.5%\n  Interferencia B→A: 0.497\n  Costo de oportunidad: 0.680\n  Transferencia de conocimiento: 97.87%\n\n[Estadísticas del Sistema - Paso 154000]\n  Plasticidad promedio: 0.204\n  Distribución: Baja=264, Media=0, Alta=64\n  Balance de plasticidad: 0.390\n\n[Métricas Empíricas - Paso 155000]\n  Retención Fase A: 2.4%\n  Interferencia B→A: 0.499\n  Costo de oportunidad: 0.675\n  Transferencia de conocimiento: 95.80%\n\n[Métricas Empíricas - Paso 156000]\n  Retención Fase A: 5.1%\n  Interferencia B→A: 0.491\n  Costo de oportunidad: 0.679\n  Transferencia de conocimiento: 96.66%\n\n[Estadísticas del Sistema - Paso 156000]\n  Plasticidad promedio: 0.206\n  Distribución: Baja=264, Media=0, Alta=65\n  Balance de plasticidad: 0.395\n\n[Métricas Empíricas - Paso 157000]\n  Retención Fase A: 3.7%\n  Interferencia B→A: 0.478\n  Costo de oportunidad: 0.678\n  Transferencia de conocimiento: 96.67%\n\n[Métricas Empíricas - Paso 158000]\n  Retención Fase A: 5.6%\n  Interferencia B→A: 0.489\n  Costo de oportunidad: 0.672\n  Transferencia de conocimiento: 95.24%\n\n[Estadísticas del Sistema - Paso 158000]\n  Plasticidad promedio: 0.223\n  Distribución: Baja=264, Media=0, Alta=72\n  Balance de plasticidad: 0.429\n\n[Métricas Empíricas - Paso 159000]\n  Retención Fase A: 2.2%\n  Interferencia B→A: 0.485\n  Costo de oportunidad: 0.670\n  Transferencia de conocimiento: 92.60%\n\n[Métricas Empíricas - Paso 160000]\n  Retención Fase A: 2.1%\n  Interferencia B→A: 0.479\n  Costo de oportunidad: 0.681\n  Transferencia de conocimiento: 98.17%\n\n[Estadísticas del Sistema - Paso 160000]\n  Plasticidad promedio: 0.201\n  Distribución: Baja=264, Media=0, Alta=63\n  Balance de plasticidad: 0.385\n\n[Métricas Empíricas - Paso 161000]\n  Retención Fase A: 1.8%\n  Interferencia B→A: 0.486\n  Costo de oportunidad: 0.678\n  Transferencia de conocimiento: 97.58%\n\n[Métricas Empíricas - Paso 162000]\n  Retención Fase A: 3.2%\n  Interferencia B→A: 0.490\n  Costo de oportunidad: 0.675\n  Transferencia de conocimiento: 96.10%\n\n[Estadísticas del Sistema - Paso 162000]\n  Plasticidad promedio: 0.215\n  Distribución: Baja=264, Media=0, Alta=69\n  Balance de plasticidad: 0.414\n\n[Métricas Empíricas - Paso 163000]\n  Retención Fase A: 2.9%\n  Interferencia B→A: 0.502\n  Costo de oportunidad: 0.675\n  Transferencia de conocimiento: 96.70%\n\n[Métricas Empíricas - Paso 164000]\n  Retención Fase A: 2.5%\n  Interferencia B→A: 0.484\n  Costo de oportunidad: 0.686\n  Transferencia de conocimiento: 98.76%\n\n[Estadísticas del Sistema - Paso 164000]\n  Plasticidad promedio: 0.189\n  Distribución: Baja=264, Media=0, Alta=58\n  Balance de plasticidad: 0.360\n\n[Métricas Empíricas - Paso 165000]\n  Retención Fase A: 3.6%\n  Interferencia B→A: 0.505\n  Costo de oportunidad: 0.671\n  Transferencia de conocimiento: 95.55%\n\n[Métricas Empíricas - Paso 168000]\n  Retención Fase A: 4.0%\n  Interferencia B→A: 0.498\n  Costo de oportunidad: 0.673\n  Transferencia de conocimiento: 94.33%\n\n[Estadísticas del Sistema - Paso 168000]\n  Plasticidad promedio: 0.220\n  Distribución: Baja=264, Media=0, Alta=71\n  Balance de plasticidad: 0.424\n\n[Métricas Empíricas - Paso 169000]\n  Retención Fase A: 10.5%\n  Interferencia B→A: 0.494\n  Costo de oportunidad: 0.685\n  Transferencia de conocimiento: 98.45%\n\n[Métricas Empíricas - Paso 170000]\n  Retención Fase A: 5.6%\n  Interferencia B→A: 0.480\n  Costo de oportunidad: 0.676\n  Transferencia de conocimiento: 96.39%\n\n[Estadísticas del Sistema - Paso 170000]\n  Plasticidad promedio: 0.213\n  Distribución: Baja=264, Media=0, Alta=68\n  Balance de plasticidad: 0.410\n\n[Métricas Empíricas - Paso 171000]\n  Retención Fase A: 4.4%\n  Interferencia B→A: 0.495\n  Costo de oportunidad: 0.685\n  Transferencia de conocimiento: 99.38%\n\n[Métricas Empíricas - Paso 172000]\n  Retención Fase A: 2.9%\n  Interferencia B→A: 0.483\n  Costo de oportunidad: 0.682\n  Transferencia de conocimiento: 98.16%\n\n[Estadísticas del Sistema - Paso 172000]\n  Plasticidad promedio: 0.199\n  Distribución: Baja=264, Media=0, Alta=62\n  Balance de plasticidad: 0.380\n\n============================================================\nÉpoca 6/6\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Época 6:   0%|          | 0/34559 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97077b2a5f0645aaa14b394d39cbb89f"}},"metadata":{}},{"name":"stdout","text":"\n[Métricas Empíricas - Paso 173000]\n  Retención Fase A: 1.7%\n  Interferencia B→A: 0.489\n  Costo de oportunidad: 0.684\n  Transferencia de conocimiento: 98.46%\n\n[Métricas Empíricas - Paso 174000]\n  Retención Fase A: 1.9%\n  Interferencia B→A: 0.499\n  Costo de oportunidad: 0.671\n  Transferencia de conocimiento: 94.96%\n\n[Estadísticas del Sistema - Paso 174000]\n  Plasticidad promedio: 0.225\n  Distribución: Baja=264, Media=0, Alta=73\n  Balance de plasticidad: 0.433\n\n[Métricas Empíricas - Paso 175000]\n  Retención Fase A: 3.2%\n  Interferencia B→A: 0.503\n  Costo de oportunidad: 0.678\n  Transferencia de conocimiento: 96.36%\n\n[Métricas Empíricas - Paso 176000]\n  Retención Fase A: 1.9%\n  Interferencia B→A: 0.509\n  Costo de oportunidad: 0.680\n  Transferencia de conocimiento: 96.65%\n\n[Estadísticas del Sistema - Paso 176000]\n  Plasticidad promedio: 0.204\n  Distribución: Baja=264, Media=0, Alta=64\n  Balance de plasticidad: 0.390\n\n[Métricas Empíricas - Paso 177000]\n  Retención Fase A: 1.6%\n  Interferencia B→A: 0.511\n  Costo de oportunidad: 0.685\n  Transferencia de conocimiento: 97.83%\n\n[Métricas Empíricas - Paso 178000]\n  Retención Fase A: 2.4%\n  Interferencia B→A: 0.514\n  Costo de oportunidad: 0.674\n  Transferencia de conocimiento: 95.51%\n\n[Estadísticas del Sistema - Paso 178000]\n  Plasticidad promedio: 0.218\n  Distribución: Baja=264, Media=0, Alta=70\n  Balance de plasticidad: 0.419\n\n[Métricas Empíricas - Paso 179000]\n  Retención Fase A: 3.1%\n  Interferencia B→A: 0.508\n  Costo de oportunidad: 0.673\n  Transferencia de conocimiento: 94.93%\n\n[Métricas Empíricas - Paso 180000]\n  Retención Fase A: 2.2%\n  Interferencia B→A: 0.527\n  Costo de oportunidad: 0.672\n  Transferencia de conocimiento: 95.24%\n\n[Estadísticas del Sistema - Paso 180000]\n  Plasticidad promedio: 0.223\n  Distribución: Baja=264, Media=0, Alta=72\n  Balance de plasticidad: 0.429\n\n[Métricas Empíricas - Paso 181000]\n  Retención Fase A: 2.3%\n  Interferencia B→A: 0.503\n  Costo de oportunidad: 0.682\n  Transferencia de conocimiento: 98.16%\n\n[Métricas Empíricas - Paso 182000]\n  Retención Fase A: 3.3%\n  Interferencia B→A: 0.515\n  Costo de oportunidad: 0.685\n  Transferencia de conocimiento: 98.14%\n\n[Estadísticas del Sistema - Paso 182000]\n  Plasticidad promedio: 0.191\n  Distribución: Baja=264, Media=0, Alta=59\n  Balance de plasticidad: 0.365\n\n[Métricas Empíricas - Paso 183000]\n  Retención Fase A: 1.2%\n  Interferencia B→A: 0.514\n  Costo de oportunidad: 0.677\n  Transferencia de conocimiento: 95.77%\n\n[Métricas Empíricas - Paso 184000]\n  Retención Fase A: 1.7%\n  Interferencia B→A: 0.525\n  Costo de oportunidad: 0.675\n  Transferencia de conocimiento: 95.50%\n\n[Estadísticas del Sistema - Paso 184000]\n  Plasticidad promedio: 0.215\n  Distribución: Baja=264, Media=0, Alta=69\n  Balance de plasticidad: 0.414\n\n[Métricas Empíricas - Paso 185000]\n  Retención Fase A: 0.6%\n  Interferencia B→A: 0.506\n  Costo de oportunidad: 0.680\n  Transferencia de conocimiento: 98.17%\n\n[Métricas Empíricas - Paso 186000]\n  Retención Fase A: 3.3%\n  Interferencia B→A: 0.506\n  Costo de oportunidad: 0.684\n  Transferencia de conocimiento: 99.07%\n\n[Estadísticas del Sistema - Paso 186000]\n  Plasticidad promedio: 0.194\n  Distribución: Baja=264, Media=0, Alta=60\n  Balance de plasticidad: 0.370\n\n[Métricas Empíricas - Paso 187000]\n  Retención Fase A: 4.5%\n  Interferencia B→A: 0.509\n  Costo de oportunidad: 0.680\n  Transferencia de conocimiento: 96.65%\n\n[Métricas Empíricas - Paso 188000]\n  Retención Fase A: 2.4%\n  Interferencia B→A: 0.526\n  Costo de oportunidad: 0.680\n  Transferencia de conocimiento: 96.65%\n\n[Estadísticas del Sistema - Paso 188000]\n  Plasticidad promedio: 0.204\n  Distribución: Baja=264, Media=0, Alta=64\n  Balance de plasticidad: 0.390\n\n[Métricas Empíricas - Paso 189000]\n  Retención Fase A: 1.9%\n  Interferencia B→A: 0.498\n  Costo de oportunidad: 0.678\n  Transferencia de conocimiento: 95.15%\n\n[Métricas Empíricas - Paso 190000]\n  Retención Fase A: 3.2%\n  Interferencia B→A: 0.507\n  Costo de oportunidad: 0.680\n  Transferencia de conocimiento: 96.95%\n\n[Estadísticas del Sistema - Paso 190000]\n  Plasticidad promedio: 0.204\n  Distribución: Baja=264, Media=0, Alta=64\n  Balance de plasticidad: 0.390\n\n[Métricas Empíricas - Paso 191000]\n  Retención Fase A: 4.8%\n  Interferencia B→A: 0.517\n  Costo de oportunidad: 0.669\n  Transferencia de conocimiento: 94.40%\n\n[Métricas Empíricas - Paso 192000]\n  Retención Fase A: 2.5%\n  Interferencia B→A: 0.511\n  Costo de oportunidad: 0.683\n  Transferencia de conocimiento: 98.46%\n\n[Estadísticas del Sistema - Paso 192000]\n  Plasticidad promedio: 0.196\n  Distribución: Baja=264, Media=0, Alta=61\n  Balance de plasticidad: 0.375\n\n[Métricas Empíricas - Paso 193000]\n  Retención Fase A: 2.2%\n  Interferencia B→A: 0.497\n  Costo de oportunidad: 0.681\n  Transferencia de conocimiento: 96.94%\n\n[Métricas Empíricas - Paso 194000]\n  Retención Fase A: 4.8%\n  Interferencia B→A: 0.511\n  Costo de oportunidad: 0.676\n  Transferencia de conocimiento: 96.08%\n\n[Estadísticas del Sistema - Paso 194000]\n  Plasticidad promedio: 0.213\n  Distribución: Baja=264, Media=0, Alta=68\n  Balance de plasticidad: 0.410\n\n[Métricas Empíricas - Paso 195000]\n  Retención Fase A: 2.2%\n  Interferencia B→A: 0.504\n  Costo de oportunidad: 0.684\n  Transferencia de conocimiento: 98.77%\n\n[Métricas Empíricas - Paso 197000]\n  Retención Fase A: 2.2%\n  Interferencia B→A: 0.520\n  Costo de oportunidad: 0.680\n  Transferencia de conocimiento: 96.65%\n\n[Métricas Empíricas - Paso 198000]\n  Retención Fase A: 2.1%\n  Interferencia B→A: 0.513\n  Costo de oportunidad: 0.681\n  Transferencia de conocimiento: 97.55%\n\n[Estadísticas del Sistema - Paso 198000]\n  Plasticidad promedio: 0.201\n  Distribución: Baja=264, Media=0, Alta=63\n  Balance de plasticidad: 0.385\n\n[Métricas Empíricas - Paso 199000]\n  Retención Fase A: 2.0%\n  Interferencia B→A: 0.518\n  Costo de oportunidad: 0.680\n  Transferencia de conocimiento: 96.34%\n\n[Métricas Empíricas - Paso 200000]\n  Retención Fase A: 2.6%\n  Interferencia B→A: 0.512\n  Costo de oportunidad: 0.680\n  Transferencia de conocimiento: 98.17%\n\n[Estadísticas del Sistema - Paso 200000]\n  Plasticidad promedio: 0.204\n  Distribución: Baja=264, Media=0, Alta=64\n  Balance de plasticidad: 0.390\n\n[Métricas Empíricas - Paso 201000]\n  Retención Fase A: 1.6%\n  Interferencia B→A: 0.518\n  Costo de oportunidad: 0.674\n  Transferencia de conocimiento: 95.21%\n\n[Métricas Empíricas - Paso 202000]\n  Retención Fase A: 2.2%\n  Interferencia B→A: 0.505\n  Costo de oportunidad: 0.673\n  Transferencia de conocimiento: 94.33%\n\n[Estadísticas del Sistema - Paso 202000]\n  Plasticidad promedio: 0.220\n  Distribución: Baja=264, Media=0, Alta=71\n  Balance de plasticidad: 0.424\n\n[Métricas Empíricas - Paso 203000]\n  Retención Fase A: 2.8%\n  Interferencia B→A: 0.523\n  Costo de oportunidad: 0.670\n  Transferencia de conocimiento: 94.97%\n\n[Métricas Empíricas - Paso 204000]\n  Retención Fase A: 3.7%\n  Interferencia B→A: 0.512\n  Costo de oportunidad: 0.682\n  Transferencia de conocimiento: 97.55%\n\n[Estadísticas del Sistema - Paso 204000]\n  Plasticidad promedio: 0.199\n  Distribución: Baja=264, Media=0, Alta=62\n  Balance de plasticidad: 0.380\n\n[Métricas Empíricas - Paso 205000]\n  Retención Fase A: 8.5%\n  Interferencia B→A: 0.529\n  Costo de oportunidad: 0.677\n  Transferencia de conocimiento: 95.47%\n\n[Métricas Empíricas - Paso 206000]\n  Retención Fase A: 3.9%\n  Interferencia B→A: 0.512\n  Costo de oportunidad: 0.675\n  Transferencia de conocimiento: 96.10%\n\n[Estadísticas del Sistema - Paso 206000]\n  Plasticidad promedio: 0.215\n  Distribución: Baja=264, Media=0, Alta=69\n  Balance de plasticidad: 0.414\n\n[Métricas Empíricas - Paso 207000]\n  Retención Fase A: 3.3%\n  Interferencia B→A: 0.510\n  Costo de oportunidad: 0.673\n  Transferencia de conocimiento: 95.22%\n\n============================================================\nENTRENAMIENTO COMPLETO - ANÁLISIS FINAL\n============================================================\n\n================================================================================\nPLASTIC LSTM - REPORTE NUMÉRICO FINAL\n================================================================================\n\n[1] RENDIMIENTO:\n  - Perplejidad Final: 4.58\n  - PPL Final (Fase A): 5.46\n  - PPL Inicial (Fase B): 5.51\n  - PPL Final (Fase B): 4.58\n  - Mejora por Adaptación (Caída de PPL en Fase B): 0.93 puntos\n\n[2] EFICIENCIA:\n  - Neuronas Activas: 329 / 1024 (32.13%)\n  - Parámetros Activos: 1,791,734\n  - Ahorro de Memoria Estimado: 67.87%\n\n[3] DINÁMICA DE POBLACIÓN:\n  - Supervivencia Fase A: 264 de 264 nacidas.\n  - Supervivencia Fase B: 65 de 65 nacidas.\n\n================================================================================\n\n[4] SISTEMA COMPETITIVO:\n  - Población Final: 329 / 307 objetivo\n  - Capacidad efectiva: 6.6%\n  - Distribución de plasticidad:\n    • Baja (preservación): 264 neuronas\n    • Media (mixta): 0 neuronas\n    • Alta (adaptación): 65 neuronas\n\n[5] APRENDIZAJE CONTINUO:\n  - Retención Fase A: 100.0%\n  - Crecimiento Fase B: 100.0%\n  - Neuronas cross-phase: 0\n  - Gradiente de plasticidad A→B: 0.990\n\nGuardando datos en CSV en: /kaggle/working/plastic_results/csv_data\n - training_history.csv guardado.\n - generation_samples.csv guardado.\n - neuron_life_events.csv guardado.\n - inheritance_history.csv guardado.\nTodos los datos CSV han sido guardados.\n\nCheckpoint del modelo guardado en: /kaggle/working/plastic_results/checkpoints/plastic_lstm_20250804_015821.pt\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import defaultdict, Counter\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List, Dict, Tuple\nimport pandas as pd\n\ndef generate_text(model, prompt, stoi, itos, device, length=100, temperature=0.8, top_k=40):\n    \"\"\"Genera texto a partir de un prompt\"\"\"\n    model.eval()\n    tokens = [stoi.get(c, 0) for c in prompt]\n    generated = list(prompt)\n    \n    with torch.no_grad():\n        for _ in range(length):\n            x = torch.tensor([tokens[-model.c.sequence_length:]], device=device)\n            logits = model(x)\n            \n            # Aplicar temperatura\n            logits = logits[0, -1, :] / temperature\n            \n            # Top-k sampling\n            if top_k > 0:\n                values, indices = torch.topk(logits, min(top_k, logits.size(-1)))\n                logits[logits < values[-1]] = -float('inf')\n            \n            probs = F.softmax(logits, dim=-1)\n            idx = torch.multinomial(probs, 1).item()\n            \n            char = itos.get(idx, ' ')\n            generated.append(char)\n            tokens.append(idx)\n    \n    return ''.join(generated)\n\ndef calculate_prediction_metrics(model, prompt, stoi, itos, device, actual_continuation=None):\n    \"\"\"Calcula métricas detalladas de predicción\"\"\"\n    model.eval()\n    metrics = {}\n    \n    tokens = [stoi.get(c, 0) for c in prompt]\n    \n    with torch.no_grad():\n        x = torch.tensor([tokens[-model.c.sequence_length:]], device=device)\n        logits, analysis = model(x, return_analysis=True)\n        \n        # Perplejidad del prompt\n        if len(tokens) > 1:\n            x_full = torch.tensor([tokens[:-1]], device=device)\n            y_full = torch.tensor([tokens[1:]], device=device)\n            logits_full = model(x_full)\n            loss = F.cross_entropy(logits_full[0], y_full[0])\n            metrics['prompt_perplexity'] = torch.exp(loss).item()\n        \n        # Entropía de la distribución de predicción\n        probs = F.softmax(logits[0, -1, :], dim=-1)\n        entropy = -torch.sum(probs * torch.log(probs + 1e-8))\n        metrics['prediction_entropy'] = entropy.item()\n        \n        # Top-k probabilidades\n        top_k_probs, top_k_indices = torch.topk(probs, 10)\n        metrics['top_10_predictions'] = [\n            (itos.get(idx.item(), '?'), prob.item()) \n            for idx, prob in zip(top_k_indices, top_k_probs)\n        ]\n        \n        # Análisis de activaciones\n        if analysis and 'activations' in analysis:\n            acts = analysis['activations'][0]\n            metrics['mean_activation'] = acts.mean().item()\n            metrics['activation_sparsity'] = (acts == 0).float().mean().item()\n            metrics['active_neurons'] = (acts > 0.1).sum().item()\n            \n            # Activaciones por fase\n            phase_a_neurons = list(model.phase_a_neurons)\n            phase_b_neurons = list(model.phase_b_neurons)\n            \n            if phase_a_neurons:\n                phase_a_acts = acts[phase_a_neurons].mean().item()\n            else:\n                phase_a_acts = 0\n                \n            if phase_b_neurons:\n                phase_b_acts = acts[phase_b_neurons].mean().item()\n            else:\n                phase_b_acts = 0\n                \n            metrics['phase_a_activation'] = phase_a_acts\n            metrics['phase_b_activation'] = phase_b_acts\n        \n        # Si tenemos la continuación real, calcular accuracy\n        if actual_continuation:\n            next_token = stoi.get(actual_continuation[0], 0)\n            predicted_token = torch.argmax(logits[0, -1, :]).item()\n            metrics['exact_match'] = (predicted_token == next_token)\n            \n            # Rank del token correcto\n            sorted_indices = torch.argsort(probs, descending=True)\n            rank = (sorted_indices == next_token).nonzero().item() + 1\n            metrics['correct_token_rank'] = rank\n            metrics['correct_token_prob'] = probs[next_token].item()\n    \n    return metrics\n\ndef comprehensive_model_evaluation(model, cfg, stoi, itos, device):\n    \"\"\"Evaluación comprehensiva del modelo\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"EVALUACIÓN COMPREHENSIVA DEL MODELO PLASTICLSTM\")\n    print(\"=\"*80)\n    \n    results = defaultdict(list)\n    \n    # 1. Evaluar con prompts de diferentes fases y categorías\n    test_cases = {\n        'Phase A - Code': [\n            (\"def fibonacci(n):\", \" return fib_helper(n, {})\"),\n            (\"import numpy as np\", \"\\nimport matplotlib.pyplot as plt\"),\n            (\"class NeuralNetwork:\", \"\\n    def __init__(self):\"),\n        ],\n        'Phase A - Narrative': [\n            (\"The quick brown fox\", \" jumps over the lazy dog\"),\n            (\"Once upon a time\", \" in a land far away\"),\n            (\"The scientist discovered\", \" a remarkable phenomenon\"),\n        ],\n        'Phase B - Narrative': [\n            (\"In a hole in the ground\", \" there lived a hobbit\"),\n            (\"The spaceship landed\", \" on the distant planet\"),\n            (\"In the year 2050,\", \" technology had advanced\"),\n        ],\n        'Cross-Phase': [\n            (\"Machine learning is\", \" a subset of artificial intelligence\"),\n            (\"The algorithm works by\", \" processing data iteratively\"),\n            (\"Neural networks can\", \" learn complex patterns\"),\n        ]\n    }\n    \n    # 2. Evaluar cada caso\n    for category, prompts in test_cases.items():\n        print(f\"\\n\\n{category}:\")\n        print(\"-\" * 50)\n        \n        for prompt, expected in prompts:\n            # Generar texto\n            generated = generate_text(model, prompt, stoi, itos, device, \n                                    length=50, temperature=0.8)\n            \n            # Calcular métricas\n            metrics = calculate_prediction_metrics(model, prompt, stoi, itos, \n                                                 device, expected[1:] if len(expected) > 1 else None)\n            \n            # Mostrar resultados\n            print(f\"\\nPrompt: '{prompt}'\")\n            print(f\"Generated: '{generated[len(prompt):len(prompt)+50]}'\")\n            print(f\"Perplexity: {metrics.get('prompt_perplexity', 'N/A'):.2f}\")\n            print(f\"Entropy: {metrics['prediction_entropy']:.3f}\")\n            print(f\"Active Neurons: {metrics.get('active_neurons', 'N/A')}\")\n            print(f\"Phase A activation: {metrics.get('phase_a_activation', 0):.3f}\")\n            print(f\"Phase B activation: {metrics.get('phase_b_activation', 0):.3f}\")\n            \n            # Top predictions\n            print(\"Top 5 predictions:\")\n            for char, prob in metrics['top_10_predictions'][:5]:\n                print(f\"  '{char}': {prob:.3f}\")\n            \n            # Guardar para análisis\n            results[category].append({\n                'prompt': prompt,\n                'generated': generated[len(prompt):len(prompt)+30],\n                **metrics\n            })\n    \n    # 3. Análisis de diversidad y calidad\n    print(\"\\n\\n\" + \"=\"*60)\n    print(\"ANÁLISIS DE CALIDAD DE GENERACIÓN\")\n    print(\"=\"*60)\n    \n    # Evaluar repetición\n    all_generated = []\n    for category, items in results.items():\n        for item in items:\n            all_generated.append(item['generated'])\n    \n    # Calcular diversidad de n-gramas\n    for n in [2, 3, 4]:\n        ngrams = []\n        for text in all_generated:\n            for i in range(len(text) - n + 1):\n                ngrams.append(text[i:i+n])\n        \n        unique_ratio = len(set(ngrams)) / max(1, len(ngrams))\n        print(f\"\\n{n}-gram diversity: {unique_ratio:.3f}\")\n    \n    # 4. Comparación de activación entre fases\n    print(\"\\n\\n\" + \"=\"*60)\n    print(\"ANÁLISIS DE ACTIVACIÓN POR FASE\")\n    print(\"=\"*60)\n    \n    phase_analysis = defaultdict(lambda: {'phase_a': [], 'phase_b': []})\n    \n    for category, items in results.items():\n        for item in items:\n            phase_analysis[category]['phase_a'].append(\n                item.get('phase_a_activation', 0))\n            phase_analysis[category]['phase_b'].append(\n                item.get('phase_b_activation', 0))\n    \n    # Crear DataFrame para mejor visualización\n    analysis_data = []\n    for category, data in phase_analysis.items():\n        analysis_data.append({\n            'Category': category,\n            'Phase A Avg': np.mean(data['phase_a']),\n            'Phase B Avg': np.mean(data['phase_b']),\n            'Ratio B/A': np.mean(data['phase_b']) / max(0.001, np.mean(data['phase_a']))\n        })\n    \n    df_analysis = pd.DataFrame(analysis_data)\n    print(df_analysis.to_string(index=False))\n    \n    # 5. Test de capacidad de continuación coherente\n    print(\"\\n\\n\" + \"=\"*60)\n    print(\"TEST DE COHERENCIA EN GENERACIÓN LARGA\")\n    print(\"=\"*60)\n    \n    long_prompt = \"The future of artificial intelligence\"\n    long_generation = generate_text(model, long_prompt, stoi, itos, device, \n                                  length=200, temperature=0.8)\n    \n    print(f\"Prompt: '{long_prompt}'\")\n    print(f\"Generated text:\\n{long_generation}\")\n    \n    # Análizar coherencia mediante ventanas deslizantes\n    window_perplexities = []\n    tokens = [stoi.get(c, 0) for c in long_generation]\n    \n    for i in range(10, len(tokens) - 10, 5):\n        window_tokens = tokens[i-10:i+10]\n        x = torch.tensor([window_tokens[:-1]], device=device)\n        y = torch.tensor([window_tokens[1:]], device=device)\n        \n        with torch.no_grad():\n            logits = model(x)\n            loss = F.cross_entropy(logits[0], y[0])\n            window_perplexities.append(torch.exp(loss).item())\n    \n    print(f\"\\nCoherence Analysis:\")\n    print(f\"Mean window perplexity: {np.mean(window_perplexities):.2f}\")\n    print(f\"Std window perplexity: {np.std(window_perplexities):.2f}\")\n    print(f\"Perplexity trend: {'Increasing' if window_perplexities[-5:] > window_perplexities[:5] else 'Stable/Decreasing'}\")\n    \n    # 6. Resumen final\n    print(\"\\n\\n\" + \"=\"*60)\n    print(\"RESUMEN DE PERFORMANCE\")\n    print(\"=\"*60)\n    \n    all_perplexities = []\n    all_entropies = []\n    \n    for category, items in results.items():\n        perps = [item.get('prompt_perplexity', 0) for item in items if 'prompt_perplexity' in item]\n        ents = [item['prediction_entropy'] for item in items]\n        \n        if perps:\n            all_perplexities.extend(perps)\n        all_entropies.extend(ents)\n        \n        print(f\"\\n{category}:\")\n        print(f\"  Avg Perplexity: {np.mean(perps) if perps else 'N/A':.2f}\")\n        print(f\"  Avg Entropy: {np.mean(ents):.3f}\")\n    \n    print(f\"\\nOVERALL:\")\n    print(f\"  Global Avg Perplexity: {np.mean(all_perplexities):.2f}\")\n    print(f\"  Global Avg Entropy: {np.mean(all_entropies):.3f}\")\n    print(f\"  Active Neurons (avg): {int(model.mask.sum().item())}\")\n    print(f\"  Parameter Efficiency: {model.calculate_efficiency_metrics()['param_efficiency']*100:.1f}%\")\n    \n    return results\n\n# Función principal para llamar\ndef evaluate_model_performance(model, cfg, stoi, itos, device):\n    \"\"\"Función principal para evaluar el modelo\"\"\"\n    \n    # Ejecutar evaluación comprehensiva\n    results = comprehensive_model_evaluation(model, cfg, stoi, itos, device)\n    \n    # Crear visualizaciones\n    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n    \n    # 1. Perplexity por categoría\n    ax = axes[0, 0]\n    perp_data = defaultdict(list)\n    for cat, items in results.items():\n        for item in items:\n            if 'prompt_perplexity' in item:\n                perp_data[cat].append(item['prompt_perplexity'])\n    \n    categories = list(perp_data.keys())\n    perplexities = [np.mean(perp_data[cat]) for cat in categories]\n    ax.bar(categories, perplexities, color=['blue', 'green', 'red', 'purple'])\n    ax.set_title('Average Perplexity by Category')\n    ax.set_ylabel('Perplexity')\n    ax.tick_params(axis='x', rotation=45)\n    \n    # 2. Activación por fase\n    ax = axes[0, 1]\n    phase_data = {'Phase A': [], 'Phase B': []}\n    for cat, items in results.items():\n        for item in items:\n            phase_data['Phase A'].append(item.get('phase_a_activation', 0))\n            phase_data['Phase B'].append(item.get('phase_b_activation', 0))\n    \n    ax.boxplot([phase_data['Phase A'], phase_data['Phase B']], \n               labels=['Phase A', 'Phase B'])\n    ax.set_title('Neuron Activation Distribution by Phase')\n    ax.set_ylabel('Mean Activation')\n    \n    # 3. Entropía de predicción\n    ax = axes[1, 0]\n    entropy_by_cat = defaultdict(list)\n    for cat, items in results.items():\n        for item in items:\n            entropy_by_cat[cat].append(item['prediction_entropy'])\n    \n    for i, (cat, entropies) in enumerate(entropy_by_cat.items()):\n        ax.scatter([i]*len(entropies), entropies, label=cat, alpha=0.6)\n    \n    ax.set_xticks(range(len(entropy_by_cat)))\n    ax.set_xticklabels(list(entropy_by_cat.keys()), rotation=45)\n    ax.set_title('Prediction Entropy by Category')\n    ax.set_ylabel('Entropy')\n    ax.legend()\n    \n    # 4. Distribución de neuronas activas\n    ax = axes[1, 1]\n    active_neurons_data = []\n    for cat, items in results.items():\n        for item in items:\n            if 'active_neurons' in item:\n                active_neurons_data.append(item['active_neurons'])\n    \n    ax.hist(active_neurons_data, bins=20, alpha=0.7, color='orange')\n    ax.set_title('Distribution of Active Neurons')\n    ax.set_xlabel('Number of Active Neurons')\n    ax.set_ylabel('Frequency')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return results\n\n# Ejemplo de uso:\nif __name__ == \"__main__\":\n    # Asumiendo que tienes el modelo, cfg, stoi, itos y device\n    # results = evaluate_model_performance(plastic_model, cfg, stoi, itos, device)\n    pass","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}